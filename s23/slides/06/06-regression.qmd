---
title: "Regression for Describing"
subtitle: "Data Analytics and Visualization with R<br>Session 6"
title-slide-attributes:
  data-background-size: stretch
  data-slide-number: none
auto-stretch: false
institute: "University of Mannheim<br>Spring 2023"
author: "Viktoriia Semenova"
footer: "[🔗 r4da.live](https://r4da.live/)"
logo: images/logo.png
format:
  revealjs:
    theme: ../slides.scss
    transition: fade
    incremental: true   
    slide-number: true
    chalkboard: true
execute:
  echo: true
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
p_needed <- c("tidyverse", "janitor", "icons", 
              "countdown", "showtext", "ggdag", 
              "gt", "plotly", "broom", "patchwork")

# check if they are already installed, install if not installed
lapply(p_needed[!(p_needed %in% rownames(installed.packages()))], install.packages, repos = "http://cran.us.r-project.org")

# load the packages
lapply(p_needed, library, character.only = TRUE)

# set width of code output
options(width = 90)

# set figure parameters for knitr
knitr::opts_chunk$set(
  fig.width = 6.5, # 7" width
  fig.asp = 0.618, # the golden ratio
  fig.retina = 3, # dpi multiplier for displaying HTML output on retina
  fig.align = "center", # center align figures
  dpi = 300 # higher dpi, sharper image
)


font_add_google(name = "Gochi Hand")
showtext::showtext_auto()
# set theme for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 14)) +
  theme(plot.title = element_text(face = "bold"))

# governors <- readr::read_csv("https://r4da.live/files/data/external_data/governors.csv")
# trains <- readr::read_tsv("../data/trains.tsv") %>% clean_names()
set.seed(2508)
```

# Intro


## Agenda for Today

<br>

**Fitting the line with OLS**

<br>

**Interpretation of Regression Coefficients**

<br>

**Statistical Control**

<!-- ## Cookies and Happiness -->

```{r make-cookies, include=FALSE}
cookies <- tibble(
  happiness = c(0.5, 2.1, 1, 2.5, 3, 1.5, 2.2, 2.5, 1.8, 3),
  cookies = 1:10
)

cookies_data <- cookies
cookies_model <- lm(happiness ~ cookies, data = cookies)
cookies_fitted <- broom::augment(cookies_model)
```

```{r, echo=FALSE, message=FALSE, include=FALSE}
cookies_base <- ggplot(cookies_fitted, aes(x = cookies, y = happiness)) +
  geom_point(size = 3) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 3.5)) +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Cookies eaten", y = "Level of happiness") +
  # theme_minimal(base_size = 14) +
  theme(
    panel.grid.minor = element_blank(),
    plot.title = element_text(face = "bold")
  ) 

# cookies_base
```


## Cookies and Happiness: OLS Regression Line {.smaller}

```{r cookies-lm, echo=FALSE, message=FALSE, out.width="100%"}
cookies_base +
  stat_smooth(method = "lm", color = "#800010", se = FALSE)
```

## Regression Line Anatomy {.smaller}

```{r  echo=FALSE, message=FALSE, out.width="100%"}
 cookies_base +
  stat_smooth(method = "lm", color = "#800010", se = FALSE, fullrange = T) +
  geom_segment(aes(xend = cookies, yend = .fitted),
    color = "#80001090",
    size = 1
  ) +
    geom_point(size = 3) +
  ggbrace::geom_brace(
    aes(
      x = c(cookies$cookies[1] + 0.3, cookies$cookies[1]),
      y = c(cookies$happiness[1], fitted(cookies_model)[1])
    ),
    inherit.data = F,
    rotate = 90,
    color = "#800010"
  ) +
  annotate(
    "text",
    x = 2,
    y = c(cookies$happiness[1], fitted(cookies_model)[1]) %>% mean(),
    label = "Residual",
    family = "Gochi Hand",
        color = "#800010"
  ) +
  geom_point(aes(y = .fitted),
             size = 3,
             color = "#800010"
               ) +
  geom_curve(
  aes(x = 0.5 + seq(0, 1.5, length.out = 10),
    y = 2.6 + seq(0.1, 0.35, length.out = 10),
    xend = cookies,
    yend = .fitted),
  curvature = -0,
  arrow = arrow(type = "closed",
                length = unit(0.03, "npc"),
                angle=10),
      color = "#800010",
  alpha = 0.5
) +
    annotate(
    "text",
    x = 0.6,
    y = 3,
    label = "Predicted happiness\n(Fitted Values)",
    family = "Gochi Hand",
        color = "#800010"
  ) +
  xlim(c(0, 10)) +
  geom_point(aes(y = 1.16,
                 x = 0),
             size = 3,
             color = "#800010"
               ) +
    annotate(
    "text",
    x = 0,
    y = 0.2,
    label = "Intercept",
    family = "Gochi Hand",
        color = "#800010"
  ) +
  geom_curve(
    data = NULL,
  aes(x = 0.1,
    y = 0.3,
    xend = 0,
    yend = 1.16),
  curvature = -0.01,
  arrow = arrow(type = "closed",
                length = unit(0.03, "npc"),
                angle=10),
      color = "#80001050") +
    coord_cartesian(xlim = c(0, 10), ylim = c(0, 3.5)) +
  scale_x_continuous(breaks = 0:10) 

# cookies_with_residual
```

# Fitting and Interpreting Models

## Workflow
<br>

```{dot}
//| echo: false
digraph G {
  forcelabels=true;
  splines=false;
  fontname="AtkinsonHyperlegible-Regular";
  causal [
    label = "Causal Estimand";
    shape = rect; 
    group=a;
  ];
  stats [
    label = "Statistical Estimand";
    shape = rect;
     group=a;
  ];
  est [
    label = "Estimate";
    shape = rect;
     group=a;
  ];
  mod [
    label = "Causal Model";
    shape = oval;
     group=b;
  ];
  data [
    label = "Data";
    shape = oval;
    group=b;
  ];

  causal -> stats [xlabel = "Identification    " ];
  stats -> est [ xlabel = "Estimation   "; dir = "backward"];
  mod -> stats;
  data -> est;

{
    rank=same;
    causal; mod; 
  }
  {
    rank=same;
    stats; data; 
  }


}

```


## Language of Models {.smaller}


-   **True Model**
    $y = \underbrace{\beta_0}_{\text{intercept}} + \underbrace{\beta_1}_{\text{slope}} x + \underbrace{\varepsilon}_{\text{error}}$
    -   Population parameters $\beta$: truth (estimand), unknown to us
-   **Estimated Model**
    $y = \underbrace{\hat\beta_0}_{\text{intercept}} + \underbrace{\hat\beta_1}_{\text{slope}} x + \underbrace{\hat\varepsilon}_{\text{residual}}$
    -   Estimates $\hat{\beta}$: our best guess about the estimand given the
        data
-   A model has two parts:
    -   **Systematic Component** of a linear model:
        $\underbrace{\hat y}_{\text{fitted}\\\text{value}} = \underbrace{\hat\beta_0}_{\text{intercept}} + \underbrace{\hat\beta_1}_{\text{slope}} x$
    -   **Stochastic Component** of a linear model: $\hat\varepsilon$


## Vocabulary

$$
y = \hat \beta_0 + \hat \beta_1 x_1 + \hat \varepsilon
$$

-   $y$: dependent variable, outcome
-   $x$: independent variable, treatment, explanatory variable, treatment,
    predictor, feature
-   $\hat y$: predicted values of y, y-hat, fitted values, regression line\
-   $\hat \beta_0$: intercept, prediction when all $x=0$, constant
-   $\hat \beta_k$: slope, the effect of $k$-th variable



## Modeling Happiness with Cookies


$$
y = \hat \beta_0 + \hat \beta_1 x_1 + \hat \varepsilon
$$

$$
\begin{aligned}
&{\text{Happiness}} =  \hat \beta_0 +  \hat \beta_1 \text{Cookies} + \hat \varepsilon
\end{aligned}
$$

$$
\begin{aligned}
&\widehat{\text{Happiness}} =  \hat \beta_0 +  \hat \beta_1 \text{Cookies} 
\end{aligned}
$$

:::task

What are the differences between equations?

:::

## Interpreting Slope Coefficient Poll {.normal}


`r equatiomatic::extract_eq(cookies_model, intercept = "beta", use_coefs = TRUE, coef_digits = 3, swap_var_names = c("cookies" = "Number of Cookies Eaten", "happiness" = "Happiness"))`

The slope of the model for predicting happiness score from number of consumed
cookies is `r round(cookies_model$coefficients["cookies"], 3)`. Which of the
following is the best interpretation of this value?

1.  For every additional cookie eaten, the happiness score goes up by
    `r round(cookies_model$coefficients["cookies"], 3)` points, on average.
2.  For every additional cookie eaten, we expect the happiness score to be
    higher by `r round(cookies_model$coefficients["cookies"], 3)` points, on
    average.
3.  For every additional cookie eaten, the happiness score goes up by
    `r round(cookies_model$coefficients["cookies"], 3)` points.
4.  For every one point increase in happiness score, the number of cookies eaten
    goes up by `r round(cookies_model$coefficients["cookies"], 3)` points, on
    average.


```{r}
#| echo: false
countdown(minutes = 1.5, color_background = "white", font_size = 2)
```

## Interpreting Slope and Intercept

::: small
`r equatiomatic::extract_eq(cookies_model, intercept = "beta", use_coefs = TRUE, coef_digits = 3, swap_var_names = c("cookies" = "Number of Cookies Eaten", "happiness" = "Happiness"))`

> Slope: For every additional cookie eaten, we expect the happiness score to be
> higher by `r round(cookies_model$coefficients["cookies"], 3)` points, on
> average.

-   Each additional cookie has the same effect on happiness, i.e. *marginal
    effect* is constant
    -   Associated increase in happiness is 0.155 for the first and, say, tenth
        cookie

> Intercept: If the number of eaten cookies is 0, we expect the happiness score
> to be `r round(cookies_model$coefficients[1], 3)` points.

-   Intercept is meaningful in the context of data because the predictor can
    feasibly take values equal to or near zero
:::

## What Are Coefficient Values in OLS {.smaller}

::: columns
::: {.column width="50%"}

$$\text{Sum of Squared Residuals (SSR)} \\= \sum^n_{i=1}\hat{\varepsilon_i}^2 \\ = \sum^n_{i=1}(y_i - \hat{y_i})^2 \\ = \sum^n_{i=1}(y_i - \hat{\beta_0} - \hat{\beta_1} x_i)^2$$

-   OLS estimator finds values of $\hat\beta$ which minimize $SSR$, the
    unexplained variance\
-   We use differential calculus to find these values of $\hat{\beta}$

:::
::: {.column width="50%"}

```{r}
#| echo: false
#| out-width: 100%
#| out-height: 100%
#| fig-width: 3
x <-  seq(-0.9, 0.9, length = 30)
y <- seq(-1, 1, length = 30)
cone <- function(x, y){
(y^2 + x^2)
}
z <- outer(x, y, cone) 

plot_ly(x = x, y = y, z = z) %>%
  add_surface() %>%
  layout(
    title = "",
    scene = list(
      xaxis = list(title = "\u03b2\u2080",showticklabels=F),
      yaxis = list(title = "\u03B2\u2081",showticklabels=F),
      zaxis = list(title = "Sum of Squared Residuals",showticklabels=F)
    )) %>% 
  hide_guides() %>%
  plotly::hide_legend()
```
:::
:::

-   Check [Resources](https://r4da.live/resource/slr.html) on the class website
    if you want to see the math behind it

## Fitting Line Example I {.smaller}


$$
\begin{aligned}
&\widehat{\text{Happiness}} =  2 - 0.1 \cdot \text{Cookies} 
\end{aligned}
$$

```{r}
#| echo: false
#| fig-asp: 0.42
p1 <-  cookies_base +
  geom_smooth(method = "lm",
              color = "#800010",
              se = FALSE) +
  geom_segment(aes(xend = cookies, yend = .fitted),
               color = "#80001090",
               size = 1) +
  geom_point(size = 3) +
  geom_rect(
    mapping = aes(
      xmin = cookies,
      xmax = cookies - abs(.resid),
      ymin = .fitted,
      ymax = happiness
    ),
    color = "black",
    alpha = 0.5
  ) +
  geom_label(aes(
    x = (cookies + cookies - abs(.resid)) / 2,
    y = (.fitted + happiness) / 2,
    label = .resid ^ 2 %>% round(2)
  ), label.size = 0.1) +
  ylim(c(0, 4)) +
  labs(title = paste0("Sum of Squared Residuals = ", sum(cookies_fitted$.resid ^ 2) %>%
                        round(2)))
p2 <- (cookies_base +
  geom_abline(
    intercept = 2,
    slope = 0,
    color = "#800010"
  ) +
  geom_point(size = 3) +
  geom_rect(
    mapping = aes(
      xmin = cookies,
      xmax = cookies - abs(2 - happiness),
      ymin = 2,
      ymax = happiness
    ),
    color = "black",
    alpha = 0.5
  ) +
  geom_label(aes(
    x = cookies,
    y = (2 + happiness) / 2,
    label = (happiness - 2)^2 %>% round(2),
    ),
     nudge_x = -0.5,label.size = 0.1,
    direction = "x",
    
    nudge_y = 0.2) +
  ylim(c(0, 4)) +
  labs(title = paste0("Sum of Squared Residuals = ", sum((
    cookies_fitted$happiness - 2
  )^2) %>%
    round(2)))) 

p3 <- cookies_base +
  geom_abline(
    intercept = mean(cookies_data$happiness),
    slope = -0.1,
    color = "#800010"
  ) +
  geom_rect(
    mapping = aes(
      xmin = cookies,
      xmax = cookies - (abs(2 - 0.1 * cookies - happiness)),
      ymin = happiness,
      ymax = 2 - 0.1 * cookies
    ),
    color = "black",
    alpha = 0.5
  ) +
  geom_label(aes(
    x = (cookies + cookies - (abs(happiness - (mean(cookies_data$happiness) - 0.1 * cookies_fitted$cookies)))) / 2,
    y = (happiness + (mean(cookies_data$happiness) - 0.1 * cookies))/
      2,
    label = (happiness - (mean(cookies_data$happiness) - 0.1 * cookies)) ^
      2 %>% round(2)),
    label.size = 0.1
        # nudge_x = 0.05
) +
  # ylim(c(0, 4)) +
  labs(title = paste0("Sum of Squared Residuals = ", sum((cookies_fitted$happiness - (mean(cookies_data$happiness) - 0.1 * cookies)) ^2) %>%
    round(2)))

p4 <- cookies_base +
  geom_abline(
    intercept = 1.1,
    slope = 0.1,
    color = "#800010"
  ) +
  geom_rect(
    mapping = aes(
      xmin = cookies,
      xmax = cookies - (abs(1.1 + 0.1 * cookies - happiness)),
      ymin = happiness,
      ymax = 1.1 + 0.1 * cookies
    ),
    color = "black",
    alpha = 0.5
  ) +
  geom_label(aes(
    x = (cookies + cookies - (abs(happiness - (1.1 + 0.1 * cookies)))) / 2,
    y = (happiness + (1.1 + 0.1 * cookies))/
      2,
    label = (happiness - (1.1 + 0.1 * cookies)) ^
      2 %>% round(2)),
    label.size = 0.1
        # nudge_x = 0.05
) +
  # ylim(c(0, 4)) +
  labs(title = paste0("Sum of Squared Residuals = ", sum((cookies_fitted$happiness - (1.1 + 0.1 * cookies_fitted$cookies)) ^2) %>%
    round(2)))
p3
```

## Fitting Line Example II {.smaller}



$$
\begin{aligned}
&\widehat{\text{Happiness}} =  2 + 0 \cdot \text{Cookies} = \overline{\text{Happiness}} 
\end{aligned}
$$

```{r}
#| echo: false
#| fig-asp: 0.42
p2

```

## Fitting Line Example III {.smaller}



$$
\begin{aligned}
&\widehat{\text{Happiness}} = 1.1 + 0.1 \cdot \text{Cookies}
\end{aligned}
$$

```{r}
#| echo: false
#| fig-asp: 0.42
p4

```

## Fitting Line Example IV: OLS Solution {.smaller}



$$
\begin{aligned}
&\widehat{\text{Happiness}} =   1.16 + 0.155 \cdot \text{Cookies} 
\end{aligned}
$$

```{r}
#| echo: false
#| fig-asp: 0.42
p1

```

## Properties of Least Squares Regression

::: smaller
-   The regression line goes through the center of mass point, the coordinates
    corresponding to average $X$ and average $Y$, $(\bar{X}, \bar{Y})$:

$$\bar{Y} = \hat \beta_0 + \hat \beta_1 \bar{X} ~ \rightarrow ~ \hat \beta_0 = \bar{y} - \hat \beta_1 \bar{x}$$

-   The slope has the same sign as the correlation coefficient:
    $\beta_X = Corr(X,Y) \dfrac{{\sigma_Y}}{{\sigma_X}}$

-   The sum of the residuals is zero (by design): $\sum_{i = 1}^n e_i = 0$
:::

# Beyond the Regression Line

## Unexplained Variance in Y (a.k.a. Errors/Residuals) {.smaller}

- random noise, the *stochastic* component of the model: the sum of “everything else” not in the systematic component of the model
- we assume errors to be, on average, zero given every value of X: $\mathbb{E}(\varepsilon|X) = 0$

<!-- $\varepsilon \sim \mathcal{N}(0, \sigma^2)$) --> 

```{r, echo=FALSE}
#| out-height: 80%
set.seed(0)
dat <- data.frame(x=(x=runif(10000, 0, 50)),
                  y=rnorm(10000, 10*x, 100))
## breaks: where you want to compute densities
breaks <- seq(0, max(dat$x), len=5)
dat$section <- cut(dat$x, breaks)
## Get the residuals
dat$res <- residuals(lm(y ~ x, data=dat))
## Compute densities for each section, flip the axes, add means of sections
## Note: densities need to be scaled in relation to section size (2000 here)
dens <- do.call(rbind, lapply(split(dat, dat$section), function(x) {
  d <- density(x$res, n=5000)
  res <- data.frame(x=max(x$x)- d$y*2000, y=d$x+mean(x$y))
  res <- res[order(res$y), ]
  ## Get some data for normal lines as well
  xs <- seq(min(x$res), max(x$res), len=5000)
  res <- rbind(res, data.frame(y=xs ,
                               x=max(x$x) - 2000*dnorm(xs, 0, sd(x$res))))
  res$type <- rep(c("empirical", "normal"), each=5000)
  res
}))
dens$section <- rep(levels(dat$section), each=10000)
dat$res <- residuals(lm(y ~ x, data=dat))
ggplot(dat, aes(x, res)) +
  geom_point(size = 0.1, alpha = 0.25) +
  geom_hline(yintercept = 0) +
  geom_path(
    data = dens[dens$type == "normal", ],
    aes(x, y, group = section),
    color = "#800010",
    lwd = 0.8
  ) +
  theme(axis.text.x = element_blank()) +
  scale_y_continuous(breaks = 0) +
    labs(y = "Residuals",
       x = "X") 
  
  
```


## Explained vs. Unexplained Variation

::: smaller
**Explained Sum of Squares:** $$ESS = \sum^{n}_{i=1}(\hat y_i - \bar y)^2$$

**Sum of Squared Residuals:**
$$RSS = \sum^n_{i=1}\hat{\varepsilon_i}^2 = \sum^n_{i=1}(y_i - \hat{y_i})^2$$

**Total Sum of Squares:** $$TSS = \sum^{n}_{i=1}(y_i - \bar y)^2 = ESS + RSS$$
:::

## Explained vs. Unexplained Variation

::: r-stack
```{r echo=FALSE, message=FALSE, out.width="100%"}
# cookies_with_residual <- 
   cookies_base +
  # geom_smooth(method = "lm", color = "#800010", se = FALSE) +
  # geom_segment(aes(xend = cookies, yend = .fitted),
  #   color = "#80001090",
  #   size = 1
  # ) +
    geom_point(size = 3) +
  geom_hline(yintercept = mean(cookies_fitted$happiness),
             color = "#000000") +
  geom_text(aes(x = 0, y = mean(cookies_fitted$happiness) + 0.1), 
            label  = expression(bar("Y")))

# cookies_with_residual

```

::: fragment
```{r echo=FALSE, message=FALSE, out.width="100%"}
cookies_base +
  geom_point(size = 3) +
  geom_hline(yintercept = mean(cookies_fitted$happiness),
             color = "#000000") +
  geom_text(aes(x = 0, y = mean(cookies_fitted$happiness) + 0.1, label  = expression(bar("Y")))) +
  geom_segment(aes(xend = cookies, yend = mean(cookies_fitted$happiness)),
               color = "#00000030",
               size = 3) +
  annotate(
    "text",
    x = 0,
    y = c(cookies$happiness[1], cookies$happiness %>% mean()) %>% mean(),
    family = "Gochi Hand"
  ) +
  ggbrace::geom_brace(aes(
    x = c(cookies$cookies[1] - 0.3, cookies$cookies[1]),
    y = c(cookies$happiness[1], cookies$happiness %>% mean())
  ),
  inherit.data = F, rotate = 270)

# cookies_with_residual
```
:::

::: fragment
```{r echo=FALSE, message=FALSE, out.width="100%"}
 cookies_base +
  geom_smooth(method = "lm", color = "#800010", se = FALSE) +
  geom_segment(aes(xend = cookies, yend = .fitted),
    color = "#80001090",
    size = 1
  ) +
    geom_point(size = 3) +
  geom_hline(yintercept = mean(cookies_fitted$happiness),
             color = "#000000") +
  geom_text(aes(x = 0, y = mean(cookies_fitted$happiness) + 0.1), 
            label  = expression(bar("Y"))) +
    geom_text(aes(x = 9.5, y = predict(object = cookies_model, newdata  = data.frame(cookies = 9.5))+ 0.09), 
            label  = expression(hat("Y")),
            color = "#800010") +
   geom_segment(aes(xend = cookies, yend = mean(cookies_fitted$happiness)),
    color = "#00000030",
    size = 3
  ) +
  annotate(
    "text",
    x = 0,
    y = c(cookies$happiness[1], cookies$happiness %>% mean()) %>% mean(),
    label = "Total\nVariation",
    family = "Gochi Hand"
  ) +
  ggbrace::geom_brace(aes(
    x = c(cookies$cookies[1] - 0.3, cookies$cookies[1]),
    y = c(cookies$happiness[1], cookies$happiness %>% mean())
  ),
  inherit.data = F, rotate = 270) +
  ggbrace::geom_brace(
    aes(
      x = c(cookies$cookies[1] + 0.3, cookies$cookies[1]),
      y = c(cookies$happiness[1], fitted(cookies_model)[1])
    ),
    inherit.data = F,
    rotate = 90,
    color = "#800010"
  ) +
  annotate(
    "text",
    x = 2,
    y = c(cookies$happiness[1], fitted(cookies_model)[1]) %>% mean(),
    label = "Residual",
    family = "Gochi Hand",
        color = "#800010"
  ) 

```
:::

::: fragment
```{r echo=FALSE, message=FALSE, out.width="100%"}
 cookies_base +
  geom_smooth(method = "lm", color = "#800010", se = FALSE) +
  geom_segment(aes(xend = cookies, yend = .fitted),
    color = "#80001090",
    size = 1
  ) +
    geom_point(size = 3) +
  geom_hline(yintercept = mean(cookies_fitted$happiness),
             color = "#000000") +
  geom_text(aes(x = 0, y = mean(cookies_fitted$happiness) + 0.1), 
            label  = expression(bar("Y"))) +
    geom_text(aes(x = 9.5, y = predict(object = cookies_model, newdata  = data.frame(cookies = 9.5))+ 0.09), 
            label  = expression(hat("Y")),
            color = "#800010") +
   geom_segment(aes(xend = cookies, yend = mean(cookies_fitted$happiness)),
    color = "#00000030",
    size = 3
  ) +
  annotate(
    "text",
    x = 0,
    y = c(cookies$happiness[1], cookies$happiness %>% mean()) %>% mean(),
    label = "Total\nVariation",
    family = "Gochi Hand"
  ) +
  ggbrace::geom_brace(aes(
    x = c(cookies$cookies[1] - 0.3, cookies$cookies[1]),
    y = c(cookies$happiness[1], cookies$happiness %>% mean())
  ),
  inherit.data = F, rotate = 270) +
  ggbrace::geom_brace(
    aes(
      x = c(cookies$cookies[1] + 0.3, cookies$cookies[1]),
      y = c(cookies$happiness[1], fitted(cookies_model)[1])
    ),
    inherit.data = F,
    rotate = 90,
    color = "#800010"
  ) +
  annotate(
    "text",
    x = 2,
    y = c(cookies$happiness[1], fitted(cookies_model)[1]) %>% mean(),
    label = "Residual",
    family = "Gochi Hand",
        color = "#800010"
  ) +
    ggbrace::geom_brace(
    aes(
      x = c(cookies$cookies[1] + 0.3, cookies$cookies[1]),
      y = c(fitted(cookies_model)[1], cookies$happiness %>% mean())
    ),
    inherit.data = F,
    rotate = 90,
    color = "#003056"
  ) +
    annotate(
    "text",
    x = 2,
    y = 2.55,
    label = "Explained\nVariation",
    family = "Gochi Hand",
        color = "#003056"
  ) +
  # annotate(
  #   "segment",
  #   x = 2,
  #   y = 2.4,
  #   xend = cookies$cookies[1] + 0.3,
  #   yend = c(fitted(cookies_model)[1], cookies$happiness %>% mean()) %>% mean(),
  #   arrow = grid::arrow(angle = 10),
  #   family = "Gochi Hand",
  #   color = "#003056"
  # )+
  geom_curve(
  aes(x = 2,
    y = 2.4,
    xend = 1.3,
    yend = 1.662273),
  curvature = -0.1,
  arrow = arrow(type = "closed",
                length = unit(0.03, "npc"),
                angle=10),
      color = "#003056"
)

```
:::
:::



```{r make-more-cookies, include=FALSE}
set.seed(250895)
cookies_data <- cookies
cookies_model <- lm(happiness ~ cookies, data = cookies)
more_cookies = data.frame(cookies = rbinom(100, size = 10, prob = 0.4))
more_cookies$weekend <- ifelse(more_cookies$cookies > 4, 0.2, 0.9)
more_cookies$weekend <- rbinom(100, size = 1, prob = more_cookies$weekend)
more_cookies$happiness <-
  predict(cookies_model, more_cookies) + 1.3 * more_cookies$weekend + rnorm(100, sd = 0.2) - 1
more_cookies$happiness_score <-
  predict(cookies_model, more_cookies) + rnorm(100, sd = 0.4) 
# cookies_fitted <- broom::augment(cookies_model)
```


## Residuals Plot

-   Scatterplot of the regression residuals against the explanatory variable X
    (or the predicted values of Y)
<!-- -   Patterns in residuals signal that systematic influences on Y still have not -->
<!--     been captured by our model, or that our model misrepresents the data, or -->
<!--     that errors do not have a constant variance. -->

```{r}
#| echo: false
#| out-height: 90%
m_cookies <- lm(happiness ~ cookies, data = cookies)
cookies_fitted <- augment(m_cookies) 

ggplot(
  cookies_fitted,
  aes(y = .resid, x = cookies)
) +
  geom_point() +
  geom_hline(
    yintercept = 0,
    linetype = "dashed"
  ) +
  labs(
    x = "Cookies eaten",
    y = "Residuals"
  )
```

## Residuals from More Cookies Data (N = 100)

```{r, echo=FALSE}
m_cookies <- lm(happiness_score ~ cookies, data = more_cookies)
cookies_fitted <- broom::augment(m_cookies) 
ggplot(
  cookies_fitted,
  aes(y = .resid, x = cookies)
) +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    x = "Cookies eaten (X)",
    y = "Residuals"
  )
```

# Multiple Linear Regression (MLR)

## Multiple ~~Multivariate~~ Linear Regression (MLR)

$$ y = \hat\beta_0 + \hat\beta_1 x_1 +
\hat\beta_2 x_2 + \cdots + \hat\beta_k x_k
+\hat\varepsilon$$

::: small
> A one unit increase in $x_k$ is *associated* with, **on average**, a $\beta_k$
> increase (decrease) in $y$, **holding all else constant**.

<!-- > Categorical X: On average, **Y** is $\hat\beta_k$ units larger (or smaller) in **x_k**, compared to **x_k**<sub>omitted</sub>, **holding all else constant**. -->

-   We obtain our coefficient for $x_k$ *independent of all other variables*
-   We are comparing observations as though they had same value of other
    variables
-   You can also think of it as comparing *within* values of other variables
-   Coefficient $\hat\beta_k$ tells us what is the value of a predictor $x$,
    once we know other predictors in the model
-   $\hat \beta_k$ captures the effect of $x_k$, which can be uniquely
    attributed to this variable $x_k$.
:::

## Cookies and Weekend Predict Happiness

$$ {\text{Happiness}}  = \hat\beta_0 + \hat\beta_1 {\text{Cookies}}  +
\hat\beta_2 {\text{Weekend}}+\hat\varepsilon$$

::: columns
::: {.column width="40%"}
```{dot}
//| echo: false
//| fig-width: 3
//| fig-height: 2.5
//| out-width: 100%
digraph D {
  node [shape=oval, fontname="AtkinsonHyperlegible-Regular"];
  edge [minlen = 1.2, arrowhead = vee, arrowsize=0.5];
  a [label = "Cookies"];
  b [label = "Happiness"];
  c [label = "Weekend"];
  
  {rank=same a b};
  {rank=sink c};
  a->b;
  c->a;
  c->b;
}
```
:::

::: {.column width="60%"}
```{r}
happiness_m2 <- lm(happiness ~ cookies + weekend, 
                   data = more_cookies)
tidy(happiness_m2)
```
:::
:::

::: small
> Reported happiness score is expected to be, on average,
> `r round(coef(happiness_m2)[1], 3)` points on weekdays when having no cookies
> eaten.\
> Each additional cookie eaten is *associated* with, **on average**, a
> `r round(coef(happiness_m2)[2], 3)` increase in happiness level, **holding all
> else constant**.\
> **On average**, happiness score is `r round(coef(happiness_m2)[3], 3)` points
> higher on weekends than on weekdays, **holding cookies consumption constant**.
:::

## Regression Line Turns into a Hyperplane

```{r plotlyfig,  echo=F, warning=F, message=F}
#| eval: false
  y <- more_cookies$happiness_score
  x1 <- more_cookies$cookies
  x2 <- more_cookies$weekend
  df <- data.frame(y, x1, x2)
  reg <- lm(y ~ x1 + x2)
  cf.mod <- coef(reg)
  x1.seq <- seq(min(x1), max(x1), length.out = 25)
  x2.seq <- seq(min(x2, na.rm = T), max(x2, na.rm = T), length.out = 25)
  z <-
    t(outer(x1.seq, x2.seq, function(x, y)
      cf.mod[1] + cf.mod[2] * x + cf.mod[3] * y))
  rbPal <- colorRampPalette(c('#CC2C24', '#003056'))
  cols <-
    rbPal(10)[as.numeric(cut(abs(y - reg$fitted.values), breaks = 10))]
  m <- list(t = 5)
  p <- plot_ly(
    x = x1.seq,
    y = x2.seq,
    z = z,
    colors = "#80001050",
    opacity = 0.9,
    name = "Reg.Plane",
    type = "surface"
  ) %>%
    add_trace(
      data = df,
      name = 'Happiness',
      x = x1,
      y = x2,
      z = y,
      mode = "markers",
      type = "scatter3d",
      marker = list(
        color = cols,
        opacity = 0.75,
        symbol = 105,
        size = 4
      )
    ) %>%
    hide_colorbar() %>%
    layout(
      margin = m,
      showlegend = FALSE,
      scene = list(
        aspectmode = "manual",
        aspectratio = list(x = 1, y = 1.3, z = 1),
        xaxis = list(title = "Cookies"),
        yaxis = list(title = "Weekend"),
        zaxis = list(title = "Happiness"),
        camera = list(
          eye = list(x = -1.5, y = -2, z = 1.05),
          center = list(x = 0,
                        y = 0,
                        z = 0)
        )
      )
      
    )
# p
# p %>% config(showLink = F)
# 
# cat(plotly:::plotly_iframe(p %>% config(showLink = F)))
# htmlwidgets::saveWidget(p,("p.html"))

```


<iframe src="p.html" width="70%" height="100%" id="igraph" scrolling="no" seamless="seamless" frameBorder="0">

</iframe>

# Statistical Control

## Confounders and Statistical Control

Closing paths means ensuring we are comparing within the same values of
confounders. What does this mean statistically?

1.  Remove the effect of the confounder W (*Weekend*) on X (*Cookies*) 
2.  Remove the effect of the confounder W (*Weekend*) on Y (*Happiness*)
3.  Regress the leftovers of Y (*Happiness*), the residuals from step 2, on leftovers of X  (*Cookies*), the residuals from step 1.

## Venn Diagrams: Variance and Covariance {.smaller}

<br>

![](images/correlation-simp.png){fig-align="center" width="700"}

## Venn Diagrams: Slope Coefficients {.smaller}

::: columns
::: {.column width="50%"}
::: smaller
Slope coefficient for $X$, is the ratio between [*covariance of* $X$ and
$Y$]{style="color: #003056;"} and [*variance of* $X$]{style="color: #61993B;"}

$$\beta_{x} = \frac{cov(x,y)}{var(x)}= \frac{B}{A+B}$$ Slope coefficient for
$Z$, is the ratio between [*covariance of* $Z$ and $Y$]{style="color: #cc2c24;"}
and [*variance of* $Z$]{style="color: #a1a1a0;"}

$$\beta_{z} = \frac{cov(z,y)}{var(z)}= \frac{D}{D+E}$$
:::
:::

::: {.column width="50%"}
![](images/corr3.png)
:::
:::

## Venn Diagrams: Statistical Control {.smaller}

::: columns
::: {.column width="50%"}
::: smaller
Here $X$ and $Z$ are correlated. If we did not include $Z$, our slope will still
be:

$$\beta_{biased} = \frac{B + F}{A+B + F + G}$$

If we include $Z$, the slope becomes: $$\beta_{unbiased} = \frac{B}{A+B}$$

Since we cannot attribute [*the effect of both* $X$ and $Z$
together]{style="color: #058B8C;"} to one single variable, that part ([section
F]{style="color: #058B8C;"}) is tossed out of the slope calculations.
:::
:::

::: {.column width="50%"}
![](images/corr2.png)
:::
:::

## Statistical Control: Data Example

```{r}
#| echo: false
#| out-width: 100%
library(tidyverse)
library(ggthemes)

# df <- data.frame(W = as.integer((1:200>100))) %>%
#   mutate(X = .5+2*W + rnorm(200)) %>%
#   mutate(Y = -.5*X + 4*W + 1 + rnorm(200),time="1") %>%
#   group_by(W) %>%
#   mutate(mean_X=mean(X),mean_Y=mean(Y)) %>%
#   ungroup()

df <- more_cookies %>% 
  rename(X = cookies,
         W = weekend,
         Y = happiness) %>%
  group_by(W) %>%
  mutate(mean_X=mean(X),mean_Y=mean(Y)) %>%
  ungroup()

#Calculate correlations
before_cor <- paste("Raw data, r = ",round(cor(df$X,df$Y),3),sep='')
after_cor <-
  paste("Estimate unconfounded\nrelationship: ", 
        round(cor(df$X - df$mean_X, df$Y - df$mean_Y), 3), sep = '')

#Add step 2 in which X is demeaned, and 3 in which both X and Y are, and 4 which just changes label
dffull <- rbind(
  #Step 1: Raw data only
  df %>% mutate(
    mean_X = NA,
    mean_Y = NA,
    time = before_cor
  ),
  #Step 2: Add x-lines
  df %>% mutate(mean_Y = NA, 
                time = 'Identify what parts in X\nare explained by W'),
  #Step 3: X de-meaned
  df %>% mutate(
    X = X - mean_X,
    mean_X = 0,
    mean_Y = NA,
    time = "Subtract parts in X\nexplained by W"
  ),
  #   df %>% mutate(
  #   mean_X = NA,
  #   mean_Y = NA,
  #   time = paste("Raw data, r = ",round(cor(df$X,df$Y),3), " ", sep='')
  # ),
  #Step 4: Remove X lines, add Y
  df %>% mutate(
    X = X - mean_X,
    mean_X = NA,
    time = "Identify what parts in Y\nare explained by W"
  ),
  #Step 5: Y de-meaned
  df %>% mutate(
    X = X - mean_X,
    Y = Y - mean_Y,
    mean_X = NA,
    mean_Y = 0,
    time = "Subtract parts in Y\nexplained by W"
  ),
  #Step 6: Raw demeaned data only
  df %>% mutate(
    X = X - mean_X,
    Y = Y - mean_Y,
    mean_X = NA,
    mean_Y = NA,
    time = after_cor
  )
) %>%
  mutate(time = as_factor(time) %>% fct_inorder())


ggplot(dffull, aes(y = Y, x = X, color = as.factor(W))) +
  geom_point(alpha =0.8) +
  geom_vline(aes(xintercept = mean_X, color = as.factor(W)), 
             size = 1.2, alpha = 0.8) +
  geom_hline(aes(yintercept = mean_Y, color = as.factor(W)), 
             size = 1.2, alpha = 0.8) +
  guides(color = guide_legend(title = "W")) +
  scale_color_manual(values = c("#000000", "#800010")) +
  # labs(title = 'The Relationship between Y and X, Controlling for a Binary Variable W') +
  facet_wrap( ~ time) +
  guides(color = F) +
  # coord_cartesian(xlim = c(-0.5, 10), ylim = c(0, 3.5)) +
  scale_x_continuous(breaks = seq(-6, 10, by = 2)) +
  labs(x = "Cookies eaten (X)", y = "Level of happiness (Y)") +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.minor = element_blank(),
    plot.title = element_text(face = "bold")
  ) +
  # ylim(c(-1.2, 4)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 3))


```

<!-- ## Statistical Control in `R` -->

<!-- ::: columns -->

<!-- ::: {.column width="50%"} -->

<!-- # ```{r} -->

<!-- # full_model <- lm(happiness ~ cookies + weekend,  -->

<!-- #                  data = more_cookies)  -->

<!-- # tidy(full_model) -->

<!-- # ggplot(more_cookies, aes( -->

<!-- #   x = cookies, -->

<!-- #   y = happiness, -->

<!-- #   color = as_factor(weekend) -->

<!-- # )) + -->

<!-- #   geom_jitter(width = 0.1) + -->

<!-- #   guides(color = F) + -->

<!-- #   geom_smooth(method = "lm") + -->

<!-- #   scale_color_viridis_d() + -->

<!-- #   labs(x = "Cookies eaten", y = "Level of happiness") + -->

<!-- #   scale_x_continuous(breaks = scales::pretty_breaks(),  -->

<!-- #                      limits = c(0, 10)) -->

<!-- # ``` -->

<!-- :::  -->

<!-- ::: {.column width="50%"} -->

<!-- ## Statistical Control in `R`: Manually  -->

<!-- ```{r} -->

<!-- # Remove the effect of the confounder W (weekend) on X (cookies) -->

<!-- step1 <- lm(cookies ~ weekend, data = more_cookies) -->

<!-- step1$residuals %>% summary() -->

<!-- # Remove the effect of the confounder W (weekend) on Y (happiness) -->

<!-- step2 <- lm(happiness ~ weekend, data = more_cookies) -->

<!-- step2$residuals %>% summary() -->

<!-- # Regress the leftovers of Y on leftovers of X -->

<!-- step3 <- lm(step2$residuals ~ step1$residuals) -->

<!-- tidy(step3) -->

<!-- ``` -->

<!-- :::  -->

<!-- ::: -->

## Predictions (a.k.a. Expected or Fitted Values) {.smaller}

$$\operatorname{\widehat{Happiness}} = 0.163 + 0.154 \cdot \operatorname{Cookies} + 1.315 \cdot \text{Weekend}$$

<br>

Fitted values for Happiness given Weekend $= 1$:

$$\operatorname{\widehat{Happiness}_{Weekend}} = 0.163 + 0.154 \cdot \operatorname{Cookies} + 1.315 \cdot 1 \\ = 1.478 + 0.154\cdot\operatorname{Cookies}$$
Fitted values for Happiness given Weekend $= 0$:

$$\operatorname{\widehat{Happiness}_{Weekday}} = 0.163 + 0.154 \cdot \operatorname{Cookies} + 1.315 \cdot 0 \\ = 0.163 + 0.154\cdot\operatorname{Cookies}$$

Expected value for Happiness given Cookies $= 5$ & Weekend $= 0$:

$$\mathbb{E}[\operatorname{Happiness|Cookies = 5, Weekend = 0}] = 0.163 + 0.154 \cdot 5 + 1.315 \cdot 0 \\ = 0.163 + 0.154\cdot\operatorname{5} = 0.933$$

## Predictions for MLR: Plot

```{r}
#| echo: false
#| out-height: 80%
augment(happiness_m2) %>%
  ggplot(aes(
    x = cookies,
    y = happiness,
    group = weekend,
    color = as.character(weekend)
  )) +
  geom_jitter(
    size = 3,
    width = 0.1,
    alpha = 0.7
  ) +
  labs(x = "Cookies eaten", y = "Level of happiness") +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.minor = element_blank(),
    plot.title = element_text(face = "bold")
  ) +
  geom_smooth(method = "lm", se = F) +
  guides(color = F) +
  scale_color_manual(values = c("#000000", "#800010")) +
  scale_x_continuous(breaks = 0:10) +
  scale_y_continuous(breaks = scales::pretty_breaks()) +
  geom_text(
    data = . %>%
      group_by(weekend) %>% summarise_all(max),
    aes(
      y = (.fitted),
      x = (cookies),
      label = c("Weekday", "Weekend")
    ),
    nudge_x = -0.3,
    nudge_y = 0.3,
    size = 6
  )
```


## Building models in `R`

<br>

##### Base `R` Functions

```{r model-template, eval=FALSE}
name_of_model <- lm(<Y> ~ <X>, data = <DATA>)

summary(name_of_model)  # See model details
```

::: small
<br> Only complete observations are used for estimation: `R` removes rows with
NAs by default <br>
:::

##### Tidy functions from `broom`

```{r broom examples, eval=FALSE}
library(broom)

# Convert model results to a data frame for plotting
tidy(name_of_model)

# Convert model diagnostics to a data frame
glance(name_of_model)

# Add columns of fitted values, residuals, and some other stuff 
augment(name_of_model)
```

# Building Models in R

## Modeling Happiness with Cookies

<br>

```{r}
happiness_model <- lm(happiness ~ cookies, data = cookies_data)
summary(happiness_model)
```


<!-- ```{r} -->
<!-- happines_res <- resid(happiness_model) # residuals  -->
<!-- happiness_fitted <- fitted(happiness_model) # fitted values  -->
<!-- ``` -->

## Modeling Happiness with Cookies: Tidy Functions

<br>

```{r tidy-cookie-model}
tidy(happiness_model, conf.int = TRUE)
```

<br>

```{r glance-cookie-model}
glance(happiness_model)
```

## Main Take Aways 

- Regression model represents our idea of the data-generation process (DAG) in the systematic component, everything else is captured in the error term
- Slope represents the expected change in $y$ in response to one-unit change in $x_k$, and is constant across every value in $x_k$
- The procedure of statistical control ensures that we we only obtain the effects that are uniquely attributed to that explanatory variable 
