{
  "hash": "7e7d3b4c990f4ecf69fc394d3af6d8b8",
  "result": {
    "markdown": "---\ntitle: \"Uncertainty and Statistical Inference\"\nsubtitle: \"R for Data Analysis<br>Session 10\"\ntitle-slide-attributes:\n  data-background-size: stretch\n  data-slide-number: none\nauto-stretch: false\ninstitute: \"University of Mannheim<br>Fall 2023\"\nauthor: \"Viktoriia Semenova\"\nfooter: \"[ðŸ”— r4da.live](https://r4da.live/)\"\nlogo: images/logo.png\nformat:\n  revealjs:\n    theme: slides.scss\n    transition: fade\n    incremental: true   \n    slide-number: true\n    chalkboard: true\nexecute:\n  echo: true\neditor_options: \n  chunk_output_type: console\neditor: \n  markdown: \n    wrap: 80\n---\n\n\n\n\n# Uncertainty\n\n## Statistical Inference {.smaller}\n\n... is the process of using sample data to make conclusions about the underlying\npopulation the sample came from\n\n\n```{dot}\n//| echo: false\ndigraph G {\n  forcelabels=true;\n  splines=false;\n  fontname=\"AtkinsonHyperlegible-Regular\";\n  population [\n    label = \"Population\";\n    shape = circle; \n  ];\n  sample [\n    label = \"Sample\";\n    shape = oval;\n  ];\n\n  population -> sample  [xlabel = <<font color=\"grey\">      Probability</font>>;  color = \"grey\"];\n  sample -> population [xlabel = \"   Inference\";];\n  \n  {rank = same population; sample}\n}\n```\n\n\n<!-- ## Sampling in Real Life {.smaller} -->\n\n<!-- ![](https://sta210-s22.github.io/website/slides/images/lec-5/soup.png){width=\"365\"} -->\n\n<!-- -   When you taste a spoonful of soup and decide the spoonful you tasted isn't -->\n\n<!--     salty enough, that's exploratory analysis -->\n\n<!-- -   If you generalize and conclude that your entire soup needs salt, that's an     inference -->\n\n<!-- -   For your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population) -->\n\n## Why Communicate Uncertainty {.smaller}\n\n> If you want to estimate a population parameter, do you prefer to report a\n> range of values the parameter might be in, or a single value?\n\n::: columns\n::: {.column width=\"50%\"}\nIf we report a point estimate, we probably won't hit the exact population\nparameter\n\n![](images/darts.png){width=\"300\"}\n:::\n\n::: {.column width=\"50%\"}\nIf we report a range of plausible values, we have a good shot at capturing the\nparameter\n\n![](images/rings.png){width=\"300\"}\n:::\n:::\n\n## Why Communicate Uncertainty\n\n![](https://media.eagereyes.org/wp-content/uploads/2017/09/irma-noaa-cone.png){width=\"732\"}\n\n## How to Communicate Uncertainty {.smaller}\n\n-   *Standard error:* since point estimates vary from sample to sample, we\n    quantify this variability with what is called the *standard error* (SE)\n    *Standard error* is a *standard deviation* of the sampling distribution of a\n    statistic\n\n$$\nSE(\\hat\\beta_1)= \\sqrt{Var(\\hat{\\beta_1})}, \\qquad Var(\\hat{\\beta_1}) = \\frac{\\overbrace{\\hat\\sigma^2}^{\\text{estimated regression error}^2\\\\\\text{a.k.a. residual variance}}}{\\underbrace{\\sum_{i=1}^{n}(x_i - \\bar x)^2}_{\\text{sum of squared deviations of } x}}\n$$\n\n-   *Confidence interval:* a **range** of our best guesses about the point\n    estimate. One way to calculate it is using the *standard error*\n\n> We call a confidence interval a $(1 - \\alpha)$% confidence interval if it is\n> constructed such that it contains the true parameter at least $(1 - \\alpha)$%\n> of the time if we repeat the experiment a large number of times.\n\n## Thought Experiment {.smaller}\n\nSuppose we had data for the whole population (say 100'000 students) and we could\nestimate the true parameter values:\n\n$$\n\\text{Evaluations} = \\beta_0 + \\beta_1 \\cdot \\text{Beauty Score} + \\varepsilon\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nCompare these values to estimates from a random sample of 500 students:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n|term        | parameter| estimate|\n|:-----------|---------:|--------:|\n|(Intercept) |     3.691|    3.756|\n|beauty      |     0.070|    0.061|\n:::\n:::\n\n\n## Sampling Variability {.smaller}\n\nNow let's take 5000 samples (N = 1000) from this population of 100'000 students,\nrun the same bivariate model on each of these samples, and plot the results:\n\n\n::: {.cell layout-align=\"center\" hash='10-slides_cache/revealjs/unnamed-chunk-4_d6b23dd60cdc08896cf34cbfd117d3de'}\n\n:::\n\n\n<!-- - Sampling distributions are not observed in real life  -->\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=2100}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=2100}\n:::\n:::\n\n:::\n:::\n\n## Sampling Distributions {.smaller}\n\n-   Sampling distributions are *hypothetical* constructs, we never observe them\n    in real life\n\n-   Sampling distributions (of most statistics) are approximately normally\n    distributed (if the sample size is sufficiently large)\n\n-   Sampling distributions are centered at the true value of the population\n    coefficient (the value we would get from linear modeling if we did indeed\n    use the full population)\n\n-   The spread of the sampling distribution gives us a measure of the\n    *precision* of our estimate:\n\n    -   If the sampling distribution is very wide, then our estimate is\n        imprecise; our estimate would vary widely from sample to sample\n    -   If the sampling distribution is very narrow, then our estimate is\n        precise; our estimate would not vary much from sample to sample\n\n## Standard Error {.smaller}\n\nStandard Error is a Standard Deviation of Sampling Distribution of that\nStatistic:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# SD of sampling distributions\nestimates %>% \n  group_by(term) %>% \n  summarize(sd = sd(estimate))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 2\n  term            sd\n  <chr>        <dbl>\n1 (Intercept) 0.0544\n2 beauty      0.0114\n```\n:::\n\n```{.r .cell-code}\n# estimates SEs  \nestimates %>% \n  filter(sample == 10) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 9\n  term        estimate std.error statistic  p.value conf.low conf.high n     sample\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl> <chr>  <int>\n1 (Intercept)   3.69      0.0522     70.7  0          3.59      3.79   1000      10\n2 beauty        0.0735    0.0114      6.43 1.94e-10   0.0511    0.0959 1000      10\n```\n:::\n\n```{.r .cell-code}\n# estimates SEs  \nestimates %>% \n  filter(sample == 1000) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 9\n  term        estimate std.error statistic  p.value conf.low conf.high n     sample\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl> <chr>  <int>\n1 (Intercept)   3.66      0.0507     72.1  0          3.56      3.76   1000    1000\n2 beauty        0.0771    0.0106      7.24 8.73e-13   0.0562    0.0980 1000    1000\n```\n:::\n:::\n\n\n> If we took samples of N = 100 and not N = 1000, what should happen to the\n> spread of sampling distributions?\n\n## Sample Size and Precision {.smaller}\n\nLarger samples allow for more precise estimates (i.e. smaller standard errors)\n\n\n::: {.cell layout-align=\"center\" hash='10-slides_cache/revealjs/unnamed-chunk-8_2ae2abb4bbb62b5519fa242548bbed8c'}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n> We cannot observe these distributions, but the SE provides us with the\n> information about their variability\\\n> We can use this info to obtain a plausible range of estimates given our data,\n> the confidence interval\n\n## Confidence Intervals {.smaller}\n\n-   Range of our best guesses about the point estimate with X% confidence\n    (accounting for sampling variability)\n-   If we have a large sample, we can estimate CIs from standard errors and\n    quantiles for the standard normal distribution\n    $\\mathcal{N}(\\mu = 0, \\sigma = 1)$: $100 \\cdot (1 - \\alpha)$% confidence\n    interval is\n    $CI_{(1 - \\alpha)} = \\hat \\beta \\pm \\underbrace{z_{\\alpha / 2} \\cdot SE}_{\\text{Margin of Error}}$\n\n$$CI_{95} = \\hat \\beta \\pm 1.96 SE = [ \\hat \\beta - 1.96 SE;  \\hat\\beta + 1.96 SE]$$\n\n-   The procedure of how 95% confidence intervals are constructed ensures that:\n    -   when we draw *repeated samples*\n    -   95% of CIs calculated with this formula *on the respective samples*\n    -   cover the true parameter\n-   one single calculated CI tells us *nothing* about:\n    -   parameters in other samples\n    -   individual observations in our sample and/or population\n\n<!-- > The confidence level does not mean that there is 95% probability of each CI including true value. Confidence level is a statement about the procedure in general, not each individual interval. -->\n\n## Confidence Intervals Illustration\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=2100}\n:::\n:::\n\n\n## Confidence Intervals as Ring Toss {.smaller}\n\n::: columns\n::: {.column width=\"70%\"}\n-   Each sample gives a different CI or toss of the ring\n-   Some samples the ring will contain the target (the CI will contain the\n    truth) other times it won't\n    -   We don't know if the CI for our sample contains the truth!\n-   *Confidence level*: percent of the time our CI will contain the population\n    parameter\n    -   Number of ring tosses that will hit the target.\n    -   We get to choose, but typical values are 90%, 95%, and 99%\n-   The confidence level of a CI determine how often the CI will be wrong\n\n> The confidence level does not mean that there is 95% probability of each CI\n> including true value. Confidence level is a statement about the procedure in\n> general, not each individual interval.\n:::\n\n::: {.column width=\"30%\"}\n![](images/rings.png)\n:::\n:::\n\n## Communicating Uncertainty: Examples {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsample_est <- students_population %>%\n  slice_sample(n = 500) %>%\n  lm(eval ~ beauty, data = .) %>%\n  tidy(conf.int = T, conf.level = 0.95) %T>% \n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)   3.71      0.0710     52.3  1.49e-204   3.58      3.85  \n2 beauty        0.0681    0.0154      4.43 1.14e-  5   0.0379    0.0983\n```\n:::\n:::\n\n\n> 95% confidence interval for the effect of *beauty* ranges from\n> 0.04 to\n> 0.1.\\\n> We are 95% confident that the effect of *beauty* on course eevaluations ranges\n> from, on average, 0.04 to\n> 0.1.\\\n> We are 95% confident that for each additional point in *beauty* score, we\n> would expect course evaluations to increase by\n> 0.04 to\n> 0.1 points, on average.\\\n> ~~With 95% probability the expected increase in course evaluataions ranges\n> between 0.04 to\n> 0.1 points for each additional unit of\n> beauty score.~~\n\n## Communicating Uncertainty {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=2100}\n:::\n:::\n\n\n## Accuracy vs. Precision Trade-off {.smaller}\n\n-   By design, confidence intervals of different levels vary in their width:\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(sample_m, conf.int = T, conf.level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)   3.62      0.0707     51.2  1.87e-200   3.48       3.76 \n2 beauty        0.0883    0.0150      5.88 7.53e-  9   0.0588     0.118\n```\n:::\n:::\n\n\n::: \n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(sample_m, conf.int = T, conf.level = 0.99)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)   3.62      0.0707     51.2  1.87e-200   3.43       3.80 \n2 beauty        0.0883    0.0150      5.88 7.53e-  9   0.0495     0.127\n```\n:::\n:::\n\n\n::: \n::: \n\n-   This relates to the precision vs. accuracy of our interval estimates: the\n    higher the level we choose, the more certain we will be to cover the true\n    value\n-   But we also loose the precision of the estimate: wider ranges can become\n    rather uninformative\n\n::: center\n![](images/download.png){width=\"500\"}\n:::\n\n<!-- ```{r} -->\n\n<!-- sample_m %>%  -->\n\n<!--   tidy(conf.int = T, conf.level = 1)  -->\n\n<!-- ``` -->\n\n# Hypothesis Testing\n\n## Decision Based on Confidence Intervals {.smaller}\n\n> Do the data provide sufficient evidence that Î²1 (the true slope for the\n> population) is different from 0?\n\n<!-- - We start with our estimates and compare them to the value we are interested in. -->\n\n-   Suppose we want to know if there is a linear effect of *beauty* on\n    *evaluations* ($H_A: \\hat\\beta_1 \\neq 0$) or if there is no linear effect\n    ($H_0: \\hat\\beta_1 = 0$)\n\n#### Workflow:\n\n1.  Calculate the standard error and confidence interval (e.g., 95% CI)\n2.  Check if the confidence interval covers zero or not.\n\n-   If CI does not cover zero, we conclude that with 95% confidence, the slope\n    coefficient is different from zero\n-   If CI covers zero, we conclude that with 95% confidence, the slope\n    coefficient is not different from zero\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsample_est\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)   3.71      0.0710     52.3  1.49e-204   3.58      3.85  \n2 beauty        0.0681    0.0154      4.43 1.14e-  5   0.0379    0.0983\n```\n:::\n:::\n\n\n## Hypothesis Testing with *p-values* {.smaller}\n\n-   Statistical hypothesis testing is a thought experiment: *What would the\n    world look like if we knew the truth?*\n\n> Do the data provide sufficient evidence that Î²1 (the true slope for the\n> population) is different from 0?\n\n#### Workflow:\n\n1.  Start with a null hypothesis, $H_0$ that represents the \"null-world\"\n2.  Set an alternative hypothesis, $H_A$ that represents the research question,\n    i.e. what we're testing for\n3.  Conduct a hypothesis test under the assumption that the null hypothesis is\n    true and calculate a p-value (probability of observed or more extreme\n    outcome given that the null hypothesis is true)\n    -   if the test results suggest that the data do not provide convincing\n        evidence for the alternative hypothesis, stick with the null hypothesis\n    -   if they do, then reject the null hypothesis in favor of the alternative\n\n## Course Evals of English Natives vs. Non-natives {.smaller}\n\n> Are courses taught by English natives evaluated higher than courses of\n> non-natives?\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevals %>% \n  group_by(nonenglish) %>% \n  summarize(avg_eval = mean(eval)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 2\n  nonenglish avg_eval\n       <dbl>    <dbl>\n1          0     4.02\n2          1     3.69\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(eval ~ nonenglish, evals) %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    4.02     0.0264    152.   0      \n2 nonenglish    -0.329    0.107      -3.07 0.00229\n```\n:::\n:::\n\n:::\n:::\n\n$$\\hat\\beta_1 = 3.69 - 4.02 = - 0.33$$\n\n## Null Words with Permutation {.smaller}\n\n> How would the effect $\\hat\\beta_1$ look like if there was no difference?\n\n-   Shuffle the `eval` and `nonenglish` columns and calculate the difference in\n    evaluations between natives and non-natives\n-   Repeat re-shuffling and estimation of the differences â†‘ 1000 times\n-   Check $\\hat\\beta_1$ in the null world: what is the probability of observing\n    data as or more extreme as our data under the null (i.e. the *p-value*)?\n\n## Null Words with Permutation: Two-sided p-value {.smaller}\n\n\n::: {.cell layout-align=\"center\" hash='10-slides_cache/revealjs/unnamed-chunk-18_d5686131da6eba38c7f9095b4eb6655a'}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Null Words with Permutation: Slope Example {.smaller}\n\nRepeated permutations allow for quantifying the variability in the slope under\nthe condition that there is no linear relationship (i.e., that the null\nhypothesis is true)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Communicating p-values {.smaller}\n\n-   By default, p-values are about comparing to zero, i.e. no difference between\n    groups with and without $X$\n\n> \"the effect of $X$ is not statistically significant/discernible (p = 0.3)\"\n> \"the effect of $X$ is not distinguishable from zero at 0.05 level\" \"the effect\n> of $X$ is not significantly distinguishable from zero (p = 0.3)\" \"X is a\n> significant/discernible predictor of Y (p â‰ˆ 0.002)\"\n\n-   Every coefficient is going to be *significant*, some at 0.05 level, some at\n    0.85 level $\\Rightarrow$ saying something is significant without specifying\n    the level is not enough\\\n-   In general, report corresponding p-value in parenthesis to just one\n    significant digit: 0.002 rather than 0.0017\n\n## Main Take Aways {.smaller}\n\n-   Sampling variability implies uncertainty related to the estimation process:\n    more data leads to more precise estimates (i.e. smaller SEs, narrower CIs)\n-   Confidence intervals quantify the uncertainty associated with the *average\n    outcome*, not each individual prediction. Confidence level indicates how\n    often, in the long run, the CI would be wrong (i.e. not contain the true\n    value). We cannot know if the CI we obtained is a \"good\" or a \"bad\" one,\n    though.\\\n-   With hypothesis testing, we are comparing the null-world to our estimates.\n    $p$-values indicate the probability of us observing the data at least as\n    extreme as we have in the null-world\n\n# Appendix\n\n## Probabilistic Interpretation of CIs {.smaller}\n\n$$CI_{95}: [\\bar x - 1.96 SE, \\bar x + 1.96 SE]$$\n\n-   Randomness comes from the stage of drawing a sample (we only have one, but\n    hypothetically, we draw them repeatedly)\n\n-   **After we draw the random sample**, calculating the CI is a matter of\n    procedure:\n\n    -   there is no more randomness, it's just applying the formula\n    -   hence, we can think of this as a realized experiment\n    -   if CI bounds are just numbers, the true fixed value is either inside (1)\n        or not (0)\n    -   we say: a single *calculated* CI contains the true parameter or not (and\n        in real life, we don't know if it does, so we hope it is one of the ones\n        that cover the true value)\n\n-   **Before the random sample is drawn**, we can apply probabilistic\n    interpretation to CI:\n\n    -   for a 95% confidence interval (*before* the sample for calculating that\n        CI is drawn!), there is a 95% chance that a CI will contain $\\mu$\n    -   the **random** interval $[\\bar x - 1.96 SE, \\bar x + 1.96 SE]$ contains\n        $\\mu$ with probability 0.95. It is a random interval, since the\n        endpoints change with different samples (i.e., we don't have not drawn\n        the sample yet)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}