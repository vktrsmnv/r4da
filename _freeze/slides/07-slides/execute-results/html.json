{
  "hash": "edf9744d02054782bad2da7fe199fc2c",
  "result": {
    "markdown": "---\ntitle: \"Linear Regression\"\nsubtitle: \"R for Data Analysis<br>Session 7\"\ntitle-slide-attributes:\n  data-background-size: stretch\n  data-slide-number: none\nauto-stretch: false\ninstitute: \"University of Mannheim<br>Fall 2023\"\nauthor: \"Viktoriia Semenova\"\nfooter: \"[🔗 r4da.netlify.app](https://r4da.netlify.app/)\"\nlogo: images/logo.png\nformat:\n  revealjs:\n    theme: slides.scss\n    transition: fade\n    incremental: true   \n    slide-number: true\n    chalkboard: true\neditor: visual\nexecute:\n  echo: true\n---\n\n\n\n\n# Intro\n\n## Agenda for Today\n\n<br>\n\n**Fitting the line with OLS**\n\n<br>\n\n**Interpretation of Regression Coefficients**\n\n<!-- ## Cookies and Happiness -->\n\n\n\n\n\n\n\n## Cookies and Happiness: OLS Regression Line {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slides_files/figure-revealjs/cookies-lm-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Regression Line Anatomy {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slides_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n# Fitting and Interpreting Models\n\n<!-- ## Workflow -->\n\n<!-- <br> -->\n\n<!-- ```{dot} -->\n\n<!-- //| echo: false -->\n\n<!-- digraph G { -->\n\n<!--   forcelabels=true; -->\n\n<!--   splines=false; -->\n\n<!--   fontname=\"AtkinsonHyperlegible-Regular\"; -->\n\n<!--   causal [ -->\n\n<!--     label = \"Causal Estimand\"; -->\n\n<!--     shape = rect;  -->\n\n<!--     group=a; -->\n\n<!--   ]; -->\n\n<!--   stats [ -->\n\n<!--     label = \"Statistical Estimand\"; -->\n\n<!--     shape = rect; -->\n\n<!--      group=a; -->\n\n<!--   ]; -->\n\n<!--   est [ -->\n\n<!--     label = \"Estimate\"; -->\n\n<!--     shape = rect; -->\n\n<!--      group=a; -->\n\n<!--   ]; -->\n\n<!--   mod [ -->\n\n<!--     label = \"Causal Model\"; -->\n\n<!--     shape = oval; -->\n\n<!--      group=b; -->\n\n<!--   ]; -->\n\n<!--   data [ -->\n\n<!--     label = \"Data\"; -->\n\n<!--     shape = oval; -->\n\n<!--     group=b; -->\n\n<!--   ]; -->\n\n<!--   causal -> stats [xlabel = \"Identification    \" ]; -->\n\n<!--   stats -> est [ xlabel = \"Estimation   \"; dir = \"backward\"]; -->\n\n<!--   mod -> stats; -->\n\n<!--   data -> est; -->\n\n<!-- { -->\n\n<!--     rank=same; -->\n\n<!--     causal; mod;  -->\n\n<!--   } -->\n\n<!--   { -->\n\n<!--     rank=same; -->\n\n<!--     stats; data;  -->\n\n<!--   } -->\n\n<!-- } -->\n\n<!-- ``` -->\n\n## Language of Models {.smaller}\n\n-   **True Model** $y = \\underbrace{\\beta_0}_{\\text{intercept}} + \\underbrace{\\beta_1}_{\\text{slope}} x + \\underbrace{\\varepsilon}_{\\text{error}}$\n    -   Population parameters $\\beta$: truth (estimand), unknown to us\n-   **Estimated Model** $y = \\underbrace{\\hat\\beta_0}_{\\text{intercept}} + \\underbrace{\\hat\\beta_1}_{\\text{slope}} x + \\underbrace{\\hat\\varepsilon}_{\\text{residual}}$\n    -   Estimates $\\hat{\\beta}$: our best guess about the estimand given the data\n-   A model has two parts:\n    -   **Systematic Component** of a linear model: $\\underbrace{\\hat y}_{\\text{fitted}\\\\\\text{value}} = \\underbrace{\\hat\\beta_0}_{\\text{intercept}} + \\underbrace{\\hat\\beta_1}_{\\text{slope}} x$\n    -   **Stochastic Component** of a linear model: $\\hat\\varepsilon$\n\n## Vocabulary\n\n$$\ny = \\hat \\beta_0 + \\hat \\beta_1 x_1 + \\hat \\varepsilon\n$$\n\n-   $y$: dependent variable, outcome\n-   $x$: independent variable, treatment, explanatory variable, treatment, predictor, feature\n-   $\\hat y$: predicted values of y, y-hat, fitted values, regression line\\\n-   $\\hat \\beta_0$: intercept, prediction when all $x=0$, constant\n-   $\\hat \\beta_k$: slope, the effect of $k$-th variable\n\n<!-- ## Modeling Happiness with Cookies -->\n\n<!-- $$ -->\n\n<!-- y = \\hat \\beta_0 + \\hat \\beta_1 x_1 + \\hat \\varepsilon -->\n\n<!-- $$ -->\n\n<!-- $$ -->\n\n<!-- \\begin{aligned} -->\n\n<!-- &{\\text{Happiness}} =  \\hat \\beta_0 +  \\hat \\beta_1 \\text{Cookies} + \\hat \\varepsilon -->\n\n<!-- \\end{aligned} -->\n\n<!-- $$ -->\n\n<!-- $$ -->\n\n<!-- \\begin{aligned} -->\n\n<!-- &\\widehat{\\text{Happiness}} =  \\hat \\beta_0 +  \\hat \\beta_1 \\text{Cookies}  -->\n\n<!-- \\end{aligned} -->\n\n<!-- $$ -->\n\n<!-- :::task -->\n\n<!-- What are the differences between equations? -->\n\n<!-- ::: -->\n\n## Interpreting Slope Coefficient Poll {.normal}\n\n$$\n\\operatorname{\\widehat{Happiness}} = 1.16 + 0.155(\\operatorname{Number\\ of\\ Cookies\\ Eaten})\n$$\n\n\nThe slope of the model for predicting happiness score from number of consumed cookies is 0.155. Which of the following is the best interpretation of this value?\n\n1.  For every additional cookie eaten, the happiness score goes up by 0.155 points, on average.\n2.  For every additional cookie eaten, we expect the happiness score to be higher by 0.155 points, on average.\n3.  For every additional cookie eaten, the happiness score goes up by 0.155 points.\n4.  For every one point increase in happiness score, the number of cookies eaten goes up by 0.155 points, on average.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_26079203\" data-update-every=\"1\" tabindex=\"0\" style=\"top:0;right:0;font-size:2;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">01</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">30</span></code>\n</div>\n```\n:::\n:::\n\n\n## Interpreting Slope and Intercept\n\n::: small\n$$\n\\operatorname{\\widehat{Happiness}} = 1.16 + 0.155(\\operatorname{Number\\ of\\ Cookies\\ Eaten})\n$$\n\n\n> Slope: For every additional cookie eaten, we expect the happiness score to be higher by 0.155 points, on average.\n\n-   Each additional cookie has the same effect on happiness, i.e. *marginal effect* is constant\n    -   Associated increase in happiness is 0.155 for the first and, say, tenth cookie\n\n> Intercept: If the number of eaten cookies is 0, we expect the happiness score to be 1.16 points.\n\n-   Intercept is meaningful in the context of data because the predictor can feasibly take values equal to or near zero\n:::\n\n# Mechanics of Linear Regression\n\n<!-- ## Errors/Residuals in Regression {.smaller} -->\n\n<!-- - Random noise, the *stochastic* component of the model: the sum of “everything else” not in the systematic component of the model -->\n\n<!-- <!-- - We assume errors to be, on average, zero given every value of X: $\\mathbb{E}(\\varepsilon|X) = 0$ -->\n\n<!-- ```{r} -->\n\n<!-- #| echo: false -->\n\n<!-- #| out-height: 90% -->\n\n<!-- m_cookies <- lm(happiness ~ cookies, data = cookies) -->\n\n<!-- cookies_fitted <- augment(m_cookies)  -->\n\n<!-- ggplot( -->\n\n<!--   cookies_fitted, -->\n\n<!--   aes(y = .resid, x = cookies) -->\n\n<!-- ) + -->\n\n<!--   geom_point() + -->\n\n<!--   geom_hline( -->\n\n<!--     yintercept = 0, -->\n\n<!--     linetype = \"dashed\" -->\n\n<!--   ) + -->\n\n<!--   labs( -->\n\n<!--     x = \"Cookies eaten\", -->\n\n<!--     y = \"Residuals\" -->\n\n<!--   ) -->\n\n<!-- ``` -->\n\n<!-- ## Regression Standard Error -->\n\n<!-- Once we fit the model, we can use the residuals to estimate error variance (i.e. residual variance): -->\n\n<!-- $$ -->\n\n<!-- \\hat\\sigma^2 = \\frac{\\overbrace{\\sum_{i=1}^{n}\\hat{e}_i^2}^{\\text{sum of squared residuals}}}{ -->\n\n<!-- \\underbrace{ -->\n\n<!-- \\underbrace{n}_{\\text{number of}\\\\\\text{observations}}-\\underbrace{k}_{\\text{number of}\\\\\\text{covariates}} - 1 -->\n\n<!-- }_{\\text{degrees of freedom}} -->\n\n<!-- } -->\n\n<!-- $$ -->\n\n<!-- Regression standard error  $\\sqrt{\\hat\\sigma^2}$: -->\n\n<!-- -  A measure of the average error (average difference between observed and  predicted values of the outcome) in same units as the outcome variable -->\n\n<!-- $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$) -->\n\n<!-- ```{r, echo=FALSE} -->\n\n<!-- #| out-height: 80% -->\n\n<!-- set.seed(0) -->\n\n<!-- dat <- data.frame(x=(x=runif(10000, 0, 50)), -->\n\n<!--                   y=rnorm(10000, 10*x, 100)) -->\n\n<!-- ## breaks: where you want to compute densities -->\n\n<!-- breaks <- seq(0, max(dat$x), len=5) -->\n\n<!-- dat$section <- cut(dat$x, breaks) -->\n\n<!-- ## Get the residuals -->\n\n<!-- dat$res <- residuals(lm(y ~ x, data=dat)) -->\n\n<!-- ## Compute densities for each section, flip the axes, add means of sections -->\n\n<!-- ## Note: densities need to be scaled in relation to section size (2000 here) -->\n\n<!-- dens <- do.call(rbind, lapply(split(dat, dat$section), function(x) { -->\n\n<!--   d <- density(x$res, n=5000) -->\n\n<!--   res <- data.frame(x=max(x$x)- d$y*2000, y=d$x+mean(x$y)) -->\n\n<!--   res <- res[order(res$y), ] -->\n\n<!--   ## Get some data for normal lines as well -->\n\n<!--   xs <- seq(min(x$res), max(x$res), len=5000) -->\n\n<!--   res <- rbind(res, data.frame(y=xs , -->\n\n<!--                                x=max(x$x) - 2000*dnorm(xs, 0, sd(x$res)))) -->\n\n<!--   res$type <- rep(c(\"empirical\", \"normal\"), each=5000) -->\n\n<!--   res -->\n\n<!-- })) -->\n\n<!-- dens$section <- rep(levels(dat$section), each=10000) -->\n\n<!-- dat$res <- residuals(lm(y ~ x, data=dat)) -->\n\n<!-- ggplot(dat, aes(x, res)) + -->\n\n<!--   geom_point(size = 0.1, alpha = 0.25) + -->\n\n<!--   geom_hline(yintercept = 0) + -->\n\n<!--   geom_path( -->\n\n<!--     data = dens[dens$type == \"normal\", ], -->\n\n<!--     aes(x, y, group = section), -->\n\n<!--     color = \"#003056\", -->\n\n<!--     lwd = 0.8 -->\n\n<!--   ) + -->\n\n<!--   theme(axis.text.x = element_blank()) + -->\n\n<!--   scale_y_continuous(breaks = 0) + -->\n\n<!--     labs(y = \"Residuals\", -->\n\n<!--        x = \"X\")  -->\n\n<!-- ``` -->\n\n## Explained vs. Unexplained Variation in Y\n\n::: r-stack\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slides_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: fragment\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slides_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: fragment\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slides_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: fragment\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slides_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n\n\n\n\n## Fitting Line Example I {.smaller}\n\n$$\n\\begin{aligned}\n&\\widehat{\\text{Happiness}} =  2 - 0.1 \\cdot \\text{Cookies} \n\\end{aligned}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slides_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=1950}\n:::\n:::\n\n\n## Fitting Line Example II {.smaller}\n\n$$\n\\begin{aligned}\n&\\widehat{\\text{Happiness}} =  2 + 0 \\cdot \\text{Cookies} = \\overline{\\text{Happiness}} \n\\end{aligned}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slides_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=1950}\n:::\n:::\n\n\n## Fitting Line Example III {.smaller}\n\n$$\n\\begin{aligned}\n&\\widehat{\\text{Happiness}} = 1.1 + 0.1 \\cdot \\text{Cookies}\n\\end{aligned}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slides_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=1950}\n:::\n:::\n\n\n## Fitting Line Example IV: OLS Solution {.smaller}\n\n$$\n\\begin{aligned}\n&\\widehat{\\text{Happiness}} =   1.16 + 0.155 \\cdot \\text{Cookies} \n\\end{aligned}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-slides_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=1950}\n:::\n:::\n\n\n## Explained vs. Unexplained Variation in Y\n\n::: smaller\n**Explained Variance (Sum of Squares):** $$ESS = \\sum^{n}_{i=1}(\\hat y_i - \\bar y)^2$$\n\n**Sum of Squared Residuals:** $$RSS = \\sum^n_{i=1}\\hat{\\varepsilon_i}^2 = \\sum^n_{i=1}(y_i - \\hat{y_i})^2$$\n\n**Total Sum of Squares:** $$TSS = \\sum^{n}_{i=1}(y_i - \\bar y)^2 = ESS + RSS$$\n:::\n\n## What Are Coefficient Values in OLS {.small}\n\n::: columns\n::: {.column width=\"50%\"}\n$$\\text{Sum of Squared Residuals (SSR)} \\\\= \\sum^n_{i=1}\\hat{\\varepsilon_i}^2 \\\\ = \\sum^n_{i=1}(y_i - \\hat{y_i})^2 \\\\ = \\sum^n_{i=1}(y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)^2$$\n\n-   OLS estimator finds values of $\\hat\\beta$ which minimize $SSR$, the unexplained variance\\\n-   We use differential calculus to find these values of $\\hat{\\beta}$ ([full derivation](https://r4da.live/resource/slr.html))\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/coefs.png){fig-align='center' width=100% height=100%}\n:::\n:::\n\n:::\n:::\n\n<!-- ## Regression Standard Error -->\n\n<!-- Once we fit the model, we can use the residuals to estimate error variance (i.e. residual variance): -->\n\n<!-- $$ -->\n\n<!-- \\hat\\sigma^2 = \\frac{\\overbrace{\\sum_{i=1}^{n}\\hat{e}_i^2}^{\\text{sum of squared residuals}}}{ -->\n\n<!-- \\underbrace{ -->\n\n<!-- \\underbrace{n}_{\\text{number of}\\\\\\text{observations}}-\\underbrace{k}_{\\text{number of}\\\\\\text{covariates}} - 1 -->\n\n<!-- }_{\\text{degrees of freedom}} -->\n\n<!-- } -->\n\n<!-- $$ -->\n\n<!-- Regression standard error  $\\sqrt{\\hat\\sigma^2}$: -->\n\n<!-- -  A measure of the average error (average difference between observed and  predicted values of the outcome) in same units as the outcome variable -->\n\n<!-- $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$) -->\n\n<!-- ```{r, echo=FALSE} -->\n\n<!-- #| out-height: 80% -->\n\n<!-- set.seed(0) -->\n\n<!-- dat <- data.frame(x=(x=runif(10000, 0, 50)), -->\n\n<!--                   y=rnorm(10000, 10*x, 100)) -->\n\n<!-- ## breaks: where you want to compute densities -->\n\n<!-- breaks <- seq(0, max(dat$x), len=5) -->\n\n<!-- dat$section <- cut(dat$x, breaks) -->\n\n<!-- ## Get the residuals -->\n\n<!-- dat$res <- residuals(lm(y ~ x, data=dat)) -->\n\n<!-- ## Compute densities for each section, flip the axes, add means of sections -->\n\n<!-- ## Note: densities need to be scaled in relation to section size (2000 here) -->\n\n<!-- dens <- do.call(rbind, lapply(split(dat, dat$section), function(x) { -->\n\n<!--   d <- density(x$res, n=5000) -->\n\n<!--   res <- data.frame(x=max(x$x)- d$y*2000, y=d$x+mean(x$y)) -->\n\n<!--   res <- res[order(res$y), ] -->\n\n<!--   ## Get some data for normal lines as well -->\n\n<!--   xs <- seq(min(x$res), max(x$res), len=5000) -->\n\n<!--   res <- rbind(res, data.frame(y=xs , -->\n\n<!--                                x=max(x$x) - 2000*dnorm(xs, 0, sd(x$res)))) -->\n\n<!--   res$type <- rep(c(\"empirical\", \"normal\"), each=5000) -->\n\n<!--   res -->\n\n<!-- })) -->\n\n<!-- dens$section <- rep(levels(dat$section), each=10000) -->\n\n<!-- dat$res <- residuals(lm(y ~ x, data=dat)) -->\n\n<!-- ggplot(dat, aes(x, res)) + -->\n\n<!--   geom_point(size = 0.1, alpha = 0.25) + -->\n\n<!--   geom_hline(yintercept = 0) + -->\n\n<!--   geom_path( -->\n\n<!--     data = dens[dens$type == \"normal\", ], -->\n\n<!--     aes(x, y, group = section), -->\n\n<!--     color = \"#003056\", -->\n\n<!--     lwd = 0.8 -->\n\n<!--   ) + -->\n\n<!--   theme(axis.text.x = element_blank()) + -->\n\n<!--   scale_y_continuous(breaks = 0) + -->\n\n<!--     labs(y = \"Residuals\", -->\n\n<!--        x = \"X\")  -->\n\n<!-- ``` -->\n\n## Properties of Least Squares Regression\n\n::: smaller\n-   The regression line goes through the center of mass point, the coordinates corresponding to average $X$ and average $Y$, $(\\bar{X}, \\bar{Y})$:\n\n$$\\bar{Y} = \\hat \\beta_0 + \\hat \\beta_1 \\bar{X} ~ \\rightarrow ~ \\hat \\beta_0 = \\bar{y} - \\hat \\beta_1 \\bar{x}$$\n\n-   The slope has the same sign as the correlation coefficient: $\\beta_X = Corr(X,Y) \\dfrac{{\\sigma_Y}}{{\\sigma_X}}$\n\n-   The sum of the residuals is zero (by design): $\\sum_{i = 1}^n e_i = 0$\n:::\n",
    "supporting": [
      "07-slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}