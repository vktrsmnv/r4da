{
  "hash": "59d3024e92587c9bec0d35cfcdce7d77",
  "result": {
    "markdown": "---\ntitle: \"Matching and inverse probability weighting\"\n---\n\n\n\n\n## Video walk-through\n\nIf you want to follow along with this example, you can download the data below:\n\n- [{{< fa table >}} `mosquito_nets.csv`](/files/data/generated_data/mosquito_nets.csv)\n\n\n::: {.cell layout-align=\"center\"}\nThere's a set of videos that walks through each section below. To make it easier for you to jump around the video examples, I cut the long video into smaller pieces and included them all in [one YouTube playlist](https://www.youtube.com/playlist?list=PLS6tnpTr39sGv7RPHEZy8CDSdCliQOW8K).\n\n- [Drawing a DAG](https://www.youtube.com/watch?v=OoYgOwxUQgI&list=PLS6tnpTr39sGv7RPHEZy8CDSdCliQOW8K)\n- [Creating an RStudio project](https://www.youtube.com/watch?v=_FNrfFe99R0&list=PLS6tnpTr39sGv7RPHEZy8CDSdCliQOW8K)\n- [Naive (and wrong!) estimate](https://www.youtube.com/watch?v=7NW7GbO44BY&list=PLS6tnpTr39sGv7RPHEZy8CDSdCliQOW8K)\n- [Matching](https://www.youtube.com/watch?v=uGwSRnET8Sg&list=PLS6tnpTr39sGv7RPHEZy8CDSdCliQOW8K)\n- [Inverse probability weighting](https://www.youtube.com/watch?v=CKm1rZlAwuA&list=PLS6tnpTr39sGv7RPHEZy8CDSdCliQOW8K)\n\nYou can also watch the playlist (and skip around to different sections) here:\n\n<div class=\"ratio ratio-16x9\">\n<iframe src=\"https://www.youtube.com/embed/playlist?list=PLS6tnpTr39sGv7RPHEZy8CDSdCliQOW8K\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n:::\n\n\n\n## Program background\n\nLet's revisit the mosquito net program we used in [the DAG example](/example/dags.qmd). Here, researchers are interested in whether using mosquito nets decreases an individual's risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. The data is **not experimental**—researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them.\n\nThe CSV file contains the following columns:\n\n- Malaria risk (`malaria_risk`): The likelihood that someone in the household will be infected with malaria. Measured on a scale of 0–100, with higher values indicating higher risk.\n- Mosquito net (`net` and `net_num`): A binary variable indicating if the household used mosquito nets.\n- Eligible for program (`eligible`): A binary variable indicating if the household is eligible for the free net program.\n- Income (`income`): The household's monthly income, in US dollars.\n- Nighttime temperatures (`temperature`): The average temperature at night, in Celsius.\n- Health (`health`): Self-reported healthiness in the household. Measured on a scale of 0–100, with higher values indicating better health.\n- Number in household (`household`): Number of people living in the household.\n- Insecticide resistance (`resistance`): Some strains of mosquitoes are more resistant to insecticide and thus pose a higher risk of infecting people with malaria. This is measured on a scale of 0–100, with higher values indicating higher resistance.\n\n\n## Our goal\n\n**Our goal in this example is to estimate the *causal* effect of bed net usage on malaria risk using *only* observational data.** This was not an RCT, so it might seem a little sketchy to make claims of causality. But if we can draw a correct DAG and adjust for the correct nodes, we can isolate the net → malaria relationship and talk about causality.\n\nBecause this is simulated data, we know the true causal effect of the net program because I built it into the data. **The true average treatment effect (ATE) is -10.** Using a mosquito net *causes* the risk of malaria to decrease by 10 points, on average.\n\nLet's see if we can find that 10 point effect!\n\n\n## Load data and libraries\n\nFirst, let's download the dataset (if you haven't already), put in a folder named `data`, and load it:\n\n- [{{< fa table >}} `mosquito_nets.csv`](/files/data/generated_data/mosquito_nets.csv)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)  # ggplot(), %>%, mutate(), and friends\nlibrary(ggdag)  # Make DAGs\nlibrary(dagitty)  # Do DAG logic with R\nlibrary(broom)  # Convert models to data frames\nlibrary(MatchIt)  # Match things\nlibrary(modelsummary)  # Make side-by-side regression tables\n\nset.seed(1234)   # Make all random draws reproducible\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load the data.\n# It'd be a good idea to click on the \"nets\" object in the Environment panel in\n# RStudio to see what the data looks like after you load it\nnets <- read_csv(\"data/mosquito_nets.csv\")\n```\n:::\n\n\n\n\n\n## DAG and adjustment sets\n\nBefore running any models, we need to find what we need to adjust for. Recall from [the DAG example](/example/dags.qmd) that we built this causal model to show the data generating process behind the relationship between mosquito net usage and malaria risk:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmosquito_dag <- dagify(\n  malaria_risk ~ net + income + health + temperature + resistance,\n  net ~ income + health + temperature + eligible + household,\n  eligible ~ income + household,\n  health ~ income,\n  exposure = \"net\",\n  outcome = \"malaria_risk\",\n  coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5,\n                      temperature = 6, resistance = 8.5, eligible = 2, household = 1),\n                y = c(malaria_risk = 2, net = 2, income = 3, health = 1,\n                      temperature = 3, resistance = 2, eligible = 3, household = 2)),\n  labels = c(malaria_risk = \"Risk of malaria\", net = \"Mosquito net\", income = \"Income\",\n             health = \"Health\", temperature = \"Nighttime temperatures\",\n             resistance = \"Insecticide resistance\",\n             eligible = \"Eligible for program\", household = \"Number in household\")\n)\n\nggdag_status(mosquito_dag, use_labels = \"label\", text = FALSE) +\n  guides(fill = \"none\", color = \"none\") +  # Disable the legend\n  theme_dag()\n```\n\n::: {.cell-output-display}\n![](matching-ipw_files/figure-html/build-mosquito-dag-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nFollowing the logic of *do*-calculus, we can find all the nodes that confound the relationship between net usage and malaria risk, since those nodes open up backdoor paths and distort the causal effect we care about. We can either do this graphically by looking for any node that points to both net and malaria risk, or we can use R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nadjustmentSets(mosquito_dag)\n## { health, income, temperature }\n```\n:::\n\n\nBased on the relationships between all the nodes in the DAG, adjusting for health, income, and temperature is enough to close all backdoors and identify the relationship between net use and malaria risk.\n\n\n## Naive correlation-isn't-causation estimate\n\nFor fun, we can calculate the difference in average malaria risk for those who did/didn't use mosquito nets. This is most definitely *not* the actual causal effect—this is the \"correlation is not causation\" effect that doesn't account for any of the backdoors in the DAG.\n\nWe can do this with a table (but then we have to do manual math to subtract the `FALSE` average from the `TRUE` average):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnets %>%\n  group_by(net) %>%\n  summarize(number = n(),\n            avg = mean(malaria_risk))\n## # A tibble: 2 × 3\n##   net   number   avg\n##   <lgl>  <int> <dbl>\n## 1 FALSE   1071  41.9\n## 2 TRUE     681  25.6\n```\n:::\n\n\nOr we can do it with regression:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_wrong <- lm(malaria_risk ~ net, data = nets)\n\ntidy(model_wrong)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)     41.9     0.405     104.  0        \n## 2 netTRUE        -16.3     0.649     -25.1 2.25e-119\n```\n:::\n\n\nAccording to this estimate, using a mosquito net is associated with a 16.33 point decrease in malaria risk, on average. We can't legally talk about this as a causal effect though—there are confounding variables to deal with.\n\n\n## Matching\n\nWe can use matching techniques to pair up similar observations and make the unconfoundedness assumption—that if we see two observations that are pretty much identical, and one used a net and one didn't, the choice to use a net was random.\n\nBecause we know from the DAG that income, nighttime temperatures, and health help cause both net use and malaria risk (and confound that relationship!), we'll try to find observations with similar values of income, temperatures, and health that both used and didn't use nets.\n\nWe can use the `matchit()` function from the **MatchIt** R package to match points based on Mahalanobis distance. There are lots of other options available—see [the online documentation](http://gking.harvard.edu/matchit) for details.\n\nWe can include the `replace = TRUE` option to make it so that points that have been matched already can be matched again (that is, we're not forcing a one-to-one matching; we have one-to-many matching instead).\n\n### Step 1: Preprocess\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmatched_data <- matchit(net ~ income + temperature + health,\n                        data = nets,\n                        method = \"nearest\",\n                        distance = \"mahalanobis\",\n                        replace = TRUE)\nsummary(matched_data)\n## \n## Call:\n## matchit(formula = net ~ income + temperature + health, data = nets, \n##     method = \"nearest\", distance = \"mahalanobis\", replace = TRUE)\n## \n## Summary of Balance for All Data:\n##             Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\n## income                955           873            0.41        1.4     0.104    0.198\n## temperature            23            24           -0.17        1.1     0.042    0.097\n## health                 55            48            0.36        1.2     0.071    0.168\n## \n## \n## Summary of Balance for Matched Data:\n##             Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\n## income                955           950           0.025        1.1     0.009    0.048            0.12\n## temperature            23            23          -0.006        1.0     0.007    0.026            0.12\n## health                 55            55           0.019        1.1     0.007    0.026            0.12\n## \n## Sample Sizes:\n##               Control Treated\n## All              1071     681\n## Matched (ESS)     321     681\n## Matched           436     681\n## Unmatched         635       0\n## Discarded           0       0\n```\n:::\n\n\nHere we can see that all 681 of the net users were paired with similar-looking non-users (439 of them). 632 people weren't matched and will get discarded. If you're curious, you can see which treated rows got matched to which control rows by running `matched_data$match.matrix`.\n\nWe can create a new data frame of those matches with `match.data()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmatched_data_for_real <- match.data(matched_data)\n```\n:::\n\n\n### Step 2: Estimation\n\nNow that the data has been matched, it should work better for modeling. Also, because we used income, temperatures, and health in the matching process, we've adjusted for those DAG nodes and have closed those backdoors, so our model can be pretty simple here:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_matched <- lm(malaria_risk ~ net,\n                    data = matched_data_for_real)\ntidy(model_matched)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)     38.3     0.599      64.0 0       \n## 2 netTRUE        -12.7     0.768     -16.6 2.28e-55\n```\n:::\n\n\nThe 12.73 point decrease here is better than the naive estimate, but it's not the true 10 point causal effect (that I built in to the data). Perhaps that's because the matches aren't great, or maybe we threw away too much data. There are a host of diagnostics you can look at to see how well things are matched (check [the documentation for **MatchIt**](http://gking.harvard.edu/matchit) for examples.)\n\nActually, the most likely culprit for the incorrect estimate is that there's some imbalance in the data. Because we set `replace = TRUE`, we did _not_ do 1:1 matching—untreated observations were paired with more than one treated observation. As a result, the multiply-matched observations are getting overcounted and have too much importance in the model. Fortunately, `matchit()` provides us with a column called `weights` that allows us to scale down the overmatched observations when running the model. **Importantly,** these weights have nothing to do with causal inference or backdoors or inverse probability weighting—their only purpose is to help scale down the imbalance arising from overmatching. If you use `replace = FALSE` and enforce 1:1 matching, the whole `weights` column will just be 1.\n\nWe can incorporate those weights into the model and get a more accurate estimate:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_matched_wts <- lm(malaria_risk ~ net,\n                        data = matched_data_for_real,\n                        weights = weights)\ntidy(model_matched_wts)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)     36.1     0.597      60.4 0       \n## 2 netTRUE        -10.5     0.765     -13.7 1.17e-39\n```\n:::\n\n\nAfter weighting to account for under- and over-matching, we find a -10.48 point causal effect. That's much better than any of the other estimates we've tried so far! The reason it's accurate is because we've closed the confounding backdoors and isolated the arrow between net use and malaria risk.\n\n\n## Inverse probability weighting\n\nOne potential downside to matching is that you generally have to throw away a sizable chunk of your data—anything that's unmatched doesn't get included in the final matched data.\n\nAn alternative approach to matching is to assign every observation some probability of receiving treatment, and then weight each observation by its inverse probability—observations that are predicted to get treatment and then don't, or observations that are predicted to not get treatment and then do will receive more weight than the observations that get/don't get treatment as predicted.\n\nGenerating these inverse probability weights requires a two step process: (1) we first generate propensity scores, or the probability of receiving treatment, and then (2) we use a special formula to convert those propensity scores into weights. Once we have inverse probability weights weights, we can incorporate them into our regression model.\n\n### Oversimplified crash course in logistic regression\n\nThere are many ways to generate propensity scores (like logistic regression, probit regression, and even machine learning techniques like random forests and neural networks), but logistic regression is probably the most common method.\n\nThe complete technical details of logistic regression are beyond the scope of this class, but if you're curious you should check out [this highly accessible tutorial](https://uc-r.github.io/logistic_regression).\n\nAll you really need to know is that the outcome variable in logistic regression models must be binary, and the explanatory variables you include in the model help explain the variation in the likelihood of your binary outcome. The Y (or outcome) in logistic regression is a logged ratio of probabilities, which forces the model's output to be in a 0-1 range:\n\n$$\n\\log \\frac{p_y}{p_{1-y}} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n$$\n\nHere's what it looks like visually. Because net usage is a binary outcome, there are lines of observations at 0 and 1 along the y axis. The blue curvy line here shows the output of a logistic regression model—people with low income have a low likelihood of using a net, while those with high income are far more likely to do so.\n\nI also included a red line showing the results from a regular old `lm()` OLS model. It follows the blue line fairly well for a while, but predicts negative probabilities if you use lower values of income, like less than 250. For strange historical and mathy reasons, many economists like using OLS on binary outcomes (they even have a fancy name for it: linear probability models (LPMs)), but I'm partial to logistic regression since it doesn't generate probabilities greater than 100% or less than 0%. (BUT DON'T EVER COMPLAIN ABOUT LPMs ONLINE. You'll start battles between economists and other social scientists. 🤣)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(nets, aes(x = income, y = net_num)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(method = \"lm\", color = \"red\", size = 0.5) +\n  geom_smooth(method = \"glm\", method.args = list(family = binomial(link = \"logit\"))) +\n  labs(x = \"Income\", y = \"Probability of using a net\")\n```\n\n::: {.cell-output-display}\n![](matching-ipw_files/figure-html/net-income-logit-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\nThe coefficients from a logistic regression model are interpreted differently than you're used to (and their interpretations can be controversial!). Here's an example for the model in the graph above:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Notice how we use glm() instead of lm(). The \"g\" stands for \"generalized\"\n# linear model. We have to specify a family in any glm() model. You can\n# technically run a regular OLS model (like you do with lm()) if you use\n# glm(y ~ x1 + x2, family = gaussian(link = \"identity\")), but people rarely do that.\n#\n# To use logistic regression, you have to specify a binomial/logit family like so:\n# family = binomial(link = \"logit\")\nmodel_logit <- glm(net ~ income + temperature + health,\n                   data = nets,\n                   family = binomial(link = \"logit\"))\n\ntidy(model_logit)\n## # A tibble: 4 × 5\n##   term        estimate std.error statistic     p.value\n##   <chr>          <dbl>     <dbl>     <dbl>       <dbl>\n## 1 (Intercept) -1.32     0.376        -3.50 0.000464   \n## 2 income       0.00209  0.000421      4.95 0.000000727\n## 3 temperature -0.0589   0.0125       -4.70 0.00000264 \n## 4 health       0.00688  0.00430       1.60 0.109\n```\n:::\n\n\nThe coefficients here aren't normal numbers—they're called \"log odds\" and represent the change in the logged odds as you move explanatory variables up. For instance, here the logged odds of using a net increase by 0.00688 for every one point increase in your health score. But what do logged odds even mean?! Nobody knows.\n\nYou can make these coefficients slightly more interpretable by unlogging them and creating something called an \"odds ratio.\" These coefficients were logged with a natural log, so you unlog them by raising $e$ to the power of the coefficient. The odds ratio for temperature is $e^{-0.0589}$, or 0.94. Odds ratios get interpreted a little differently than regular model coefficients. Odds ratios are all centered around 1—values above 1 mean that there's an increase in the likelihood of the outcome, while values below 1 mean that there's a decrease in the likelihood of the outcome.\n\nOur nighttime temperature odds ratio here is 0.94, which is 0.06 below 1, which means we can say that for every one point increase in nighttime temperatures, a person is 6% less likely to use a net. If the coefficient was something like 1.34, we could say that they'd be *34% more likely* to use a net; if it was something like 5.02 we could say that they'd be *5 times more* likely to use a net; if it was something like 0.1, we could say that they're *90% less likely* to use a net.\n\nYou can make R exponentiate the coefficients automatically by including `exponentiate = TRUE` in `tidy()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(model_logit, exponentiate = TRUE)\n## # A tibble: 4 × 5\n##   term        estimate std.error statistic     p.value\n##   <chr>          <dbl>     <dbl>     <dbl>       <dbl>\n## 1 (Intercept)    0.268  0.376        -3.50 0.000464   \n## 2 income         1.00   0.000421      4.95 0.000000727\n## 3 temperature    0.943  0.0125       -4.70 0.00000264 \n## 4 health         1.01   0.00430       1.60 0.109\n```\n:::\n\n\n**BUT AGAIN** this goes beyond the scope of this class! Just know that when you build a logistic regression model, you're using explanatory variables to predict the probability of an outcome.\n\nJust one last little explanation for why we have to use weird log odds things. When working with binary outcomes, we're dealing with probabilities, and we can create something called \"odds\" with probabilities. If there's a 70% chance of something happening, there's a 30% chance of it not happening. The ratio of those two probabilities is called \"odds\": $\\frac{p}{1 - p} = \\frac{0.7}{1 - 0.7} = \\frac{0.7}{0.3} = 2.333$. Odds typically follow a curvy relationship—as you move up to higher levels of your explanatory variable, the odds get bigger faster. If you log these odds, though ($\\log \\frac{p}{1 - p}$), the relationship becomes linear, which means we can use regular old linear regression on probabilities. Magic!\n\nYou can see the relationship between log odds and odds ratios in the first two panels here (this is fake data where X ranges between -5 and 5, and Y is either 0 or 1; you can see the data points in the final panel as dots):\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](matching-ipw_files/figure-html/odds-probs-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nThe coefficients from logistic regression are log odds because they come from model that creates that nice straight line in the first panel. Log odds are impossible to interpret, so we can unlog them ($e^\\beta$) to turn them into odds ratios.\n\nThe bottom panel shows predicted probabilities. You can do one more mathematical transformation with the odds ($\\frac{p}{1 - p}$) to generate a probability instead of odds: $\\frac{\\text{odds}}{1 + \\text{odds}}$. ***That*** is what a propensity score is.\n\n**BUT AGAIN I cannot stress enough how much you don't need to worry about the inner mechanics of logistic regression for this class!** If that went over your head, don't worry! All we're doing for IPW is using logistic regression to create propensity scores, and the code below shows how to do that. Behind the scenes you're moving from log odds (they're linear!) to odds (they're interpretable-ish) to probabilities (they're super interpretable!), but you don't need to worry about that.\n\nIf you're confused, [watch this TikTok for consolation](https://twitter.com/ChelseaParlett/status/1304436259926896640).\n\n\n### Step 1: Generate propensity scores\n\nPHEW. With that little tangent into logistic regression done, we can now build a model to generate propensity scores (or predicted probabilities).\n\nWhen we include variables in the model that generates propensity scores, we're making adjustments and closing backdoors in the DAG, just like we did with matching. But unlike matching, we're not throwing any data away! We're just making some observations more important and others less important.\n\nFirst we build a model that predicts net usage based on income, nighttime temperatures, and health (since those nodes are our confounders from the DAG):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_net <- glm(net ~ income + temperature + health,\n                 data = nets,\n                 family = binomial(link = \"logit\"))\n\n# We could look at these results if we wanted, but we don't need to for this class\n# tidy(model_net, exponentiate = TRUE)\n```\n:::\n\n\nWe can then plug in the income, temperatures, and health for every row in our dataset and generate a predicted probability using this model:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# augment_columns() handles the plugging in of values. You need to feed it the\n# name of the model and the name of the dataset you want to add the predictions\n# to. The type.predict = \"response\" argument makes it so the predictions are in\n# the 0-1 scale. If you don't include that, you'll get predictions in an\n# uninterpretable log odds scale.\nnet_probabilities <- augment_columns(model_net,\n                                     nets,\n                                     type.predict = \"response\") %>%\n  # The predictions are in a column named \".fitted\", so we rename it here\n  rename(propensity = .fitted)\n\n# Look at the first few rows of a few columns\nnet_probabilities %>%\n  select(id, net, income, temperature, health, propensity) %>%\n  head()\n## # A tibble: 6 × 6\n##      id net   income temperature health propensity\n##   <dbl> <lgl>  <dbl>       <dbl>  <dbl>      <dbl>\n## 1     1 TRUE     781        21.1     56      0.367\n## 2     2 FALSE    974        26.5     57      0.389\n## 3     3 FALSE    502        25.6     15      0.158\n## 4     4 TRUE     671        21.3     20      0.263\n## 5     5 FALSE    728        19.2     17      0.308\n## 6     6 FALSE   1050        25.3     48      0.429\n```\n:::\n\n\nThe propensity scores are in the `propensity` column. Some people, like person 3, are unlikely to use nets (only a 15.8% chance) given their levels of income, temperature, and health. Others like person 6 have a higher probability (42.9%) since their income and health are higher. Neat.\n\nNext we need to convert those propensity scores into inverse probability weights, which makes weird observations more important (i.e. people who had a high probability of using a net but didn't, and vice versa). To do this, we follow this equation:\n\n$$\n\\frac{\\text{Treatment}}{\\text{Propensity}} + \\frac{1 - \\text{Treatment}}{1 - \\text{Propensity}}\n$$\n\nThis equation will create weights that provide the average treatment effect (ATE), but there are other versions that let you find the average treatment effect on the treated (ATT), average treatment effect on the controls (ATC), and a bunch of others. [You can find those equations here](https://livefreeordichotomize.com/2019/01/17/understanding-propensity-score-weighting/#how-do-we-incorporate-a-propensity-score-in-a-weight).\n\nWe'll use `mutate()` to create a column for the inverse probability weight:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnet_ipw <- net_probabilities %>%\n  mutate(ipw = (net_num / propensity) + ((1 - net_num) / (1 - propensity)))\n\n# Look at the first few rows of a few columns\nnet_ipw %>%\n  select(id, net, income, temperature, health, propensity, ipw) %>%\n  head()\n## # A tibble: 6 × 7\n##      id net   income temperature health propensity   ipw\n##   <dbl> <lgl>  <dbl>       <dbl>  <dbl>      <dbl> <dbl>\n## 1     1 TRUE     781        21.1     56      0.367  2.72\n## 2     2 FALSE    974        26.5     57      0.389  1.64\n## 3     3 FALSE    502        25.6     15      0.158  1.19\n## 4     4 TRUE     671        21.3     20      0.263  3.81\n## 5     5 FALSE    728        19.2     17      0.308  1.44\n## 6     6 FALSE   1050        25.3     48      0.429  1.75\n```\n:::\n\n\nThese first few rows have fairly low weights—those with low probabilities of using nets didn't, while those with high probabilities did. But look at person 4! They only had a 26% chance of using a net and they *did*! That's weird! They therefore have a higher inverse probability weight (3.81).\n\n### Step 2: Estimation\n\nNow that we've generated inverse probability weights based on our confounders, we can run a model to find the causal effect of mosquito net usage on malaria risk. Again, we don't need to include income, temperatures, or health in the model since we already used them when we created the propensity scores and weights:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_ipw <- lm(malaria_risk ~ net,\n                data = net_ipw,\n                weights = ipw)\n\ntidy(model_ipw)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)     39.7     0.468      84.7 0       \n## 2 netTRUE        -10.1     0.658     -15.4 3.21e-50\n```\n:::\n\n\nCool! After using the inverse probability weights, we find a -10.13 point causal effect. That's a tiiiny bit off from the true value of 10, but not bad at all!\n\nIt's important to check the values of your inverse probability weights. Sometimes they can get too big, like if someone had an income of 0 and the lowest possible health and lived in a snow field *and yet* still used a net. Having really really high IPW values can throw off estimation. To fix this, we can truncate weights at some lower level. There's no universal rule of thumb for a good maximum weight—I've often seen 10 used. In our mosquito net data, no IPWs are higher than 10 (the max is exactly 10 with person 877), so we don't need to worry about truncation.\n\nIf you did want to truncate, you'd do something like this (here we're truncating at 8 instead of 10 so truncation actually does something):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnet_ipw <- net_ipw %>%\n  # If the IPW is larger than 8, make it 8, otherwise use the current IPW\n  mutate(ipw_truncated = ifelse(ipw > 8, 8, ipw))\n\nmodel_ipw_truncated <- lm(malaria_risk ~ net,\n                          data = net_ipw,\n                          weights = ipw_truncated)\n\ntidy(model_ipw_truncated)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)     39.7     0.467      85.1 0       \n## 2 netTRUE        -10.2     0.656     -15.5 4.32e-51\n```\n:::\n\n\nNow the causal effect is -10.19, which is slightly lower and probably less accurate since we don't really have any exceptional cases messing up our original IPW estimate.\n\n\n## Results from all the models\n\nAll done! We just used observational data to estimate causal effects. Causation without RCTs! Do you know how neat that is?!\n\nLet's compare all the ATEs that we just calculated:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodelsummary(list(\"Naive\" = model_wrong,\n                  \"Matched\" = model_matched, \"Matched + weights\" = model_matched_wts,\n                  \"IPW\" = model_ipw, \"IPW truncated at 8\" = model_ipw_truncated))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table style=\"NAborder-bottom: 0; width: auto !important; margin-left: auto; margin-right: auto;\" class=\"table\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:center;\"> Naive </th>\n   <th style=\"text-align:center;\"> Matched </th>\n   <th style=\"text-align:center;\"> Matched + weights </th>\n   <th style=\"text-align:center;\"> IPW </th>\n   <th style=\"text-align:center;\"> IPW truncated at 8 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:center;\"> 41.937*** </td>\n   <td style=\"text-align:center;\"> 38.335*** </td>\n   <td style=\"text-align:center;\"> 36.090*** </td>\n   <td style=\"text-align:center;\"> 39.679*** </td>\n   <td style=\"text-align:center;\"> 39.679*** </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> (0.405) </td>\n   <td style=\"text-align:center;\"> (0.599) </td>\n   <td style=\"text-align:center;\"> (0.597) </td>\n   <td style=\"text-align:center;\"> (0.468) </td>\n   <td style=\"text-align:center;\"> (0.467) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;background-color: #F6D645 !important;\"> netTRUE </td>\n   <td style=\"text-align:center;background-color: #F6D645 !important;\"> −16.332*** </td>\n   <td style=\"text-align:center;background-color: #F6D645 !important;\"> −12.730*** </td>\n   <td style=\"text-align:center;background-color: #F6D645 !important;\"> −10.485*** </td>\n   <td style=\"text-align:center;background-color: #F6D645 !important;\"> −10.131*** </td>\n   <td style=\"text-align:center;background-color: #F6D645 !important;\"> −10.190*** </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;box-shadow: 0px 1pxborder-bottom: 1px solid\">  </td>\n   <td style=\"text-align:center;box-shadow: 0px 1pxborder-bottom: 1px solid\"> (0.649) </td>\n   <td style=\"text-align:center;box-shadow: 0px 1pxborder-bottom: 1px solid\"> (0.768) </td>\n   <td style=\"text-align:center;box-shadow: 0px 1pxborder-bottom: 1px solid\"> (0.765) </td>\n   <td style=\"text-align:center;box-shadow: 0px 1pxborder-bottom: 1px solid\"> (0.658) </td>\n   <td style=\"text-align:center;box-shadow: 0px 1pxborder-bottom: 1px solid\"> (0.656) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Num.Obs. </td>\n   <td style=\"text-align:center;\"> 1752 </td>\n   <td style=\"text-align:center;\"> 1117 </td>\n   <td style=\"text-align:center;\"> 1117 </td>\n   <td style=\"text-align:center;\"> 1752 </td>\n   <td style=\"text-align:center;\"> 1752 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R2 </td>\n   <td style=\"text-align:center;\"> 0.265 </td>\n   <td style=\"text-align:center;\"> 0.198 </td>\n   <td style=\"text-align:center;\"> 0.144 </td>\n   <td style=\"text-align:center;\"> 0.119 </td>\n   <td style=\"text-align:center;\"> 0.121 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R2 Adj. </td>\n   <td style=\"text-align:center;\"> 0.265 </td>\n   <td style=\"text-align:center;\"> 0.197 </td>\n   <td style=\"text-align:center;\"> 0.144 </td>\n   <td style=\"text-align:center;\"> 0.119 </td>\n   <td style=\"text-align:center;\"> 0.121 </td>\n  </tr>\n</tbody>\n<tfoot><tr><td style=\"padding: 0; \" colspan=\"100%\">\n<sup></sup> + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001</td></tr></tfoot>\n</table>\n\n`````\n:::\n:::\n\n\nWhich one is right? In this case, because this is fake simulated data where I built in a 10 point effect, we can see which of these models gets the closest: here, the non-truncated IPW model wins. But that won't always be the case. Both matching and IPW work well for closing backdoors and adjusting for confounders. In real life, you won't know the true value, so try multiple ways.\n",
    "supporting": [
      "matching-ipw_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}