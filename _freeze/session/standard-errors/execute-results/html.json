{
  "hash": "3d61f426fc48f79bf7409107087e60ec",
  "result": {
    "markdown": "---\ntitle: \"Robust and clustered standard errors with R\"\n---\n\n\n\n\nAs you read in [chapter 13.3 of *The Effect*](https://theeffectbook.net/ch-StatisticalAdjustment.html#your-standard-errors-are-probably-wrong), your standard errors in regressions are probably wrong. And as you read [in the article by Guido Imbens](https://doi.org/10.1257/jep.35.3.157), we want accurate standard errors because we should be focusing on confidence intervals when reporting our findings because nobody actually cares about or understands p-values.\n\nSo why are our standard errors wrong, and how do we fix them?\n\nFirst, let's make a model that predicts penguin weight based on bill length, flipper length, and species. We'll use our regular old trusty `lm()` for an OLS model with regular standard errors.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)       # For ggplot, dplyr, and friends\nlibrary(broom)           # Convert model objects into data frames\nlibrary(palmerpenguins)  # Our favorite penguin data\n\n# Get rid of rows with missing values\npenguins <- penguins %>% drop_na(sex)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel1 <- lm(body_mass_g ~ bill_length_mm + flipper_length_mm + species,\n             data = penguins)\ntidy(model1, conf.int = TRUE)\n## # A tibble: 5 × 7\n##   term              estimate std.error statistic  p.value conf.low conf.high\n##   <chr>                <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n## 1 (Intercept)        -3864.     534.       -7.24 3.18e-12  -4914.    -2814. \n## 2 bill_length_mm        60.1      7.21      8.34 2.06e-15     45.9      74.3\n## 3 flipper_length_mm     27.5      3.21      8.58 3.77e-16     21.2      33.9\n## 4 speciesChinstrap    -732.      82.1      -8.93 3.22e-17   -894.     -571. \n## 5 speciesGentoo        113.      89.2       1.27 2.05e- 1    -62.2     289.\n```\n:::\n\n\nA 1 mm increase in bill length is associated with a 60.1 g increase in penguin weight, on average. This is significantly significant and different from zero (p < 0.001), and the confidence interval ranges between 45.9 and 74.3, which means that we're 95% confident that this range captures the true population parameter (this is [a frequentist interval](/resource/bayes.qmd#frequentist-confidence-intervals) since we didn't do any Bayesian stuff, so we have to talk about the confidence interval like a net and use awkward language). \n\nThis confidence interval is the coefficient estimate ± (1.96 × the standard error) (or 60.1 ± (1.96 × 7.21)), so it's entirely dependent on the accuracy of the standard error. One of the assumptions of this estimate and its corresponding standard error is that the residuals of the regression (i.e. the distance from the predicted values and the actual values—[remember this plot from Session 2](/slides/02-slides.html#23)) *must not have any patterns in them*. The official name for this assumption is that the errors in an OLS must be **homoskedastic** (or exhibit **homoskedasticity**). If errors are **heteroskedastic**—if the errors aren't independent from each other, if they aren't normally distributed, and if there are visible patterns in them—your standard errors (and confidence intervals) will be wrong.\n\n## Checking for heteroskedasticity\n\nHow do you know if your errors are homoskedastic of heterosketastic though? \n\nThe easiest way for me (and most people probably) is to visualize the residuals and see if there are any patterns. First, we can use `augment()` to calculate the residuals for each observation in the original penguin data and then make a scatterplot that puts the actual weight on the x-axis and the residuals on the y-axis. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Plug the original data into the model and find fitted values and\n# residuals/errors\nfitted_data <- augment(model1, data = penguins)\n\n# Look at relationship between fitted values and residuals\nggplot(fitted_data, aes(x = .fitted, y = .resid)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](standard-errors_files/figure-html/check-residuals-1-1.png){fig-align='center' width=576}\n:::\n:::\n\n\nThere seems to be two clusters of points here, which is potentially a sign that the errors aren't independent of each other—there might be a systematic pattern in the errors. We can confirm this if we color the points by species:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(fitted_data, aes(x = .fitted, y = .resid)) + \n  geom_point(aes(color = species)) +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](standard-errors_files/figure-html/check-residuals-2-1.png){fig-align='center' width=576}\n:::\n:::\n\n\nAnd there are indeed species-based clusters within the residuals! We can see this if we look at the distribution of the residuals too. For OLS assumptions to hold—and for our standard errors to be correct—this should be normally distributed:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(fitted_data, aes(x = .resid)) +\n  geom_histogram(binwidth = 100, color = \"white\", boundary = 3000)\n```\n\n::: {.cell-output-display}\n![](standard-errors_files/figure-html/residual-distribution-1.png){fig-align='center' width=576}\n:::\n:::\n\n\nThis looks fairly normal, though there are some more high residual observations (above 500) than we'd expect. That's likely because the data isn't actually heteroskedastic—it's just clustered, and this clustering structure within the residuals means that our errors (and confidence intervals and p-values) are going to be wrong.\n\nIn this case the residual clustering creates a fairly obvious pattern. Just for fun, let's ignore the Gentoos and see what residuals without clustering can look like. The residuals here actually look fairly random and pattern-free:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_no_gentoo <- lm(body_mass_g ~ bill_length_mm + flipper_length_mm + species,\n                      data = filter(penguins, species != \"Gentoo\"))\n\nfitted_sans_gentoo <- augment(model_no_gentoo,\n                              filter(penguins, species != \"Gentoo\"))\n\nggplot(fitted_sans_gentoo, aes(x = .fitted, y = .resid)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](standard-errors_files/figure-html/model-plot-no-gentoo-1.png){fig-align='center' width=576}\n:::\n:::\n\n\nWe can do a neat little visual test for this. Let's shuffle the residuals a bunch of times and make scatterplots with those shuffled values. If we can't spot the actual residual plot among the shuffled ones, we can be fairly confident that there aren't any patterns. [The **nullabor** package](https://dicook.github.io/nullabor/) makes this easy, allowing us to create a lineup of shuffled plots with the real residual plot mixed in there somewhere.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(nullabor)\n\nset.seed(1234)  # Shuffle these the same way every time\n\nshuffled_residuals <- lineup(null_lm(body_mass_g ~ bill_length_mm + flipper_length_mm + species,\n                                     method = \"rotate\"),\n                             true = fitted_sans_gentoo,\n                             n = 9)\n## decrypt(\"23eg MuPu NE KwWNPNwE FF\")\n\nggplot(shuffled_residuals, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  facet_wrap(vars(.sample))\n```\n\n::: {.cell-output-display}\n![](standard-errors_files/figure-html/resid-shuffled-1.png){fig-align='center' width=576}\n:::\n:::\n\n\nWhich one is the actual residual plot? There's no easy way to tell. That cryptic `decrypt(...)` thing is a special command that will tell us which plot is the correct one:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecrypt(\"sD0f gCdC En JP2EdEPn ZZ\")\n## Warning in decrypt(\"sD0f gCdC En JP2EdEPn ZZ\"): NAs introduced by coercion\n## [1] \"gbkJ eSDS nP KtTnDntP  NA\"\n```\n:::\n\n\nThe fact that we can't tell is a good sign that the residuals are homoskedastic and independent and that we don't need to worry much about correcting the errors. \n\n\n\n\n\n\n## Adjusting standard errors\n\nHowever, often you will see patterns or clusters in the residuals, which means you need to make some adjustments to the errors to ensure they're accurate. In [chapter 13.3 of *The Effect*](https://theeffectbook.net/ch-StatisticalAdjustment.html#your-standard-errors-are-probably-wrong) you read about a bunch of different mathy ways to make these adjustments, and people get PhDs and write whole dissertations on new fancy ways to adjust standard errors. I'm not super interested in the deeper mathy mechanics of error adjustment, and most people aren't either, so statistical software packages generally try to make it easy to make these adjustments without needing to think about the deeper math. \n\nIf you're familiar with Stata, you can get robust standard errors like this:\n\n```r\n# Run this in R first to export the penguins data as a CSV\nwrite_csv(\"~/Desktop/penguins.csv\")\n```\n\n```text\n/* Stata stuff */\n\nimport delimited \"~/Desktop/penguins.csv\", clear \n\nencode species, generate(species_f)  /* Make species a factor /*\n\nreg body_mass_g bill_length_mm flipper_length_mm i.species_f, robust\n\n/* \nLinear regression                               Number of obs     =        333\n                                                F(4, 328)         =     431.96\n                                                Prob > F          =     0.0000\n                                                R-squared         =     0.8243\n                                                Root MSE          =     339.57\n-----------------------------------------------------------------------------------\n                  |               Robust\n      body_mass_g |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n------------------+----------------------------------------------------------------\n   bill_length_mm |   60.11733   6.429263     9.35   0.000     47.46953    72.76512\nflipper_length_mm |   27.54429   3.054304     9.02   0.000     21.53579    33.55278\n                  |\n        species_f |\n       Chinstrap  |  -732.4167     75.396    -9.71   0.000    -880.7374    -584.096\n          Gentoo  |   113.2541   88.27028     1.28   0.200    -60.39317    286.9014\n                  |\n            _cons |  -3864.073   500.7276    -7.72   0.000    -4849.116   -2879.031\n----------------------------------------------------------------------------------- \n*/\n```\n\nAnd you can get clustered robust standard errors like this:\n\n```text\nreg body_mass_g bill_length_mm flipper_length_mm i.species_f, cluster(species_f)\n\n/*\nLinear regression                               Number of obs     =        333\n                                                F(1, 2)           =          .\n                                                Prob > F          =          .\n                                                R-squared         =     0.8243\n                                                Root MSE          =     339.57\n                                   (Std. Err. adjusted for 3 clusters in species_f)\n-----------------------------------------------------------------------------------\n                  |               Robust\n      body_mass_g |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n------------------+----------------------------------------------------------------\n   bill_length_mm |   60.11733   12.75121     4.71   0.042     5.253298    114.9814\nflipper_length_mm |   27.54429   4.691315     5.87   0.028     7.359188    47.72939\n                  |\n        species_f |\n       Chinstrap  |  -732.4167   116.6653    -6.28   0.024    -1234.387   -230.4463\n          Gentoo  |   113.2541   120.5977     0.94   0.447    -405.6361    632.1444\n                  |\n            _cons |  -3864.073   775.9628    -4.98   0.038    -7202.772   -525.3751\n-----------------------------------------------------------------------------------\n*/\n```\n\nBasically add `, robust` (or even just `,r`) or `cluster(whatever)` to the end of the regression command.\n\nDoing this in R is a little trickier since our favorite standard `lm()` command doesn't have built-in support for robust or clustered standard errors, but there are some extra packages that make it really easy to do. Let's look at three different ways.\n\n\n### **sandwich** and `coeftest()`\n\nOne way to adjust errors is to use [the **sandwich** package](http://sandwich.r-forge.r-project.org/articles/sandwich.html), which actually handles the standard error correction behind the scenes for most of the other approaches we'll look at. **sandwich** comes with a bunch of standard error correction functions, like `vcovHC()` for heteroskedasticity-consistent (HC) errors, `vcovHAC()` for heteroskedastiticy- and autocorrelation-consistent (HAC) errors, and `vcovCL()` for clustered errors ([see their website](http://sandwich.r-forge.r-project.org/articles/sandwich.html) for all the different ones). Within each of these different functions, there are different types (again, things that fancy smart statisticians figure out). If you want to replicate Stata's `, robust` option exactly, you can use `vcovHC(type = \"HC1\")`.\n\nYou can use these correction functions as an argument to the `coeftest()` function, the results of which conveniently work with `tidy()` and other **broom** functions. Here's how we can get robust standard errors for our original penguin model (`model1`):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(sandwich)  # Adjust standard errors\nlibrary(lmtest)    # Recalculate model errors with sandwich functions with coeftest()\n\n# Robust standard errors with lm()\nmodel1_robust <- coeftest(model1, \n                          vcov = vcovHC)\n\n# Stata's robust standard errors with lm()\nmodel1_robust_stata <- coeftest(model1, \n                                vcov = vcovHC,\n                                type = \"HC1\")\n\ntidy(model1_robust) %>% filter(term == \"bill_length_mm\")\n## # A tibble: 1 × 5\n##   term           estimate std.error statistic  p.value\n##   <chr>             <dbl>     <dbl>     <dbl>    <dbl>\n## 1 bill_length_mm     60.1      6.52      9.22 3.67e-18\n\ntidy(model1_robust_stata) %>% filter(term == \"bill_length_mm\")\n## # A tibble: 1 × 5\n##   term           estimate std.error statistic  p.value\n##   <chr>             <dbl>     <dbl>     <dbl>    <dbl>\n## 1 bill_length_mm     60.1      6.43      9.35 1.40e-18\n```\n:::\n\n\nThose errors shrunk a little, likely because just using robust standard errors here isn't enough. Remember that there are clear clusters in the residuals (Gentoo vs. not Gentoo), so we'll actually want to cluster the errors by species. `vcovCL()` lets us do that:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Clustered robust standard errors with lm()\nmodel1_robust_clustered <- coeftest(model1,\n                                    vcov = vcovCL,\n                                    type = \"HC1\",\n                                    cluster = ~species)\n\ntidy(model1_robust_clustered, conf.int = TRUE) %>% \n  filter(term == \"bill_length_mm\")\n## # A tibble: 1 × 7\n##   term           estimate std.error statistic    p.value conf.low conf.high\n##   <chr>             <dbl>     <dbl>     <dbl>      <dbl>    <dbl>     <dbl>\n## 1 bill_length_mm     60.1      12.8      4.71 0.00000359     35.0      85.2\n```\n:::\n\n\nThose errors are huge now, and the confidence interval ranges from 35 to 85! That's because we're now accounting for the clustered structure in the errors.\n\nBut we're still not getting the same results as the clustered robust errors from Stata (or as `feols()` and `lm_robust()` below). This is because the data we're working with here has a small number of clusters and `coeftest()`/`vcovCL()` doesn't deal with that automatically (but Stata, `feols()`, and `lm_robust()` all do—see [this section about it in the documentation for `feols()`](https://lrberge.github.io/fixest/articles/fixest_walkthrough.html#small-sample-correction-1)). To fix this, we need to specify the number of degrees of freedom in the `coeftest()` function. There are three species/clusters in this data, so the degrees of freedom is 1 less than that, or 2.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Clustered robust standard errors with lm(), correcting for small sample\nmodel1_robust_clustered_corrected <- coeftest(model1,\n                                              vcov = vcovCL,\n                                              type = \"HC1\",\n                                              df = 2,  # There are 3 species, so 3-1 = 2\n                                              cluster = ~species)\n\ntidy(model1_robust_clustered_corrected, conf.int = TRUE) %>% \n  filter(term == \"bill_length_mm\")\n## # A tibble: 1 × 7\n##   term           estimate std.error statistic p.value conf.low conf.high\n##   <chr>             <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n## 1 bill_length_mm     60.1      12.8      4.71  0.0422     5.25      115.\n```\n:::\n\n\nThere we go. Now we have *absolutely massive* confidence intervals ranging from 5 to 115. Who even knows what the true parameter is?! Our net probably caught it.\n\n\n### **fixest** and `feols()`\n\nStandard error adjustment with the functions from **sandwich** and `coeftest()` works fine, but it requires multiple steps: (1) create a model with `lm()`, and (2) feed that model to `coeftest()`. It would be great if we could do that all at the same time in one command! Fortunately there are a few R packages that let us do this.\n\nThe `feols()` function from the [**fixest** package](https://lrberge.github.io/fixest/) was designed for OLS models that have lots of fixed effects (i.e. indicator variables), and it handles lots of fixed effects really really fast. It can also handle instrumental variables (which [we'll get to later in the semester](/example/iv.qmd)). It's a fantastic way to run models in R. It uses the same formula syntax as `lm()`, but with one extra feature: you can put fixed effects (again, indicator variables) after a `|` to specify that the variables are actually fixed effects. This (1) speeds up model estimation, and (2) hides the fixed effects from `summary()` and `tidy()` output, which is super convenient if you're using something like county, state, or country fixed effects and you don't want to see dozens or hundreds of extra rows in the regression output.\n\nIt includes an argument `vcov` for specifying how you want to handle the standard errors, and [you can use lots of different options](https://lrberge.github.io/fixest/articles/standard_errors.html). Here we'll make two models: one with heteroskedastic robust SEs (basically what Stata uses, or like `vcovHC()` in **sandwich**), and one with clustered robust standard errors (similar to `vcovCL()` in **sandwich**):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(fixest)  # Run models with feols()\n\n# Because species comes after |, it's being treated as a fixed effect\nmodel_feols_hetero <- feols(body_mass_g ~ bill_length_mm + flipper_length_mm | species,\n                            vcov = \"hetero\",\n                            data = penguins)\n\ntidy(model_feols_hetero) %>% filter(term == \"bill_length_mm\")\n## # A tibble: 1 × 5\n##   term           estimate std.error statistic  p.value\n##   <chr>             <dbl>     <dbl>     <dbl>    <dbl>\n## 1 bill_length_mm     60.1      6.43      9.35 1.40e-18\n\nmodel_feols_clustered <- feols(body_mass_g ~ bill_length_mm + flipper_length_mm | species,\n                               cluster = ~ species,\n                               data = penguins)\n\ntidy(model_feols_clustered, conf.int = TRUE) %>% filter(term == \"bill_length_mm\")\n## # A tibble: 1 × 7\n##   term           estimate std.error statistic p.value conf.low conf.high\n##   <chr>             <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n## 1 bill_length_mm     60.1      12.7      4.73  0.0419     5.42      115.\n```\n:::\n\n\nWe get basically the same standard errors we did before with `coeftest(lm(), vcov = ...)`, only now we did it all in one step.\n\n### **estimatr** and `lm_robust()`\n\nThe `lm_robust()` function in [the **estimatr** package](https://declaredesign.org/r/estimatr/articles/getting-started.html) also allows you to calculate robust standard errors in one step using the `se_type` argument. See the documentation for all the possible options. Here we can replicate Stata's standard errors by using `se_type = \"stata\"` (`se_type = \"HC1\"` would do the same thing). `lm_robust()` also lets you specify fixed effects separately so that they're hidden in the results, but instead of including them in the formula like we did with `feols()`, we have to use the `fixed_effects` argument.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(estimatr)  # Run models with lm_robust()\n\nmodel_lmrobust_clustered <- lm_robust(body_mass_g ~ bill_length_mm + flipper_length_mm,\n                                      fixed_effects = ~ species,\n                                      se_type = \"stata\",\n                                      clusters = species,\n                                      data = penguins)\n\ntidy(model_lmrobust_clustered, conf.int = TRUE) %>% filter(term == \"bill_length_mm\")\n##             term estimate std.error statistic p.value conf.low conf.high df     outcome\n## 1 bill_length_mm       60        13       4.7   0.042      5.3       115  2 body_mass_g\n```\n:::\n\n\n\n### On-the-fly SE adjustment\n\nWhile it's neat that we can specify standard errors directly in `feols()` and `lm_robust()`, it's sometimes ([almost always](https://grantmcdermott.com/better-way-adjust-SEs/)) a good idea to actually not adjust SEs within the models themselves and instead make the adjustments after you've already fit the model. This is especially the case if you have a more complex model that takes a while to run. Note how we ran a couple different `feols()` models previously—it would be nice if we could just run the model once and then choose whatever standard error adjustments we want later. This is what we already did with `coeftest(model, vcov = ...)`. We can do similar things with `feols()`. Watch how we can specify the standard error options and clusters inside `tidy()` directly, with just one model!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_feols_basic <- feols(body_mass_g ~ bill_length_mm + flipper_length_mm | species,\n                           data = penguins)\n\ntidy(model_feols_basic, se = \"hetero\") %>% filter(term == \"bill_length_mm\")\n## # A tibble: 1 × 5\n##   term           estimate std.error statistic  p.value\n##   <chr>             <dbl>     <dbl>     <dbl>    <dbl>\n## 1 bill_length_mm     60.1      6.43      9.35 1.40e-18\n\ntidy(model_feols_basic, cluster = \"species\") %>% filter(term == \"bill_length_mm\")\n## # A tibble: 1 × 5\n##   term           estimate std.error statistic p.value\n##   <chr>             <dbl>     <dbl>     <dbl>   <dbl>\n## 1 bill_length_mm     60.1      12.7      4.73  0.0419\n```\n:::\n\n\nThe `modelsummary()` function from [the **modelsummary** package](https://vincentarelbundock.github.io/modelsummary/) also lets us make SE adjustments on the fly. Check this out—with just one basic model with `lm()`, we can get all these different kinds of standard errors!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(modelsummary)  # Make nice tables and plots for models\n\nmodel_basic <- lm(body_mass_g ~ bill_length_mm + flipper_length_mm + species,\n                  data = penguins)\n\n# Add an extra row with the error names\nse_info <- tibble(term = \"Standard errors\", \"Regular\", \"Robust\", \"Stata\", \"Clustered by species\")\n\nmodelsummary(model_basic, \n             # Specify how to robustify/cluster the model\n             vcov = list(\"iid\", \"robust\", \"stata\", function(x) vcovCL(x, cluster = ~ species)),\n             # Get rid of other coefficients and goodness-of-fit (gof) stats\n             coef_omit = \"species|flipper|Intercept\", gof_omit = \".*\",\n             add_rows = se_info)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:center;\"> Model 1 </th>\n   <th style=\"text-align:center;\"> Model 2 </th>\n   <th style=\"text-align:center;\"> Model 3 </th>\n   <th style=\"text-align:center;\"> Model 4 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> bill_length_mm </td>\n   <td style=\"text-align:center;\"> 60.117 </td>\n   <td style=\"text-align:center;\"> 60.117 </td>\n   <td style=\"text-align:center;\"> 60.117 </td>\n   <td style=\"text-align:center;\"> 60.117 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> (7.207) </td>\n   <td style=\"text-align:center;\"> (6.519) </td>\n   <td style=\"text-align:center;\"> (6.429) </td>\n   <td style=\"text-align:center;\"> (12.751) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standard errors </td>\n   <td style=\"text-align:center;\"> Regular </td>\n   <td style=\"text-align:center;\"> Robust </td>\n   <td style=\"text-align:center;\"> Stata </td>\n   <td style=\"text-align:center;\"> Clustered by species </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n### Plot all these confidence intervals\n\nThe **modelsummary** package also comes with a `modelplot()` function that will create a coefficient plot showing the point estimates and 95% confidence intervals. Look at all these different standard errors!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodelplot(\n  list('lm_robust(se_type = \"stata\", clusters = species)' = model_lmrobust_clustered,\n       'feols(se = \"cluster\", cluster = ~species)' = model_feols_clustered,\n       'feols(se = \"hetero\")' = model_feols_hetero,\n       'lm() + vcovCL(cluster = \"species\") [small sample corrected]' = model1_robust_clustered_corrected,\n       'lm() + vcovCL(cluster = \"species\")' = model1_robust_clustered,\n       'lm() + vcovHC(type = \"HC1\") [Stata]' = model1_robust_stata,\n       \"lm() + vcovHC() [robust]\" = model1_robust,\n       \"Basic lm() model\" = model1),\n  coef_omit = \"species|flipper|Intercept\") + \n  guides(color = guide_legend(reverse = TRUE))\n```\n\n::: {.cell-output-display}\n![](standard-errors_files/figure-html/modelsummary-coef-plot-1.png){fig-align='center' width=768}\n:::\n:::\n",
    "supporting": [
      "standard-errors_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}