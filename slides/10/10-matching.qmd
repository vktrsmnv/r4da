---
title: "Matching"
subtitle: "Data Analytics and Visualization with R<br>Session 10"
title-slide-attributes:
  data-background-size: stretch
  data-slide-number: none
auto-stretch: false
institute: "University of Mannheim<br>Spring 2023"
author: "Viktoriia Semenova"
footer: "[🔗 r4da.live](https://r4da.live/)"
logo: images/logo.png
format:
  revealjs:
    theme: ../slides.scss
    transition: fade
    incremental: true   
    slide-number: true
    chalkboard: true
execute:
  echo: true
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
p_needed <- c("tidyverse", "janitor", "icons", "infer", "haven",
              "countdown", "showtext", "ggdag", "magrittr", "knitr",
              "MatchIt", "cobalt",
              "gt", "plotly", "broom", "patchwork", "modelsummary")

# check if they are already installed, install if not installed
lapply(p_needed[!(p_needed %in% rownames(installed.packages()))], install.packages, repos = "http://cran.us.r-project.org")

# load the packages
lapply(p_needed, library, character.only = TRUE)

# set width of code output
options(width = 90)

# set figure parameters for knitr
knitr::opts_chunk$set(
  fig.width = 7, # 7" width
  fig.asp = 0.618, # the golden ratio
  fig.retina = 3, # dpi multiplier for displaying HTML output on retina
  fig.align = "center", # center align figures
  dpi = 300 # higher dpi, sharper image
)


font_add_google(name = "Gochi Hand")
showtext::showtext_auto()
# set theme for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 20)) +
  theme(plot.title = element_text(face = "bold"),
        plot.title.position = "plot",
        panel.grid.minor = element_blank())

# governors <- readr::read_csv("https://r4da.live/files/data/external_data/governors.csv")
# trains <- readr::read_tsv("../data/trains.tsv") %>% clean_names()
UA <- read_csv("../data/UA_survey.csv") %>% clean_names()
cabinet <- read_dta("../data/cabinet_size_replication_data.dta")
judges <- read_rds("../data/judges.rds")
cdat <- read_csv("cdat.csv")
set.seed(2508)
bp <- causaldata::black_politicians
```

# Matching and Weighting Methods

## Ideal Setting: Random Experiment {.smaller}

-   When we run a randomized experiment, we are making sure that, on average,
    the treated and control groups are exactly the same before we apply
    treatment
-   *Gold standard* implies that all causal inferences will be valid it you do
    the experiment right. Why?
-   Randomization solves many internal validity issues:
    -   Selection: Treatment and control groups are comparable; people don't
        self-select
-   Goal: estimate ATE (average treatment effect)/CATE (conditional ATE)/ATT
    (ATE on the Treated)
-   Under randomization, ATE = ATT because the selection bias is zero

## Matching Is A Way of Closing Backdoors

```{dot}
//| echo: false
//| fig-width: 6
//| fig-height: 4
//| out-width: 100%
digraph D {
  node [shape=circle];
  edge [len = 1.2, arrowhead = vee];
  a [label = "X"];
  b [label = "Y"];
  c [label = "Z"];
  d [label = "W"];
  
  {rank=same a b};
  {rank=same d c};
  {rank=sink a};
  a->b;
  c->a;
  c->b;
  d->a;
  d->b;
}
```

. . .

> Selecting a sample where people have similar levels of W (Z) is one way of
> controlling for W (Z)

## Why Matching? {.smaller}

-   Reduces dependence of estimates on parametric models (i.e. coefficient
    estimates do not depend heavily on the model specification)

    -   Model dependence → researcher discretion → bias

-   Makes counterfactual comparisons more transparent: we make comparisons
    across comparable units

-   Alternative way to statistical control to close the backdoors

-   But: matching is **NOT** a solution for selection on unobservables, i.e. we
    still need to list all plausible confounders and assume that they are enough

::: task
Matching is an *estimation* technique, not an identification strategy
:::

## Model Dependence: All Data {.smaller}

```{r simulated-matching-data, echo=FALSE, out.width="80%"}
set.seed(1234)
matched_stuff <- tibble(education = rnorm(50, 20, 3)) %>% 
  mutate(outcome = 15 + education * rnorm(n(), -0.2, 0.05)) %>% 
  mutate(treatment = as.logical(rbinom(n(), 1, 0.5)),
         type = "Matched")

unmatched_stuff <- tibble(education = rnorm(20, 12, 2),
                          outcome = rnorm(20, 5, 2),
                          treatment = FALSE, 
                          type = "Unmatched")

more_unmatched_stuff <- tibble(education = rnorm(5, 28, 1),
                               outcome = rnorm(5, 5, 0.5),
                               treatment = FALSE,
                               type = "Unmatched")

all_data <- bind_rows(matched_stuff, unmatched_stuff, more_unmatched_stuff) %>% 
  mutate(treatment = factor(treatment, labels = c("Untreated", "Treated")))

matched_stuff_real <- filter(all_data, type == "Matched")

model_wrong <- lm(outcome ~ education + treatment, data = all_data) %>% 
  tidy()

model_wrong1 <- lm(outcome ~ education + treatment + I(education^2), data = all_data)

model_wrong1_fitted <- expand_grid(education = seq(8, 30, 0.1),
                                   treatment = c("Treated", "Untreated")) %>% 
  augment(model_wrong1, newdata = .)

model_better <- lm(outcome ~ education + treatment, data = matched_stuff_real) %>% 
  tidy()

model_better1 <- lm(outcome ~ education + treatment + I(education^2), data = matched_stuff_real)

model_better1_fitted <- expand_grid(education = seq(8, 30, 0.1),
                                    treatment = c("Treated", "Untreated")) %>% 
  augment(model_better1, newdata = .)
```

$$
\color{white}{\beta_0 \text{E}^2}
$$

```{r matching-general, echo=FALSE, out.width="80%"}
ggplot(all_data, aes(x = education, y = outcome, fill = treatment)) +
  geom_point(size = 5,  color = "white", shape = 21) +
  scale_fill_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) +
  # scale_shape_manual(values = c(21, 21)) + 
  labs(x = "Education", y = "Outcome") +
  coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) +
  # theme_bw(base_size = 20, base_family = "Fira Sans Condensed") +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0))
```

## Model Dependence: All Data {.smaller}

$$
\color{white}{\beta_0 \text{E}^2} \text{Outcome} = \beta_0 + \beta_1 \text{Education} + \beta_2 \text{Treatment} \color{white}{\beta_0 \text{E}^2}
$$

```{r matching-dependency1, echo=FALSE, out.width="80%", fig.width = 6}
ggplot(all_data, aes(x = education, y = outcome, fill = treatment, color = treatment)) +
  geom_point(size = 5, pch = 21, color = "white") +
  geom_abline(slope = filter(model_wrong, term == "education")$estimate, 
              intercept = filter(model_wrong, term == "(Intercept)")$estimate,
              color = "#440154FF", size = 0.75) +
  geom_abline(slope = filter(model_wrong, term == "education")$estimate, 
              intercept = filter(model_wrong, term == "(Intercept)")$estimate +
                filter(model_wrong, term == "treatmentTreated")$estimate,
              color = "#7AD151FF", size = 0.75) +
  scale_fill_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) +
  labs(x = "Education", y = "Outcome") +
  coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) +
  # theme_bw(base_size = 20, base_family = "Fira Sans Condensed") +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0))
```

## Model Dependence: All Data {.smaller}

$$
\text{Outcome} = \beta_0 + \beta_1 \text{Education} + \beta_2 \text{Education}^2 + \beta_3 \text{Treatment}
$$

```{r dependency2, echo=FALSE, out.width="80%"}
ggplot(all_data, aes(x = education, y = outcome, fill = treatment, color = treatment)) +
  geom_point(size = 5, pch = 21, color = "white") +
  geom_abline(slope = filter(model_wrong, term == "education")$estimate, 
              intercept = filter(model_wrong, term == "(Intercept)")$estimate,
              color = "#440154FF", size = 0.75) +
  geom_abline(slope = filter(model_wrong, term == "education")$estimate, 
              intercept = filter(model_wrong, term == "(Intercept)")$estimate +
                filter(model_wrong, term == "treatmentTreated")$estimate,
              color = "#7AD151FF", size = 0.75) +
  geom_line(data = model_wrong1_fitted, 
            aes(x = education, y = .fitted, color = treatment),
            size = 0.75) +
  scale_fill_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) +
  scale_color_manual(values = c("#440154FF", "#7AD151FF"), guide = FALSE) +
  labs(x = "Education", y = "Outcome") +
  coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) +
  # theme_bw(base_size = 20, base_family = "Fira Sans Condensed") +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0))
```

## Model Dependence: Subset {.smaller}

$$
\color{white}{\text{Outcome} = \beta_0 + \beta_1 \text{Education} + \beta_2 \text{Education}^2 + \beta_3 \text{Treatment}}
$$

```{r reduced, echo=FALSE, out.width="80%"}
ggplot(all_data, aes(x = education, y = outcome, fill = treatment)) +
  geom_point(aes(alpha = type), size = 5, pch = 21, color = "white") +
  scale_fill_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) +
  scale_alpha_manual(values = c(1, 0.4), name = NULL) +
  guides(alpha = FALSE, color = FALSE) +
  labs(x = "Education", y = "Outcome") +
  coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) +
  # theme_bw(base_size = 20, base_family = "Fira Sans Condensed") +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0))
```

## Model Dependence: Subset {.smaller}

$$
\color{white}{\beta_0 \text{E}^2} \text{Outcome} = \beta_0 + \beta_1 \text{Education} + \beta_2 \text{Treatment} \color{white}{\beta_0 \text{E}^2}
$$

```{r reduced-dependency1, echo=FALSE, out.width="80%"}
ggplot(all_data, aes(x = education, y = outcome, fill = treatment)) +
  geom_point(aes(alpha = type), size = 5, pch = 21, color = "white") +
  geom_abline(slope = filter(model_better, term == "education")$estimate, 
              intercept = filter(model_better, term == "(Intercept)")$estimate,
              color = "#440154FF", size = 0.75) +
  geom_abline(slope = filter(model_better, term == "education")$estimate, 
              intercept = filter(model_better, term == "(Intercept)")$estimate +
                filter(model_better, term == "treatmentTreated")$estimate,
              color = "#7AD151FF", size = 0.75) +
  scale_fill_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) +
  scale_alpha_manual(values = c(1, 0.4), guide = FALSE) +
  labs(x = "Education", y = "Outcome") +
  coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) +
  # theme_bw(base_size = 20, base_family = "Fira Sans Condensed") +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0))
```

## Model Dependence: Subset {.smaller}

$$
\text{Outcome} = \beta_0 + \beta_1 \text{Education} + \beta_2 \text{Education}^2 + \beta_3 \text{Treatment}
$$

```{r reduced-dependency2, echo=FALSE, out.width="80%"}
ggplot(all_data, aes(x = education, y = outcome, fill = treatment)) +
  geom_point(aes(alpha = type), size = 5, pch = 21, color = "white") +
  geom_abline(slope = filter(model_better, term == "education")$estimate, 
              intercept = filter(model_better, term == "(Intercept)")$estimate,
              color = "#440154FF", size = 0.75) +
  geom_abline(slope = filter(model_better, term == "education")$estimate, 
              intercept = filter(model_better, term == "(Intercept)")$estimate +
                filter(model_better, term == "treatmentTreated")$estimate,
              color = "#7AD151FF", size = 0.75) +
  geom_line(data = model_better1_fitted, 
            aes(x = education, y = .fitted, color = treatment),
            size = 0.75) +
  scale_fill_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) +
  scale_color_manual(values = c("#440154FF", "#7AD151FF"), guide = FALSE) +
  scale_alpha_manual(values = c(1, 0.4), guide = FALSE) +
  labs(x = "Education", y = "Outcome") +
  coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) +
  # theme_bw(base_size = 20, base_family = "Fira Sans Condensed") +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0))
```

<!-- --- -->

<!-- **How do we know that we can remove these points?** -->

<!-- ```{r reduced-again, echo=FALSE, fig.width=12, fig.height=5.75, out.width="100%"} -->

<!-- ggplot(all_data, aes(x = education, y = outcome, fill = treatment)) + -->

<!--   geom_point(aes(alpha = type), size = 5, pch = 21, color = "white") + -->

<!--   scale_fill_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) + -->

<!--   scale_alpha_manual(values = c(1, 0.4), name = NULL) + -->

<!--   guides(alpha = FALSE, color = FALSE) + -->

<!--   labs(x = "Education", y = "Outcome") + -->

<!--   coord_cartesian(xlim = c(8, 30), ylim = c(0, 15)) + -->

<!--   # theme_bw(base_size = 20, base_family = "Fira Sans Condensed") + -->

<!--   theme(legend.position = "bottom", -->

<!--         legend.margin = margin(t = 0, b = 0)) -->

<!-- ``` -->

## Matching in Nutshell {.smaller}

-   We want to mimic the *gold standard*, RCT, by artificially creating a
    *"control"* group from the untreated observations to compare to our
    *treated* group
-   In regression, we did this by removing explained variation with statistical
    controlling and comparing *within* values of control variables
    (explain-and-subtract the differences)
-   In matching, we are doing this more explicitly by constructing a *"control"*
    group so that we compare treated cases to untreated ones that are closest on
    the values of all confounders
-   We can't observe what would have happened in the *counterfactual* where
    treated units didn't get treatment, but if we pick the most-comparable
    untreated group possible, that's about as close as we can get
-   Assuming that the variables we picked are enough to block all the back
    doors, then such an artificially selected control group closes all the back
    doors
-   Matching works because picking a sample where people have similar levels of
    $W$ is one way of controlling for $W$: once we remove all the variation
    related to $W$, the leftover variation is explained by the treatment

## Workflow for Matching {.smaller}

**Step 1: Preprocessing. Do something to guess or model the assignment to
treatment**

0.  Hide the your main outcome variable for a while
1.  Pick a set of variables (confounders) to match on, i.e. *matching variables*
2.  Separate out the treated and untreated cases
3.  For each treated observation, check how "close" each untreated observation
    is on the matching variables and select the closest one (or few)

**Step 2: Estimation. Use the new trimmed/preprocessed data to build a model and
calculate the effects**

4.  Go back to the main model and compare the average treated outcome vs. the
    average untreated outcome

**Matching methods differ in how they deal with steps 3 and 4.**

# Various Matching Methods

## Example for Two Counfounders

```{dot}
//| echo: false
//| fig-width: 6
//| fig-height: 4
//| out-width: 100%
digraph D {
  node [shape=oval];
  edge [len = 1.2, arrowhead = vee];
  a [label = "Treatment"];
  b [label = "Outcome"];
  c [label = "Education"];
  d [label = "Age"];
  
  {rank=same a b};
  {rank=same d c};
  {rank=sink a};
  a->b;
  c->a;
  c->b;
  d->a;
  d->b;
}
```

```{r, include=FALSE}
set.seed(1234)
edu_age_treated <- tibble(education = rnorm(30, 20, 3),
                          age = rnorm(30, 50, 8),
                          treatment = "Treated")

edu_age_untreated <- tibble(
  education = runif(
    70,
    min(edu_age_treated$education) - 2,
    max(edu_age_treated$education) + 2
  ),
  age = runif(70,
              min(edu_age_treated$age) - 5,
              max(edu_age_treated$age) + 5),
  treatment = "Untreated"
) 
  # bind_rows(slice_sample(edu_age_treated, n = 10) %>% 
  #             mutate(treatment = "Untreated"))

edu_age <- bind_rows(edu_age_treated, edu_age_untreated) %>% 
  mutate(treat_bin = recode(treatment, Treated = 1, Untreated = 0)) %>% 
  mutate(id = 1:n()) %>% 
  mutate(treatment = factor(treatment, levels = c("Untreated", "Treated")),
         outcome = 2 + 0.5 * age + 2 * education + 5 * I(treatment == "Treated") + rnorm(100))
dat <- edu_age
```

## Exact Matching {.smaller}

1.  **Preprocessing step**

-   Choose matches for treated units that have the *same* value in each $X$
-   Discard all unmatched control units

2.  **Estimation step**

-   Calculate the effect of interest

::: task
-   Straightforward process
-   No matching bias introduced
-   Not feasible with high-dimesional or continuous data
-   Strongly reduces the number of units if works
:::

## Exact Matching: No Observations Matched {.smaller}

```{r, echo=FALSE, message=FALSE}
ggplot(
  edu_age %>%
    mutate(matched = 0) %>%
    arrange(treatment), # plot treated units after untreated (avoid overlapping)
  aes(education,
    age,
    fill = treatment,
    alpha = factor(matched)
  )
) +
 geom_point(size = 5, pch = 21, color = "white") +
  scale_fill_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) +
  scale_alpha_manual(values = c(0.3, 0.7)) +
  guides(color = FALSE,
         alpha = F)  +
  labs(x = "Education", y = "Age") +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0)) +
  theme(legend.position = "bottom")
```

## Coarsened Exact Matching {.smaller}

1.  **Preprocessing step**

-   Discretize and group covariates into substantively meaningful bins
-   Exact match on these bins ⇝ accounts for interactions
-   Have to drop treated units in bins with no controls (lack of overlap) ⇝
    changes estimand
-   Allows you to control bias/variance trade-off through coarsening

2.  **Estimation step**

-   Calculate the effect of interest (ATE, ATT, ATC)
-   Weight the untreated observations so each treated observation is matched to
    the same number of untreated ones

::: task
-   Allows to control the amount of imbalance up front by setting the degree of
    the coarsening: coarser means more imbalance, fineness means less imbalance
    but also fewer matched units
-   Can still break down in high dimensional datasets
:::

## Coarsened Exact Matching Results {.smaller}

```{r}
#| echo: false
cem <- matchit(treatment ~ age + education, data = edu_age,
                   method = "cem")

ggplot(
  edu_age %>%
    mutate(weights = cem$weights,
           subclass = cem$subclass) %>%
    group_by(subclass) %>%
    mutate(matched = if_else(
      is.na(subclass),
      "Not matched",
      "Matched")
    ) %>%
    arrange(treatment, matched),
  aes(
    x = education,
      y = age,
    fill = factor(treatment),
    alpha = factor(matched) %>% fct_infreq(),
    shape = subclass
    # size = n
  )
) +
  geom_point(
    # alpha = 0.7,
    pch = 21,
    size = 5,
    color = "grey80",
    width = 0.1,
    height = 0.01
  ) +
  ggrepel::geom_label_repel(aes(label = subclass,
                                color = subclass),
                            # alpha = 0.5,
                            fill = "white",
                            # nudge_x = 0.4, nudge_y = 0.4,
            size = 6) +
  scale_fill_viridis_d(end = 0.8) +
  guides(alpha = "none",
         color = "none") +
  labs(fill = "",
       size = "Units in Subgroup",
       x = "Education", y = "Age",
       # y = "Share of Black Voters in District",
       # x = "Median Household Income in District"
       ) +
  theme(legend.position = "bottom",
        legend.box = "vertical")

# ggplot(
#   edu_age %>%
#     mutate(matched = 0) %>%
#     arrange(treatment), # plot treated units after untreated (avoid overlapping)
#   aes(education,
#     age,
#     fill = treatment,
#     alpha = factor(matched)
#   )
# ) +
#  geom_point(size = 5, pch = 21, color = "white") +
#   scale_fill_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) +
#   scale_alpha_manual(values = c(0.3, 0.7)) +
#   guides(color = FALSE,
#          alpha = F)  +
#   labs(x = "Education", y = "Age") +
#   theme(legend.position = "bottom",
#         legend.margin = margin(t = 0, b = 0))
#   theme(legend.position = "bottom")
```

```{r, include=FALSE}
dat <- dat %>%
  mutate(ed_coarse = cut(education, breaks = 3),
         age_coarse = cut(age, breaks = 5))

control <- dat %>% filter(treatment != 'Treated') %>%
  group_by(ed_coarse, age_coarse)  %>%
  summarize(untreated_outcome = mean(outcome))
treatment <- dat %>% filter(treatment == 'Treated') 

treatment %>% inner_join(control) %>%
  summarize(treated_mean = mean(outcome),
            untreated_mean = mean(untreated_outcome))
```

## Nearest Neighbor Matching {.smaller}

1.  **Preprocessing step**

-   Find untreated observations that are very close/similar to treated
    observations based on confounders
-   We can define closeness using *lower-dimensional* distance metrics, i.e. we
    reduce dimensionality in confounders
-   Many different way to calculate the distance, e.g. propensity score,
    Euclidean, or Mahalanobis distance (accounts for covariances between
    variables)

2.  **Estimation step**

-   Calculate the effect of interest on matched dataset

::: task
-   Order of the matching matters in terms of which units get matched to which
    other units (when matching without replacement)
:::

## Distance Measures {.smaller}

::: columns
::: {.column width="50%"}
#### Euclidean distance

For observations $X_1$ and $X_2$ with $K$ number of matching variables:

$$
d(X_1,X_2) = \sqrt{\frac{\sum_{k=1}^K(X_{1k} - X_{2k})^2}{\hat\sigma^2_k}},
$$ where $\hat\sigma^2_k$ is the standard deviation of the $k$th variable

-   Standardize the matching variables for them to have equal weight on the
    measure (divide by variables' st. deviation)
-   For each matching variable, compute the difference between values for two
    observations of interest and square that difference
-   Sum the squared differences to get a single number and take a square root of
    that number
:::

::: {.column width="50%"}
#### Mahalanobis distance

$$
d(X_1,X_2) = \sqrt{{(X_1-X_2)}'\widehat\Sigma^{-1}(X_1-X_2)}
$$

-   Euclidean distance adjusted for covariance in the data, $\hat\Sigma$ (i.e.
    not only the variance of each variables separately as before)
-   Intuition: if $X_k$ and $X_{k'}$ are two covariates that are highly
    correlated, then their contribution to the distances should be lower
    -   Easy to get close on correlated covariates ⇝ downweight
    -   Harder to get close on uncorrelated covariates ⇝ upweight
-   **This is what we want to use!**
:::
:::

## Distance Measures {.smaller}

![](images/dist.jpeg)

## 1:1 Nearest Neighbour Matching without Replacement {.smaller}

```{r edu-age, echo=FALSE}
set.seed(1234)
edu_age_treated <- tibble(education = rnorm(30, 20, 3),
                          age = rnorm(30, 50, 8),
                          treatment = "Treated")

edu_age_untreated <- tibble(education = runif(70, 
                                              min(edu_age_treated$education) - 2, 
                                              max(edu_age_treated$education) + 2),
                            age = runif(70,
                                        min(edu_age_treated$age) - 5, 
                                        max(edu_age_treated$age) + 5),
                            treatment = "Untreated")

edu_age <- bind_rows(edu_age_treated, edu_age_untreated) %>% 
  mutate(treat_bin = recode(treatment, Treated = 1, Untreated = 0)) %>% 
  mutate(id = 1:n()) %>% 
  mutate(treatment = factor(treatment, levels = c("Untreated", "Treated")))

matched <- matchit(treat_bin ~ education + age, data = edu_age,
                   method = "nearest", distance = "mahalanobis")

matched_pairs <- tibble(trt = rownames(matched$match.matrix),
                        ctrl = matched$match.matrix) %>% 
  mutate_all(as.numeric)

edu_age_filtered <- edu_age %>% 
  filter(id %in% c(matched_pairs$trt, matched_pairs$ctrl)) %>% 
  left_join(matched_pairs, by = c("id" = "trt")) %>% 
  left_join(select(edu_age, id, education_ctrl = education, age_ctrl = age),
            by = c("ctrl" = "id"))

edu_age_matched <- edu_age %>% 
  left_join(matched_pairs, by = c("id" = "trt")) %>% 
  left_join(select(edu_age, id, education_ctrl = education, age_ctrl = age),
            by = c("ctrl" = "id")) %>% 
  filter(treatment == "Treated")
```

```{r edu-age-unmatched, echo=FALSE,  out.width="100%"}
ggplot(edu_age, aes(x = education, y = age, fill = treatment)) +
  geom_point(size = 5, pch = 21, color = "white") +
  scale_fill_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) +
  guides(color = FALSE)  +
  labs(x = "Education", y = "Age") +
  # theme_bw(base_size = 20, base_family = "Fira Sans Condensed") +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0))
```

## 1:1 Nearest Neighbour: Pairs {.smaller}

```{r edu-age-matched, echo=FALSE,  out.width="100%"}
ggplot(edu_age, aes(x = education, y = age, fill = treatment)) +
  geom_point(size = 5, pch = 21, color = "white", alpha = 0.4) +
  geom_point(data = edu_age_filtered, size = 5, pch = 21, color = "white") +
  geom_segment(data = edu_age_matched, 
               aes(x = education, xend = education_ctrl,
                   y = age, yend = age_ctrl),
               linetype = "11", color = "grey50", size = 1) +
  scale_fill_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) +
  scale_color_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) +
  guides(color = FALSE)  +
  labs(x = "Education", y = "Age") +
  # theme_bw(base_size = 20, base_family = "Fira Sans Condensed") +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0))
```

## 1:1 Nearest Neighbour: Matched Data {.smaller}

```{r edu-age-trimmed, echo=FALSE, out.width="100%"}
ggplot(edu_age, aes(x = education, y = age, fill = treatment)) +
  geom_point(data = edu_age_filtered, size = 5, pch = 21, color = "white") +
  scale_fill_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) +
  scale_color_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) +
  guides(color = FALSE) +
  labs(x = "Education", y = "Age") +
  coord_cartesian(xlim = c(min(edu_age$education), max(edu_age$education)),
                  ylim = c(min(edu_age$age), max(edu_age$age)))  +
  # theme_bw(base_size = 20, base_family = "Fira Sans Condensed") +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0))
```

## Problem: Curse of Dimensionality {.smaller}

![](https://www.i2tutorials.com/wp-content/media/2019/09/Curse-of-Dimensionality-i2tutorials.png)

-   As the number of matching variables grows, the likelihood of finding exact
    matches, or approximate ones, for all units falls quickly
-   The more covariates we have, the higher will be the distance between units
    and their matches

## Other Matching Choices {.smaller}

-   **Matching ratio:** how many control units per treated?
    -   Lower reduces bias (only use the closest matches)
    -   Lower increases variance
    -   ATT is easy to calculate with 1:1 exact matches on the treated units
    -   If number of control units varies by treated unit, need to weight
        observations to ensure balance
-   **With or without replacement:** same control matched to multiple treated?
    -   With replacement gives better matches & matching order doesn't matter
    -   Without replacement simplifies variance estimation
-   **Caliper:** drop poor matches?
    -   Only keep matches below an arbitrary distance threshold
    -   Reduces imbalance, but if you drop treated units, estimand changes from
        ATT (ATE)

# Inverse Probability Weighting

## Weighting {.smaller}

> Weighting for surveys: down-weight over-sampled respondents

```{r, echo=FALSE}
wts <- tribble(
  ~` `, ~Young, ~Middle, ~Old,
  "Population", "30%", "40%", "30%",
  "Sample", "60%", "30%", "10%"
) 

wts %>% knitr::kable(align = "lccc")
```

. . .

```{r, echo=FALSE}
wts %>% 
  add_row(` ` = "Weight",
          Young = "30 / 60<br>**0.5**",
          Middle = "40 / 30<br>**1.333**",
          Old = "30 / 10<br>**3**") %>% 
  knitr::kable(align = "lccc")
```

-   Multiply weights by average values (or use in regression) to adjust for
    importance

## Propensity Scores {.smaller}

1.  **Preprocessing step:**

-   Predict the probability of assignment to treatment using a model (e.g. with
    logit). This is the *propensity score*.

$$
\operatorname{log} \frac{p_\text{Treated}}{1 - p_\text{Treated}} = \beta_0 + \beta_1 \text{Education} + \beta_2 \text{Age}
$$

2.  **Estimation step:** Calculate the effect of interest

-   Use propensity scores (predicted probabilities) to weight observations by
    how "weird" they are:
    -   Observations with high probability of treatment who don't get it (and
        vice versa) have higher weight

$$
IPW = \frac{\text{Treatment}}{\text{Propensity}} + \frac{1 - \text{Treatment}}{1 - \text{Propensity}}
$$

::: task
-   Propensity scores should **not** be used for NN matching as a measure of
    distance (King and Nielsen 2019)
-   Instead, we can use them to weight each observation in the control group
    such that it looks like the treatment group
-   You may want to trim the weights when they get too high (e.g., above 10)
:::

## Inverse Probability Weights {.smaller}

```{r}
glm(treatment ~ age + education, edu_age, family = "binomial") %>%
  augment(type.predict = "response") %>%
  mutate(treatment = treatment %>% as.numeric() - 1) %>%
  dplyr::select(education, treatment, age, propensity = .fitted) %>%
  mutate(ip_weight = (treatment / propensity) + ((1 - treatment) / (1 - propensity))) %>% 
  slice_sample(n = 6) %>%
  kable()
```

-   Inverse-probability weighting removes confounding by creating a
    "pseudo-population" in which the treatment is independent of the measured
    confounders
-   Units who were assigned to the treatment group even though they were much
    more likely to be assigned to the control group are a rare, and we want to
    upweight them

--------------------------------------------------------------------------------

```{r edu-age-ipw, echo=FALSE, fig.width=12, fig.height=6.5, out.width="100%"}
logit_edu_age <- glm(treatment ~ education + age, data = edu_age,
                     family = binomial(link = "logit"))

edu_age_ipw <- augment(logit_edu_age, edu_age, type.predict = "response") %>%
  mutate(p_treatment = .fitted) %>% 
  mutate(w_ate = (treat_bin / p_treatment) + ((1 - treat_bin) / (1 - p_treatment)))

ggplot(edu_age_ipw, aes(x = education, y = age, fill = treatment, size = w_ate)) +
  geom_point(pch = 21, color = "white") +
  scale_fill_manual(values = c("#440154FF", "#7AD151FF"), name = NULL) +
  scale_size_continuous(range = c(1, 7)) +
  guides(color = FALSE, size = FALSE) +
  labs(x = "Education", y = "Age") +
  guides(fill = guide_legend(override.aes = list(size = 5))) +
  # theme_bw(base_size = 20, base_family = "Fira Sans Condensed") +
  theme(legend.position = "bottom",
        legend.margin = margin(t = 0, b = 0))
```

<!-- Different ways to measure distance -->

<!-- Mahalanobis distance / Euclidean distance -->

# Covariate Balance Assessment

## Covariate Balance

-   Goal of matching is to maximize covariate balance across *treatment* and
    *"control"* groups
    -   Ideally, we need to see if the joint distribution of all covariates in
        matching variables is similar between treated and matched controls
    -   In practice, check lower-dimensional summaries (e.g., standardized mean
        difference, variance ratio, empirical CDF difference)
-   Hypothesis tests for balance are problematic: Dropping units can lower power
    (↑ p-values) without a change in balance
-   If *"control"* and *treatment* groups differ, on average, try again

## Covariate Balance Plot for NN Matching

```{r, echo=FALSE}
love.plot(matched, binary = "std", thresholds = c(m = .1))
```

## Problems When Using Summary Stats

![](https://theeffectbook.net/the-effect_files/figure-html/matching-bad-balance-distribution-1.png)

## More Details on Workflow

1.  Check the balance before the matching
2.  Decide on covariates for which balance must be achieved
3.  Choose matching type (compute/estimate the distance/balancing score if
    necessary)
4.  Condition on the distance measure (e.g., using matching, weighting, or
    subclassification), i.e. create matched dataset
5.  Assess balance on the covariates of interest; if poor, repeat steps 2-5
6.  Estimate the treatment effect in the conditioned sample (i.e. matched
    dataset)

## Assumptions {.smaller}

#### Conditional Independence Assumption

-   The set of matching variables you've chosen is enough to close all back
    doors

#### Common Support Assumption

-   There are appropriate control observations to match with treated units
-   There must be substantial overlap in the distributions of the matching
    variables comparing the treated and control observations

#### Balance

-   The approach to selecting a matched group has closed back doors for the
    variables we're interested in

## Main Takeaways

-   Matching is a technique to reduce model dependence and avoid parametric
    modeling assumptions when no unmeasured confounders holds
-   Lots of different ways to match, each has advantages and disadvantages. Try
    different methods and aim for best covariate balance
-   Pay careful attention to the quantity of interest when you drop units
