---
title: "Binary Dependent Variable"
subtitle: "Data Analytics and Visualization with R<br>Session 9"
title-slide-attributes:
  data-background-size: stretch
  data-slide-number: none
auto-stretch: false
institute: "University of Mannheim<br>Spring 2023"
author: "Viktoriia Semenova"
footer: "[ðŸ”— r4da.live](https://r4da.live/)"
logo: images/logo.png
format:
  revealjs:
    theme: ../slides.scss
    transition: fade
    incremental: true   
    slide-number: true
    chalkboard: true
execute:
  echo: true
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
p_needed <- c("tidyverse", "janitor", "icons", "infer", "haven",
              "countdown", "showtext", "ggdag", "magrittr", "knitr",
              "gt", "plotly", "broom", "patchwork", "modelsummary")

# check if they are already installed, install if not installed
lapply(p_needed[!(p_needed %in% rownames(installed.packages()))], install.packages, repos = "http://cran.us.r-project.org")

# load the packages
lapply(p_needed, library, character.only = TRUE)

# set width of code output
options(width = 90)

# set figure parameters for knitr
knitr::opts_chunk$set(
  fig.width = 7, # 7" width
  fig.asp = 0.618, # the golden ratio
  fig.retina = 3, # dpi multiplier for displaying HTML output on retina
  fig.align = "center", # center align figures
  dpi = 300 # higher dpi, sharper image
)


font_add_google(name = "Gochi Hand")
showtext::showtext_auto()
# set theme for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 14)) +
  theme(plot.title = element_text(face = "bold"),
        plot.title.position = "plot",
        panel.grid.minor = element_blank())

# governors <- readr::read_csv("https://r4da.live/files/data/external_data/governors.csv")
# trains <- readr::read_tsv("../data/trains.tsv") %>% clean_names()
UA <- read_csv("../data/UA_survey.csv") %>% clean_names()
cabinet <- read_dta("../data/cabinet_size_replication_data.dta")
judges <- read_rds("../data/judges.rds")
cdat <- read_csv("cdat.csv")
set.seed(2508)
```

# Intro

## Housekeeping {.smaller}

-   Final paper: June 19-July 3, 2023 (?)
-   Blog post extra assignment:
    -   Pick a topic in Data visualization/wrangling area and make a 1-5-2 page
        tutorial
    -   Select an (polisci) dataset and make a visualization that answers a RQ
        with these data
    -   Alone or in pairs, by Monday May 29 latest

## Quiz: Which of these statements are correct? {.smaller}

```{r}
#| echo: false
countdown(minutes = 4, color_background = "white",top = "0")
```

Indridason and Bowler (2014) explore the determinants of cabinet size in parliamentary systems. Below you can find a plot based on one of their model. 
<!-- $$\widehat {Cabinet~Size} = \hat{\beta}_{0} + \hat{\beta}_{1}(\operatorname{Legislature Size}) + \hat{\beta}_{2}(\operatorname{Legislature Size}) + \hat{\beta}_{3}(\operatorname{Population Size}) + \hat{\beta}_{4}(\operatorname{EU membership}) + \epsilon$$ -->

::: columns
::: {.column width="50%"}
1.  Systematic component of the model likely includes variable *Legislature Size* interacted with another variable.   
2.  Marginal effect of the variable *Legislature Size* is constant across all values of *Legislature Size* variable. 
3.  The relationship between *legislature size* and *cabinet size*  is strongest for smaller values of *legislature size*. 
4. For legislatures with sizes above 500, there is, on average, no significant effect of *legislature size* on *cabinet size*. 
5. *Legislature size* seems to be inversely related to *cabinet size*. 
:::

::: {.column width="50%"}
::: r-stack
```{r}
#| echo: false
#| fig-asp: 1
m5 <- lm(noministers ~ sizeleg + I(sizeleg^2) + popint + eu + as.factor(country),
  data = cabinet
)

# cdat <- margins::cplot(m5, "sizeleg", what = "effect", draw = F)
ggplot(cdat, aes(x = xvals * 100)) + 
  geom_line(aes(y = yvals),  color = "#800010") +
  geom_ribbon(aes(ymax = upper, ymin = lower), fill = "#80001050") +
  geom_line(aes(y = upper), color = "#800010") +
  geom_line(aes(y = lower), color = "#800010") +
  geom_hline(yintercept = 0, linetype = 2) +
  # xlim(50, 650) +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  # ggtitle("AME of Axle Ratio on Fuel Economy (mpg) by Weight") +
  xlab("Legislature Size") + ylab("Marginal Effect of Legislature Size") +
  geom_rug(
    data = cabinet,
    aes(x = sizeleg * 100)
  )
# summary(mult.fit)
# # create a grid for all possible combinations 
# # tibble() only allows for different values in one variable 
# plt <- expand_grid(
#   any_girls = 0:1,
#   female = 0:1,
#   age = mean(evals$age, na.rm = T),
#   minority = 0,
#   nonenglish = 0
# ) 
# 
# predict(m0,
#         interval = "confidence",
#         newdata = plt,
#         level = 0.95) %>%
#   bind_cols(plt) %>%
#     mutate(female = if_else(female == 1, "Female", "Male")) %>%
#   ggplot(aes(beauty, y = fit, 
#              color = as_factor(female),
#              fill  = as_factor(female))) +
#   labs(
#     x = "Beauty",
#     y = "Average Teaching Evaluation",
#     title = "Relationship between Instructor Beauty\nand Course Evaluations",
#     # subtitle = "Intercept Shift with Same Slope",
#     color = "Instructor is",
#     fill = "Instructor is"
#   ) +
#   geom_line() +
#   geom_ribbon(aes(ymin = lwr,
#                   ymax = upr), 
#                 ) +
#   # geom_pointrange(aes(y = fit, ymin = lwr, ymax = upr)) + # plots the parallel lines
#   scale_color_viridis_d(end = 0.8, alpha = 0.8) +
#   scale_fill_viridis_d(end = 0.8, alpha = 0.5) +
#   theme(legend.position = "top") +
#   scale_x_continuous(limits = c(1, 10), breaks = 1:10) +
#   ylim(c(1, 5)) +
#   theme(panel.grid.minor = element_blank(),
#         plot.title.position = "plot",
#         text = element_text(size = 20))
#   #  geom_rug(
#   #    data = augment(m0) %>%
#   #   mutate(female = if_else(female == 1, "Female", "Male")),
#   #    aes(x = beauty, y = eval),
#   #   alpha = 0.3,
#   #   sides = "b",
#   #   position = "jitter",
#   #   # size = 4,
#   #   length = unit(0.05, "npc")
#   # )
```

::: fragment
```{r}
#| echo: false
#| fig-asp: 1
augment(m5, interval = "c", 
        newdata = tibble(sizeleg = seq(0.51, 7, length.out = 50), 
                                             popint = 81.41416,
                                             eu = 1,
                                             country = "GER")) %>%
  ggplot(aes(x = sizeleg * 100, y = .fitted)) +
  geom_smooth(color = "black") +
  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 0.5) +
  labs(
    x = "Legislature Size",
    y = "Cabinet Size",
    title = "Legislative Size Affects Cabinet Size",
    subtitle = "Expected Values and 99% CIs"
  ) 
```
:::
:::
:::
:::

# Binary Dependent Variable

## Does TV Propaganda Affect Voting? {.smaller}

```{dot}
//| echo: false
//| out-width: 100%
digraph D {
  node [shape=ellipse];
  edge [len = 1.2, arrowhead = vee];
  a [label = "Russian TV\nReception"];
  b [label = "Pro-Russian Vote"];
  c [label = "Living within\n25 km from Border"];
  
  {rank=same a b};
  {rank=sink c};
  a->b;
  c->a;
  c->b;
}
```

## Data {.smaller}

```{r,echo=FALSE}
datasummary_skim(UA)
```

- `russian_tv`: indicator for whether voter's precinct received Russian TV (1) or not (0)
- `pro_russian_vote`: indicator for whether respondent voted for pro-Russian party in 2014 Ukrainian elections (1) or not (0)
- `within_25km`: indicator for whether respondent's precinct is within 25 kilometers from Russian border (1) or not (0)

## Linear Probability Model {.smaller}

$$\text{Pro-Russian Vote} \sim \beta_0 + \beta_1 \text{Russian TV} + \beta_2 \text{Living within 25km} + \varepsilon$$

```{r}
#| echo: false
#| out-height: 80%
m1 <- lm(pro_russian_vote ~ russian_tv + within_25km, data=UA)
newdata <- expand_grid(russian_tv = 0:1,
                        within_25km = 0:1)
m1 %>%
  predict(interval = "c", newdata) %>%
  bind_cols(newdata) %>%
  ggplot(aes(
    y = fit,
    ymin = lwr,
    shape = if_else(within_25km == 0, "No", "Yes"),
    color = if_else(within_25km == 0, "No", "Yes"),
    ymax = upr,
    x = if_else(russian_tv == 0, "No", "Yes")
  )) +
  geom_pointrange(position = position_dodge(width = .5),
                  size = 1.2,
                  linewidth = 1.5) +
  labs(
    y = "Pro-Russian Vote",
    shape = "Living within 25 km",
    color = "Living within 25 km",
    x = "Russian TV Reception"
  ) +
  theme(legend.position = "bottom") +
  scale_color_viridis_d(end = 0.8) +
  ylim(-0.2, 1)
```

## Out-of-Bound Predictions

```{r}
#| echo: false
m1 %>%
  predict(interval = "c", newdata) %>%
  bind_cols(newdata) %>%
  ggplot(aes(
    y = fit,
    ymin = lwr,
    shape = if_else(within_25km == 0, "No", "Yes"),
    color = if_else(within_25km == 0, "No", "Yes"),
    ymax = upr,
    x = if_else(russian_tv == 0, "No", "Yes")
    # x = russian_tv
  )) +
    geom_hline(yintercept = c(0,1), lty = 2, alpha = 0.5) +
  geom_pointrange(position = position_dodge(width = 0.3),
                  size = 1.2,
                  linewidth = 1.5) +
  labs(
    y = "Pro-Russian Vote",
    shape = "Living within 25 km",
    color = "Living within 25 km",
    x = "Russian TV Reception"
  ) +
  theme(legend.position = "bottom") +
  scale_color_viridis_d(end = 0.8) +
  ylim(-0.2, 1) +
  geom_text(aes(label = round(fit, 3),
                y = fit + 0.005), 
            size = 4.5,
            position = position_dodge(width = -0.4))
```

## Logistic Regression Solution {.smaller}

-   Apply a transformation to the *linear predictor*
    $\beta_0 + \beta_1 \text{Russian TV} + \beta_2 \text{Living within 25km}$,
    to ensure that outcome is bounded between 0 and 1

-   The inverse logit (aka sigmoid) function takes a value between $-\infty$ and
    $+\infty$ and maps it to a value between 0 and 1:

$$
logit^{-1}(x) = \frac{\exp(x)}{1+\exp(x)}
$$

## Logistic Function {.smaller}

```{r}
#| echo: false
#| out-height: 50%

inv_logit <- function(x) {
  exp(x) / (1 + exp(x))
}

ggplot(
  data = tibble(
    x = seq(-10, 10, length.out = 50),
    y = inv_logit(seq(-10, 10, length.out = 50))
  ),
  aes(x, y)
) +
  geom_line(alpha = 0.5)
```

---

```{r, echo=FALSE}
ggplot(mtcars, aes(x = mpg - 20, y = vs)) + geom_point() +
  stat_smooth(
    method = "glm",
    method.args = list(family = "binomial"),
    se = FALSE,
    color = "#800010",
    alpha = 0.6
  ) +
  stat_smooth(
    method = "lm",
    se = FALSE,
    fullrange = TRUE,
    color = "#003056"
  ) +
  xlim(-10, 15) +
  scale_y_continuous(breaks = c(0,  1)) +
  labs(y = "Binary Y",
       x = "Continuous X",
       # title = "Model Predictions"
       ) +
  annotate(
    geom = "text",
    x = 2,
    y = 0.8,
    label = "Logit",
    color = "#800010",
    # size = 10
  ) +
  annotate(
    geom = "text",
    x = 7,
    y = 0.7,
    label = "Linear",
    color = "#003056",
    # size = 10
  )
```

## Logistic Regression Model {.smaller}

-   $Y = 1:$ yes, $Y = 0:$ no; two mutually exclusive outcomes
-   $\pi = Pr(Y = 1)$: **probability** that $Y=1$
-   $\frac{\pi}{1-\pi}$: **odds** that $Y = 1$
-   $\log\Big(\frac{\pi}{1-\pi}\Big)$: **log odds**
-   Go from $\pi$ to $\log\Big(\frac{\pi}{1-\pi}\Big)$ using the **logit
    transformation**

. . .

$$\underbrace{\log\Big(\frac{\pi}{1-\pi}\Big)}_{\text{straight line}} = \underbrace{\beta_0 + \beta_1~X_1 + \cdots + \beta_k~X_k}_{\text{linear predictor}}$$

## Odds and Probabilities Example {.smaller}

Suppose there is a **70% chance** it will rain tomorrow

-   Probability it will rain is $\pi = \mathbf{p = 0.7}$
-   Probability it won't rain is $1 - \pi = \mathbf{1 - p = 0.3}$
-   Odds $\omega$ it will rain are **7 to 3**, **7:3**,
    $\omega = \frac{\pi}{1-\pi}= \mathbf{\frac{0.7}{0.3} \approx 2.33}$

. . . 

```{r}
pi <- seq(0, 1, length.out = 6)
tibble(pi, 1 - pi, "odds" = pi / (1 - pi), "log odds" = log(pi / (1 - pi))) %>% 
  kable()
```

## Probabilities, Odds, and Log Odds {.smaller}

**Odds**

$$\omega = \frac{\pi}{1-\pi}  = \exp\Big\{\log\Big(\frac{\pi}{1-\pi}\Big)\Big\} 
= \exp\{\beta_0 + \beta_1~X_1 + \cdots + \beta_k~X_k\}$$

**Log odds**

$$\log(\omega) = \log\Big(\frac{\pi}{1-\pi}\Big) = \beta_0 + \beta_1~X_1 + \cdots + \beta_k~X_k$$

**Probability**

$$\pi = \frac{\omega}{1 + \omega}  = \frac{\exp\{\beta_0 + \beta_1~X_1 + \cdots + \beta_k~X_k\}}{1 + \exp\{\beta_0 + \beta_1~X_1 + \cdots + \beta_k~X_k\}}$$

<!-- (1) **Logistic model**: log odds = $\log\Big(\frac{\pi}{1-\pi}\Big) = \beta_0 + \beta_1~X$ -->

<!-- (2) **Odds =** $\exp\Big\{\log\Big(\frac{\pi}{1-\pi}\Big)\Big\} = \frac{\pi}{1-\pi}$ -->

<!-- (3) Combining (1) and (2) with what we saw earlier -->

<!-- $$\text{Probability} = \pi = \frac{\exp\{\beta_0 + \beta_1~X_1 + \cdots + \beta_k~X_k\}}{1 + \exp\{\beta_0 + \beta_1~X_1 + \cdots + \beta_k~X_k\}}$$ -->

<!-- ## Logistic Regression Recap -->

<!-- -   Model for binary DV $\pi = Pr(Y = 1)$ -->
<!-- -   Coefficients represent expected changes in **log-odds**, hence no intuitive -->
<!--     interpretation as with linear models -->

## Fitting the Model in `R` {.smaller}

```{r, echo=FALSE}
# OLS
m1 <- lm(pro_russian_vote ~ russian_tv + within_25km, data = UA)
# logistic regression 
m2 <- glm(pro_russian_vote ~ russian_tv + within_25km, data = UA, family = binomial)
```

```{r}
#| echo: false
modelsummary(list("OLS" = m1, "Logit" = m2), gof_omit = "RMSE|AIC|BIC")
```

## Logit Coefficients Interpretation {.smaller}

$$
\log\left[ \frac { \widehat{Pr( \text{Pro-russian Vote} = \operatorname{1} )} }{ 1 - \widehat{Pr(  \text{Pro-russian Vote} = \operatorname{1} )} } \right] = -1.474 + 1.79 \cdot\text{Russian TV} - 1.32\cdot\text{Living within 25 km}
$$

-   Sign and significance are straightforward to interpret

. . .

> The **log-odds** of voting for a pro-Russian party are expected to be 1.79
> more for those exposed to Russian TV compared to those without exposure to
> Russian TV (the baseline group), holding all else constant.

> The **odds** of voting for a pro-Russian party or those exposed to Russian TV
> are expected to be 5.98 ($e^{1.79}$) **times** the odds for those without
> exposure to Russian TV, holding all else constant.

$$\text{Odds Ratio}  = e^{\hat{\beta}_j} = \exp\{\hat{\beta}_j\}$$

## Predicted Probabilities Plot

```{r}
#| echo: false
m2 %>%
  augment(newdata = newdata, type.predict = "response") %>%
  # bind_cols(newdata) %>%
  ggplot(aes(
    y = .fitted,
    # ymin = lwr,
    shape = if_else(within_25km == 0, "No", "Yes"),
    color = if_else(within_25km == 0, "No", "Yes"),
    # ymax = upr,
    x = if_else(russian_tv == 0, "No", "Yes")
    # x = russian_tv
  )) +
  geom_point(position = position_dodge(width = 0.3),
             size = 3) +
  labs(
    y = "Probability of Pro-Russian Vote",
    shape = "Living within 25 km",
    color = "Living within 25 km",
    x = "Russian TV Reception"
  ) +
  theme(legend.position = "bottom") +
  scale_color_viridis_d(end = 0.8) +
  ylim(0, 1) +
  geom_text(aes(label = round(.fitted, 3),
                y = .fitted + 0.005), size = 4.5,
            position = position_dodge(width = -0.3))
  # geom_hline(yintercept = c(0,1), lty = 2) 
```

## Predicted Probabilities By Hand {.smaller}

$$
\log\left[ \frac { \widehat{Pr( \text{Pro-russian Vote} = \operatorname{1} )} }{ 1 - \widehat{Pr(  \text{Pro-russian Vote} = \operatorname{1} )} } \right] = -1.474 + 1.79 \cdot\text{Russian TV} - 1.32\cdot\text{Living within 25 km}
$$

. . . 


$$
 \hat\pi = \widehat{Pr( \text{Pro-russian Vote} = \operatorname{1} )} =\frac{exp(-1.474
 + 1.79 \cdot \text{Russian TV} - 1.32\cdot\text{Living within 25 km})}{1 + exp(-1.474 + 1.79 \cdot \text{Russian TV}- 1.32\cdot\text{Living within 25 km})}
$$ 

. . . 

$$
\widehat{Pr( \text{Pro-russian Vote} = \operatorname{1} |~\text{Russian TV} = 1,~\text{Living within 25 km} = 1)} \\=\frac{exp(-1.474 + 1.79  - 1.32)}{1 + exp(-1.474 + 1.79  - 1.32)}\\
= \frac{exp(-1.004)}{1 + exp(1 -1.004)} \approx \frac{0.366}{1.996} \approx 0.18
$$

. . . 

$$
\widehat{Pr( \text{Pro-russian Vote} = \operatorname{1} |~\text{Russian TV} = 0,~\text{Living within 25 km} = 1)} \\=\frac{exp(-1.474  - 1.32)}{1 + exp(-1.474   - 1.32)}\\
= \frac{exp(-2.794)}{1 + exp(1 -2.794)} \approx \frac{0.061}{1.166} \approx 0.05
$$


## Predicted Values {.smaller}

- Transforming probabilities $\pi$ back to 0/1 scale of the *response* variable $Y$
- Requires threshold for which values translate to $Y = 1$ and $Y = 0$; $\pi > 0.5$ most common 

```{r}
#| echo: false
# augment(m2, type.predict = "link") # .fitted is log odds
m2_df <- augment(m2, type.predict = "response") %>%
  mutate(prediction = if_else(.fitted > 0.5, 1, 0))
m2_df %>% dplyr::select(pro_russian_vote:.fitted, prediction) %>%
  slice_head(n = 10) %>% kable()
```

## Model Predictions vs. Actual Outomes {.smaller}

```{r}
#| echo: false
m2_df %>%
  group_by(prediction, pro_russian_vote) %>%
  summarise(count = n()) %>%
  cbind(" " = c("True Negatives", "False Negatives", "False Positives", "True Positives")) %>% kable()
```


# Uncertainty and Inference

## Model Plots and Confidence Intervals {.smaller}

::: columns
::: {.column width="50%"}
```{r}
tidy(m2, conf.int = T, conf.level = 0.99) %>%
  dplyr::select(term, estimate, starts_with("conf")) %>%
  kable()
```
:::

::: {.column width="50%"}
```{r}
modelplot(m2, conf_level = 0.99) +
  geom_vline(xintercept = 0, lty = 2)
```
:::
:::

-   \[0.561, 3.20\] is an interval for the difference in the log-odds between
    voters in precincts with and without Russian TV coverage, holding proximity
    to the border constant.

-   Living in a precinct with Russian TV coverage is, on average, positively
    related to voting pro-Russian, controlling for the proximity to the border.
    This effect is significantly different from zero at 1% significance level.

## Better Interpretation {.smaller}


-   With 95% confidence, predicted probability to vote for a pro-Russian party
    when living within 25km to the border and being exposed to Russian
    propaganda ranges from 0.21 to 0.34.
-   We are 95% confident that for voter living within 25km from the Russian
    border, exposure to Russian TV propaganda is associated with an increase in
    the probability to vote for a pro-Russian party of 12 to 27 percentage
    points.
-   For those living in a precinct further than 25km from the border, the
    average effect of exposure to Russian TV propaganda on the probability of
    pro-Russian voting ranges from 16 to 59 percentage points.


## Sampling Distributions  {.smaller}

- The sampling distribution of a statistic is a probability distribution based on a large number of samples of size  from a given population

-   Sampling distributions represent the variability of our estimates: if we had
    taken different samples from the population, we would have obtained slightly
    different estimates

-   Sampling distributions of most of the parameters are normal:

    -   Determined by two parameters, mean (center) and standard deviation
        (spread)

## Draws from Simulated Sampling Distributions {.smaller}

We can use our coefficient estimates and uncertainty about them to simulate
sampling distributions:

```{r}
#| echo: false
#| out-height: 80%
library(clarify)
set.seed(2023)
# get draws from multivariate normal distribution 
sims <- clarify::sim(m2)
as_tibble(sims$sim.coefs) %>%
  pivot_longer(values_to = "estimate",
               names_to = "term",
               cols = everything()) %>%
  ggplot(aes(x = estimate)) +
  geom_histogram(color = "white")   +
  facet_wrap(~term, scales = "free") +
  labs(title = "Simulated Coefficients") +
  geom_vline(data = tidy(m2), aes(xintercept = estimate)) +
  theme(plot.title.position = "plot")
```

## Simulated Sampling Distributions {.smaller}

```{r}
# get draws from multivariate normal distribution 
sims <- clarify::sim(m2, n = 1000)
```

```{r, echo=FALSE}
sims$sim.coefs %>% 
  head() %>%
  kable() 
```



## Simulated Sampling Distributions {.smaller}

::: columns
::: {.column width="50%"}
```{r}
as_tibble(sims$sim.coefs) %>% 
  summarise_all(.funs = list(mean = ~ mean(.))) %>% 
  kable()
as_tibble(sims$sim.coefs) %>% 
  summarise_all(.funs = list(sd = ~ sd(.))) %>% 
  kable()
```
:::

::: {.column width="50%"}
```{r}
tidy(m2) %>% 
  kable()
```
:::
:::

## Log Odds with Simulated Coefficients {.smaller}

-   Now instead of one equation with *estimated* coefficients, we have many with
    similar, *simulated* coefficients
-   Each equation will result in slightly different log odds value (and
    predicted probability, too)

. . . 

$$
{ \begin{array}{c}
        \tilde{\beta}_0^1 \times 1+\tilde{\beta}_1^1 \times   \text{Russian TV}_i  +\tilde{\beta}_2^1 \times \text{Living within 25km}_i &= \log\Big(\dfrac{\tilde{\pi}^1}{1-\tilde{\pi}^1}\Big)\\
 \tilde{\beta}_0^2 \times 1+\tilde{\beta}_1^2 \times   \text{Russian TV}_i  +\tilde{\beta}_2^2 \times \text{Living within 25km}_i &= \log\Big(\dfrac{\tilde{\pi}^2}{1-\tilde{\pi}^2}\Big)\\
  \tilde{\beta}_0^3 \times 1+\tilde{\beta}_1^3 \times   \text{Russian TV}_i  +\tilde{\beta}_2^3 \times \text{Living within 25km}_i  &= \log\Big(\dfrac{\tilde{\pi}^3}{1-\tilde{\pi}^3}\Big)\\
        \dots \\
        \tilde{\beta}_0^{1000} \times 1+\tilde{\beta}_1^{1000} \times \text{Russian TV}_i  +\tilde{\beta}_2^{1000} \times \text{Living within 25km}_i  &= \log\Big(\dfrac{\tilde{\pi}^{1000}}{1-\tilde{\pi}^{1000}}\Big)\\
    \end{array} }
$$


## Calculating Predicted Probabilities for Chosen Scenarios {.smaller}

$$
{\text{Russian TV} = 0,~\text{Living within 25 km} = 1}
$$ 

::: columns 
:::{.column width="50%"}

```{r}
# manually calculate the log odds (not predicted probabilities yet)
lo1 <- sims$sim.coefs[1,1] +  sims$sim.coefs[1,2] * 0 +
  sims$sim.coefs[1,3] * 1 
lo2 <- sims$sim.coefs[2,1] +  sims$sim.coefs[2,2] * 0 + 
  sims$sim.coefs[2,3] * 1
# and so on for every row in the matrix 

# custom inverse logit function 
inv_logit <- function(x) {
  exp(x) / (1 + exp(x))
}

# transform log odds to probabilities 
inv_logit(lo1) 
inv_logit(lo2)
```

::: 
::: {.column width="50%"}

```{r}
# with clarify 
evs <- sim_setx(sim = sims, # object with simulated coefs  
         x = list(russian_tv = 0, # scenario 
                  within_25km = 1))

# compare to manual calculations 
as.matrix(evs) %>% head(6) %>% kable()
```

::: 
:::

## Summarize Predicted Probabilities  {.smaller}


::: columns 
:::{.column width="50%"}
```{r, echo=FALSE}
#| fig.asp = 1 
# obtain predictions based on specified values
plot(evs)
```

::: 
:::{.column width="50%"}
```{r, echo=FALSE}
summary(evs, level = .95) %>% kable()
```

> We are 95% confident that predicted probability to vote pro-Russian ranges
> from `r round(summary(evs, level = .95)[2], 2)` to
> `r round(summary(evs, level = .95)[3], 2)` percentage points in case
> $${\text{Russian TV} = 0,~\text{Living within 25 km} = 1}$$

:::  
:::


## Expected Values for Two Scenarios {.smaller}

$${\text{Russian TV} = 0,~\text{Living within 25 km} = 1}$$
$${\text{Russian TV} = 1,~\text{Living within 25 km} = 1}$$

```{r, echo=FALSE}
# with clarify 
evs <- sim_setx(sim = sims, # object with simulated coefs  
         x = list(russian_tv = 0:1, # scenario with desired (plausible) values 
                  within_25km = 1))

# compare to manual calculations 
as.matrix(evs) %>% head() %>% kable()
```

--------------------------------------------------------------------------------

```{r, echo=FALSE}
plot(evs) 
```


```{r, echo=FALSE}
summary(evs, level = .95) %>% kable()
```

## What Is the Effect of Russian TV Propaganda? {.smaller}

```{r}
fds <- transform(evs, 
          `First Difference` = `russian_tv = 1` - `russian_tv = 0`) 
fds %>% 
  summary() %>%
  kable()
```

> On average, for those living in precincts within 25km to the border, being
> exposed to Russian TV propaganda is associates with an increase in the
> probability to vote for a pro-Russian party of
> `r round(summary(fds, level = .95)[3, 1], 0)` percentage points. \ 95%
> confidence interval for this quantity ranges from
> `r round(summary(fds, level = .95)[3, 2], 0)` to
> `r round(summary(fds, level = .95)[3, 3], 0)` percentage points, making the
> effect of Russian TV propaganda significantly different from zero.

--------------------------------------------------------------------------------

```{r, echo=FALSE}
fds %>% 
  plot()
```

## Multiple Scenarios and First Differences {.smaller}

```{r}
evs <- sim_setx(sim = sims, # object with simulated coefs  
         x = list(russian_tv = 0:1, # scenario with desired (plausible) values 
                  within_25km = 0:1))

as.matrix(evs) %>% head(3) %>% kable()
```

## Multiple Scenarios {.smaller}

```{r}
fds <- transform(
  evs,
  `FD_within_25km = 0` = `russian_tv = 1, within_25km = 0` - `russian_tv = 0, within_25km = 0`,
  `FD_within_25km = 1` = `russian_tv = 1, within_25km = 1` - `russian_tv = 0, within_25km = 1`
) 

summary(fds) %>% kable()
```

--------------------------------------------------------------------------------

```{r, echo=FALSE}
plot(fds)
```

--------------------------------------------------------------------------------

```{r}
#| echo: false
plt <- summary(fds) %>%
  as.data.frame() %>%
  rownames_to_column(var = "quantity") %>%
  clean_names() %>%
  mutate(
    russian_tv = if_else(str_detect(quantity, "russian_tv = 0"), "No", "Yes"),
    within_25km = if_else(str_detect(quantity, "within_25km = 0"), "No", "Yes"),
    quantity = if_else(str_detect(quantity, "FD"), "FD", "EV")
  ) 

plt %>%
  filter(quantity == "EV") %>%
  ggplot(aes(
    y = estimate,
    ymin = x2_5_percent,
    ymax = x97_5_percent,
    x = russian_tv,
    shape = within_25km,
    color = within_25km
  )) +
geom_pointrange(position = position_dodge(width = .5),
                  size = 1.2,
                  linewidth = 1.2) +
  labs(
    title = "Russian TV Reception in Precinct and Probability of Pro-Russian Voting",
    y = "Probability of Pro-Russian Vote",
    shape = "Living within 25 km",
    color = "Living within 25 km",
    x = "Russian TV Reception\nin Precinct"
  ) +
  theme(legend.position = "bottom") +
  scale_color_viridis_d(end = 0.8) +
  ylim(0, 1) +
  coord_flip() +
  theme(plot.title.position = "plot")
```

--------------------------------------------------------------------------------

```{r}
#| echo: false
plt %>%
  filter(quantity == "FD") %>%
  ggplot(aes(
    y = estimate,
    ymin = x2_5_percent,
    ymax = x97_5_percent,
    x = within_25km,
    shape = within_25km,
    color = within_25km
  )) +
geom_pointrange(position = position_dodge(width = .5),
                  size = 1.2,
                  linewidth = 1.2)  +
  theme(legend.position = "none") +
  scale_color_viridis_d(end = 0.8) +
  ylim(0, 0.6) + 
  coord_flip() +
  labs(x = "Living within 25 km",
       y = "Difference in Predicted Probabilities of Pro-Russian Voting",
       title = "The Effect of Russian TV Reception in Precinct on Pro-Russian Voting",
       # subtitle = "Pr(Pro-Russian Voting = 1 | Russian TV Reception = 1) \n- Pr(Pro-Russian Voting = 1 | Russian TV Reception = 0)"
       ) +
  theme(plot.title.position = "plot") +
  geom_hline(yintercept = 0, lty = 2)

# p1 + p2 + patchwork::plot_layout(guides = "collect") &
#   theme(legend.position = "bottom") 
```

<!-- ```{r} -->

<!-- #| echo: false -->

<!-- # custom function to generate starts  -->

<!-- stats_fun <- function(x){ -->

<!--   c(quantile(x, probs = 0.025), mean(x), quantile(x, probs = 0.975)) -->

<!-- } -->

<!-- as.data.frame(evs) %>% -->

<!--   clean_names() %>% -->

<!--   ggplot(aes(x = x1)) + -->

<!--   geom_density(fill = "grey") + -->

<!--   labs(x = "Predicted Probabilites of Voting Pro-Russian", -->

<!--        y = "") + -->

<!--   geom_vline(xintercept = as.numeric(evs) %>% stats_fun()) -->

<!-- ``` -->

<!-- ##  -->

<!-- ```{r} -->

<!-- # results <- sim_setx(sims,  -->

<!-- #                     x = list(female = 0:1, -->

<!-- #                              beauty = 1:10)) -->

<!-- as.data.frame(evs) %>%  -->

<!--   summarise_all(.funs = list(upr = ~ quantile(x = ., probs = 0.025), -->

<!--                              mean = ~ mean(x = .), -->

<!--                              lwr = ~ quantile(x = ., probs = 0.975))) %>%  -->

<!--   ggplot(aes(x = mean, xmin = lwr, xmax = upr, y = 1)) + -->

<!--   geom_pointrange() -->

<!-- ``` -->

<!-- ## tt  -->

<!-- ```{r, eval=FALSE} -->

<!-- m1 <- lm(primary_06 ~ treatment, data = shaming)  -->

<!-- m2 <- glm(primary_06 ~ treatment, data = shaming, family = binomial) -->

<!-- modelsummary::modelsummary(list(m1, m2)) -->

<!-- ``` -->

<!-- ## Models for Binary DV -->

<!-- This model (called a logistic regression model) only produces predictions between 0 and 1. -->

<!-- ```{r, eval=FALSE} -->

<!-- lm(pro_russian_vote ~ russian_tv, data=UA) %>%  -->

<!--   predict(interval = "c") -->

<!-- lm(pro_russian_vote ~ russian_tv + within_25km, data=UA) %>%  -->

<!--   predict(interval = "c", newdata = expand_grid(russian_tv = 0:1, -->

<!--                         within_25km = 0:1)) %>% -->

<!--   bind_cols(expand_grid(russian_tv = 0:1, -->

<!--                         within_25km = 0:1)) %>% -->

<!--   ggplot(aes(y = fit, ymin = lwr, -->

<!--              ymax = upr, x = russian_tv)) + -->

<!--   geom_pointrange() -->

<!-- glm(pro_russian_vote ~ russian_tv + within_25km, data=UA, family = binomial) %>%  -->

<!-- predict(type = "response") -->

<!-- ``` -->

<!-- ## How To Model Binary Variables: Review -->

<!-- Based on the three GLM criteria we have: -->

<!--   - $y_i \sim \text{Bernoulli}(\pi_i)$: our underlying distribution  -->

<!--   - $X\beta = \beta_0+\beta_1 x_{1,i} + \cdots + \beta_n x_{n,i}$: our systematic component  -->

<!--   - response function the response function to transform linear predictor: -->

<!--       - $\pi_i = \frac{\exp(X\beta)}{1+\exp(X\beta)}$: for logit model (`plogis`) -->

<!--       - $\pi_i = \Phi(X\beta)$: for probit model (`pnorm`) -->

# Appendix: Interpretation of Coefficients in Logit Models

## Compare the odds for two groups {.smaller}

| Russian TV | Pro-Russian Vote | No pro-Russian Vote |
|:-----------|-----------------:|--------------------:|
| No         |               27 |                 131 |
| Yes        |               58 |                 142 |

-   We want to compare the risk of Pro-Russian Vote for those with exposure to
    Russian TV and those without it.
-   We'll use the odds to compare the two groups

$$\text{odds} = \frac{P(\text{success})}{P(\text{failure})} = \frac{\text{# of successes}}{\text{# of failures}}$$

## Compare the odds for two groups {.smaller}

| Russian TV | Pro-Russian Vote | No pro-Russian Vote |
|:-----------|-----------------:|--------------------:|
| No         |               27 |                 131 |
| Yes        |               58 |                 142 |

-   Odds of voting pro-Russian with Russian TV exposure:
    $\frac{58}{142} = 0.408$

-   Odds of voting pro-Russian without Russian TV exposure:
    $\frac{27}{131} = 0.206$

-   Based on this, we see those with a Russian TV exposure had higher odds of
    voting pro-Russian than those without Russian propaganda exposure.

-   We can summarize the relationship with odds ratio (OR):
    $OR = \frac{\text{odds}_1}{\text{odds}_2} = \frac{\omega_1}{\omega_2}$

## Odds Ratio: with vs. without exposure to Russian TV {.smaller}

| Russian TV | Pro-Russian Vote | No pro-Russian Vote |
|:-----------|-----------------:|--------------------:|
| No         |               27 |                 131 |
| Yes        |               58 |                 142 |

-   Odds of voting pro-Russian with Russian TV exposure:
    $\frac{58}{142} = 0.408$
-   Odds of voting pro-Russian without Russian TV exposure:
    $\frac{27}{131} = 0.206$

$OR = \frac{\text{odds}_{with}}{\text{odds}_{without}} = \frac{0.408}{0.206} = 1.982$

The odds of voting pro-Russian are 1.982 times higher for those with exposure to
Russian TV than those without exposure to Russian TV.

## Coefficients in Logit Model {.smaller}

```{r}
m3 <- glm(pro_russian_vote ~ russian_tv,
          family = binomial,
          data = UA)
m3$coefficients[2] # log odds 
```

> The log odds of voting pro-Russian are 0.684 higher for those with exposure to
> Russian TV compared to those without exposure to Russian TV.

```{r}
exp(m3$coefficients[2]) # odds 
```

> The odds of voting pro-Russian are 1.982 **times** higher for those with
> exposure to Russian TV than those without exposure to Russian TV.

## Continous Predictors {.smaller}

> For each additional unit change in $X_k$, the log-odds of Y are expected to
> increase by $\beta_k$ (holding all else contant).

> For each additional unit change in $X_k$, the odds of Y are expected to
> mulitply by a factor of $e^{\beta_k}$ (holding all else contant).

OR

> For each additional unit change in $X_k$, the odds of Y are expected to
> increase by $e^{\beta_k}%$ (holding all else contant).

## Confidence Intervals {.smaller}

::: columns
::: {.column width="50%"}
#### Log Odds

We can calculate the **C% confidence interval** for $\beta_k$ as the following:

$$
\Large{\hat{\beta}_k \pm z^* SE_{\hat{\beta}_k}}
$$

where $z^*$ is calculated from the $N(0,1)$ distribution

This is an interval for the change in the log-odds for every one unit increase
in $x_k$.
:::

::: {.column width="50%"}
#### Odds

The change in **odds** for every one unit increase in $x_k$.

$$
\Large{e^{\hat{\beta}_k \pm z^* SE_{\hat{\beta}_k}}}
$$

. . .

**Interpretation:** We are $C\%$ confident that for every one unit increase in
$x_k$, the odds multiply by a factor of
$e^{\hat{\beta}_k - z^* SE_{\hat{\beta}_k}}$ to
$e^{\hat{\beta}_k + z^* SE_{\hat{\beta}_k}}$, holding all else constant.
:::
:::

# Appendix: Fitting Logit Models

## Maximum Likelihood Estimation (MLE) {.smaller}

-   20 data points that we assume come from a normal distribution. We know that
    normal distribution has two parameters, mean and variance
-   Which of the plotted distributions has most likely generated the data
    points?

```{r, echo=FALSE}
set.seed(1201)
ggplot(data = data.frame(y = c(-3, 3)), 
       aes(y)) +
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 0, sd = 1),
                col = viridis::viridis(4)[1],
                lwd = 1) + 
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 0, sd = 2),
                col = viridis::viridis(4)[2],
                lwd = 1) + 
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 2, sd = 1),
                col = viridis::viridis(4)[3],
                lwd = 1) + 
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 1, sd = 2),
                col = viridis::viridis(4)[4],
                lwd = 1) + 
  geom_point(data = data.frame(x = rnorm(30, 0, 1), y = 0),
             mapping = aes(x, y)) +
  ylab("") +
  scale_y_continuous(breaks = NULL) +
  xlim(c(-6,6))
```



## Maximum Likelihood Estimation (MLE) {.smaller}


```{r, echo=FALSE}
set.seed(1201)
ggplot(data = data.frame(y = c(-3, 3)), 
       aes(y)) +
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 0, sd = 1),
                col = viridis::viridis(4)[1],
                lwd = 1) + 
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 0, sd = 2),
                col = viridis::viridis(4)[2],
                lwd = 1) + 
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 2, sd = 1),
                col = viridis::viridis(4)[3],
                lwd = 1) + 
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 1, sd = 2),
                col = viridis::viridis(4)[4],
                lwd = 1) + 
  geom_point(data = data.frame(x = rnorm(30, 0, 1), y = 0),
             mapping = aes(x, y)) +
  ylab("") +
  scale_y_continuous(breaks = NULL) +
  xlim(c(-6,6))
```

-   The points seem to be centered around zero and they range is between
    $[-2,2]$
-   Most likely, it is the violet distribution that has generated the points
-   With MLE: (1) we observe the data, (2) assume a distribution it come from,
    and (3) look for the values of parameters defining this distribution that
    result in the curve that best fits the data

--------------------------------------------------------------------------------
```{r, echo=FALSE}
set.seed(1201)
x = seq(-10, 10, length.out = 1000)
dat <- data.frame(x = x,
  # x=runif(1000, 0, 50),
                  y=rnorm(1000, 5*x, 20))

## breaks: where you want to compute densities
breaks <- seq(min(dat$x), max(dat$x), len=5)
dat$section <- cut(dat$x, breaks)

## Get the residuals
dat$res <- residuals(lm(y ~ x, data=dat))
dat$fitted <- fitted(lm(y ~ x, data=dat))


dens <- do.call(rbind, lapply(split(dat, dat$section), function(x) {
  d <- density(dat$res, n=50)
  # res <- data.frame(x=max(x$x)- d$y*2000, y=d$x+mean(x$y))
  # res <- res[order(res$y), ]
  ## Get some data for normal lines as well
  xs <- seq(min(dat$res), max(dat$res), len=50)
  res <- data.frame(y = xs + mean(x$y),
                    x = max(x$x) - 100 * dnorm(xs, 0, sd(dat$res)))
  res$type <- rep("normal", 50)
  res
}))
dens$section <- rep(levels(dat$section), each=50)


dens %<>% 
  filter(type == "normal") %>%
  group_by(section) %>%
  mutate(mn = if_else(x == min((x)), 1, 0))

dat1 = sample_n(dat, 200)
MLE <- ggplot(dat, aes(x, y)) +
  geom_point(alpha = 0.3, data = dat1, mapping = aes(x, y)) +
  # geom_smooth(method="lm", fill=NA, lwd=2) +
  geom_path(data=dens[dens$type=="normal",], 
            aes(x, y, group=section), 
            color="#800010", lwd=0.5) +
  annotate("segment", 
           x = dens$x[dens$mn == 1], 
           xend = breaks[2:5], 
           y = dens$y[dens$mn == 1], 
           yend = predict(lm(y ~ x, dat), 
                          newdata = data.frame(x = breaks[2:5])),
           lty = 2,
           lwd=0.7,
           color="#800010") +
  geom_point(data = dens[dens$mn == 1,],
             aes(x = x, 
             y = y),
             color="#800010") +
  # geom_vline(xintercept = breaks, lty = 2) +
  labs(
    x = "X",
    y = "Y",
    title = "MLE Estimator",
    subtitle = "Find Parameter Values for which Observed \nData Are Most Likely"
  ) +
  geom_smooth(
    method = "lm",
    color = "#003056",
    se = FALSE,
    size = 0.5
  ) +
  ylim(-100, 100)

OLS <- ggplot(dat,
              mapping = aes(x, y)) +
  geom_point(alpha = 0.3,
             data = dat1,
             mapping = aes(x, y)) +
  geom_smooth(method = "lm", color = "#003056", se = FALSE,
              dat,
              mapping = aes(x, y)) +
  geom_segment(aes(xend = x,
                   yend = fitted),
               color = "#800010",
               # size = 1,
               data = dat1) +
  labs(x = "X",
       y = "Y",
       title = "OLS Estimator",
       subtitle = ("Find Parameter Values to Minimize Sum of \nSquared Residuals")) +
  ylim(-100, 100) +
  theme(plot.subtitle = element_text(margin=margin(0,0,0,0)))


OLS + MLE
```
<!-- # Intro -->

<!-- ```{r} -->

<!-- #| echo: false -->

<!-- countdown(minutes = 4, color_background = "white", left = 1) -->

<!-- ``` -->

<!-- ::: columns -->

<!-- ::: {.column width="30%"} -->

<!-- ```{dot} -->

<!-- //| echo: false -->

<!-- //| fig-width: 3 -->

<!-- //| fig-height: 3 -->

<!-- //| out-width: 100% -->

<!-- digraph D { -->

<!--   node [shape=oval, fontname="AtkinsonHyperlegible-Regular"]; -->

<!--   edge [minlen = 1.2, arrowhead = vee, arrowsize=0.5]; -->

<!--   a [label = "Beauty Score"]; -->

<!--   b [label = "Evaluations"]; -->

<!--   c [label = "Female"]; -->

<!--   {rank=same a b}; -->

<!--   {rank=sink c}; -->

<!--   a->b; -->

<!--   c->a; -->

<!--   c->b; -->

<!-- } -->

<!-- ``` -->

<!-- :::  -->

<!-- ::: {.column width="70%"} -->

<!-- 1.  After specifying the DGP in a DAG and following our identification strategy, we can interpret the estimated coefficient for the effect of *Beauty Score* on *Evaluations* causally. -->

<!-- 2.  After specifying the DGP in a DAG and following our identification strategy, we can interpret the estimated coefficient for the effect of *Female* on *Evaluations* causally. -->

<!-- 3. On average, a 5-point increase in *Beauty Score* would be associated with 0.4-point increase in *Teaching Evaluations*, for both female and female instructors.  -->

<!-- 4. The effect of *Female* on *Teaching Evaluations* would be represented through the change in the slope of the regression coefficient for *Beauty Score*.  -->

<!-- 5. For $evals = \beta_0 + \beta_1 \cdot Female$,  $\beta_1$ would be the equal to difference-in-means estimator.  -->

<!-- :::  -->

<!-- :::  -->
