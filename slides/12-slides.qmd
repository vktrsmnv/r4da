---
title: "Binary Dependent Variable"
subtitle: "R for Data Analysis<br>Session 12"
title-slide-attributes:
  data-background-size: stretch
  data-slide-number: none
auto-stretch: false
institute: "University of Mannheim<br>Fall 2023"
author: "Viktoriia Semenova"
footer: "[ðŸ”— r4da.live](https://r4da.live/)"
logo: images/logo.png
format:
  revealjs:
    theme: slides.scss
    transition: fade
    incremental: true   
    slide-number: true
    chalkboard: true
execute:
  echo: true
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
p_needed <- c("tidyverse", "janitor", "icons", "infer",
              "countdown", "showtext", "ggdag", "magrittr",
              "gt", "haven", "broom", "patchwork", "knitr", "modelsummary")

# check if they are already installed, install if not installed
lapply(p_needed[!(p_needed %in% rownames(installed.packages()))], install.packages, repos = "http://cran.us.r-project.org")

# load the packages
lapply(p_needed, library, character.only = TRUE)

# set width of code output
options(width = 90)

# set figure parameters for knitr
knitr::opts_chunk$set(
  fig.width = 7, # 7" width
  fig.asp = 0.618, # the golden ratio
  fig.retina = 3, # dpi multiplier for displaying HTML output on retina
  fig.align = "center", # center align figures
  dpi = 300 # higher dpi, sharper image
)


font_add_google(name = "Gochi Hand")
showtext::showtext_auto()
# set theme for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 14)) + 
  theme(plot.title.position = "plot") 
theme_update(plot.title = element_text(face = "bold"),
             plot.title.position = "plot")

# governors <- readr::read_csv("https://r4da.live/files/data/external_data/governors.csv")
# trains <- readr::read_tsv("../data/trains.tsv") %>% clean_names()
evals <- read_csv("data/beauty.csv") %>% clean_names()
load("data/student_population.Rda")
cabinet <- read_dta("data/cabinet_size_replication_data.dta")
cdat <- read_csv("data/cdat.csv")
UA <- read_csv("data/UA_survey.csv") %>% clean_names()

set.seed(2508)
m2 <- lm(eval ~ beauty + female, data = evals)
```


# Visualising Marginal Effects

## Marginal Effects Plot

$$
Y = \beta_0 + \beta_1X + \beta_2D + \beta_3X \cdot D
$$
![](https://tompepinsky.files.wordpress.com/2017/04/mfx1.jpeg){fig-align="center"}


## Quiz: Which of these statements are correct? {.smaller}

```{r}
#| echo: false
countdown(minutes = 4, color_background = "white",top = "0")
```

Indridason and Bowler (2014) explore the determinants of cabinet size in
parliamentary systems. Below you can find a plot based on one of their model.
<!-- $$\widehat {Cabinet~Size} = \hat{\beta}_{0} + \hat{\beta}_{1}(\operatorname{Legislature Size}) + \hat{\beta}_{2}(\operatorname{Legislature Size}) + \hat{\beta}_{3}(\operatorname{Population Size}) + \hat{\beta}_{4}(\operatorname{EU membership}) + \epsilon$$ -->

::: columns
::: {.column width="50%"}
1.  Systematic component of the model likely includes variable *Legislature
    Size* interacted with another variable.\
2.  Marginal effect of the variable *Legislature Size* is constant across all
    values of *Legislature Size* variable.
3.  The relationship between *legislature size* and *cabinet size* is strongest
    for smaller values of *legislature size*.
4.  For legislatures with sizes above 500, there is, on average, no significant
    effect of *legislature size* on *cabinet size*.
5.  *Legislature size* seems to be inversely related to *cabinet size*.
:::

::: {.column width="50%"}
::: r-stack
```{r}
#| echo: false
#| fig-asp: 1
m5 <- lm(noministers ~ sizeleg + I(sizeleg^2) + popint + eu + as.factor(country),
  data = cabinet
)

# cdat <- margins::cplot(m5, "sizeleg", what = "effect", draw = F)
ggplot(cdat, aes(x = xvals * 100)) + 
  geom_line(aes(y = yvals),  color = "#800010") +
  geom_ribbon(aes(ymax = upper, ymin = lower), fill = "#80001050") +
  geom_line(aes(y = upper), color = "#800010") +
  geom_line(aes(y = lower), color = "#800010") +
  geom_hline(yintercept = 0, linetype = 2) +
  # xlim(50, 650) +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  # ggtitle("AME of Axle Ratio on Fuel Economy (mpg) by Weight") +
  xlab("Legislature Size") + ylab("Marginal Effect of Legislature Size") +
  geom_rug(
    data = cabinet,
    aes(x = sizeleg * 100)
  )
# summary(mult.fit)
# # create a grid for all possible combinations 
# # tibble() only allows for different values in one variable 
# plt <- expand_grid(
#   any_girls = 0:1,
#   female = 0:1,
#   age = mean(evals$age, na.rm = T),
#   minority = 0,
#   nonenglish = 0
# ) 
# 
# predict(m0,
#         interval = "confidence",
#         newdata = plt,
#         level = 0.95) %>%
#   bind_cols(plt) %>%
#     mutate(female = if_else(female == 1, "Female", "Male")) %>%
#   ggplot(aes(beauty, y = fit, 
#              color = as_factor(female),
#              fill  = as_factor(female))) +
#   labs(
#     x = "Beauty",
#     y = "Average Teaching Evaluation",
#     title = "Relationship between Instructor Beauty\nand Course Evaluations",
#     # subtitle = "Intercept Shift with Same Slope",
#     color = "Instructor is",
#     fill = "Instructor is"
#   ) +
#   geom_line() +
#   geom_ribbon(aes(ymin = lwr,
#                   ymax = upr), 
#                 ) +
#   # geom_pointrange(aes(y = fit, ymin = lwr, ymax = upr)) + # plots the parallel lines
#   scale_color_viridis_d(end = 0.8, alpha = 0.8) +
#   scale_fill_viridis_d(end = 0.8, alpha = 0.5) +
#   theme(legend.position = "top") +
#   scale_x_continuous(limits = c(1, 10), breaks = 1:10) +
#   ylim(c(1, 5)) +
#   theme(panel.grid.minor = element_blank(),
#         plot.title.position = "plot",
#         text = element_text(size = 20))
#   #  geom_rug(
#   #    data = augment(m0) %>%
#   #   mutate(female = if_else(female == 1, "Female", "Male")),
#   #    aes(x = beauty, y = eval),
#   #   alpha = 0.3,
#   #   sides = "b",
#   #   position = "jitter",
#   #   # size = 4,
#   #   length = unit(0.05, "npc")
#   # )
```

::: fragment
```{r}
#| echo: false
#| fig-asp: 1
augment(m5, interval = "c", 
        newdata = tibble(sizeleg = seq(0.51, 7, length.out = 50), 
                                             popint = 81.41416,
                                             eu = 1,
                                             country = "GER")) %>%
  ggplot(aes(x = sizeleg * 100, y = .fitted)) +
  geom_smooth(color = "black") +
  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 0.5) +
  labs(
    x = "Legislature Size",
    y = "Cabinet Size",
    title = "Legislative Size Affects Cabinet Size",
    subtitle = "Expected Values and 99% CIs"
  ) 
```
:::
:::
:::
:::

# Binary Dependent Variable

## Does TV Propaganda Affect Voting? {.smaller}

```{dot}
//| echo: false
//| out-width: 100%
digraph D {
  node [shape=ellipse];
  edge [len = 1.2, arrowhead = vee];
  a [label = "Russian TV\nReception"];
  b [label = "Pro-Russian Vote"];
  c [label = "Living within\n25 km from Border"];
  
  {rank=same a b};
  {rank=sink c};
  a->b;
  c->a;
  c->b;
}
```

## Data {.smaller}

```{r,echo=FALSE}
datasummary_skim(UA)
```

- `russian_tv`: indicator for whether voter's precinct received Russian TV (1) or not (0)
- `pro_russian_vote`: indicator for whether respondent voted for pro-Russian party in 2014 Ukrainian elections (1) or not (0)
- `within_25km`: indicator for whether respondent's precinct is within 25 kilometers from Russian border (1) or not (0)

## Linear Probability Model {.smaller}

$$\text{Pro-Russian Vote} \sim \beta_0 + \beta_1 \text{Russian TV} + \beta_2 \text{Living within 25km} + \varepsilon$$

```{r}
#| echo: false
#| out-height: 80%
m1 <- lm(pro_russian_vote ~ russian_tv + within_25km, data=UA)
newdata <- expand_grid(russian_tv = 0:1,
                        within_25km = 0:1)
m1 %>%
  predict(interval = "c", newdata) %>%
  bind_cols(newdata) %>%
  ggplot(aes(
    y = fit,
    ymin = lwr,
    shape = if_else(within_25km == 0, "No", "Yes"),
    color = if_else(within_25km == 0, "No", "Yes"),
    ymax = upr,
    x = if_else(russian_tv == 0, "No", "Yes")
  )) +
  geom_pointrange(position = position_dodge(width = .5),
                  size = 1.2,
                  linewidth = 1.5) +
  labs(
    y = "Pro-Russian Vote",
    shape = "Living within 25 km",
    color = "Living within 25 km",
    x = "Russian TV Reception"
  ) +
  theme(legend.position = "bottom") +
  scale_color_viridis_d(end = 0.8) +
  ylim(-0.2, 1)
```

## Out-of-Bound Predictions

```{r}
#| echo: false
m1 %>%
  predict(interval = "c", newdata) %>%
  bind_cols(newdata) %>%
  ggplot(aes(
    y = fit,
    ymin = lwr,
    shape = if_else(within_25km == 0, "No", "Yes"),
    color = if_else(within_25km == 0, "No", "Yes"),
    ymax = upr,
    x = if_else(russian_tv == 0, "No", "Yes")
    # x = russian_tv
  )) +
    geom_hline(yintercept = c(0,1), lty = 2, alpha = 0.5) +
  geom_pointrange(position = position_dodge(width = 0.3),
                  size = 1.2,
                  linewidth = 1.5) +
  labs(
    y = "Pro-Russian Vote",
    shape = "Living within 25 km",
    color = "Living within 25 km",
    x = "Russian TV Reception"
  ) +
  theme(legend.position = "bottom") +
  scale_color_viridis_d(end = 0.8) +
  ylim(-0.2, 1) +
  geom_text(aes(label = round(fit, 3),
                y = fit + 0.005), 
            size = 4.5,
            position = position_dodge(width = -0.4))
```

## Logistic Regression Solution {.smaller}

-   Apply a transformation to the *linear predictor*
    $\beta_0 + \beta_1 \text{Russian TV} + \beta_2 \text{Living within 25km}$,
    to ensure that outcome is bounded between 0 and 1

-   The inverse logit (aka sigmoid) function takes a value between $-\infty$ and
    $+\infty$ and maps it to a value between 0 and 1:

$$
logit^{-1}(x) = \frac{\exp(x)}{1+\exp(x)}
$$

## Logistic Function {.smaller}

```{r}
#| echo: false
#| out-height: 50%

inv_logit <- function(x) {
  exp(x) / (1 + exp(x))
}

ggplot(
  data = tibble(
    x = seq(-10, 10, length.out = 50),
    y = inv_logit(seq(-10, 10, length.out = 50))
  ),
  aes(x, y)
) +
  geom_line(alpha = 0.5)
```

---

```{r, echo=FALSE}
ggplot(mtcars, aes(x = mpg - 20, y = vs)) + geom_point() +
  stat_smooth(
    method = "glm",
    method.args = list(family = "binomial"),
    se = FALSE,
    color = "#800010",
    alpha = 0.6
  ) +
  stat_smooth(
    method = "lm",
    se = FALSE,
    fullrange = TRUE,
    color = "#003056"
  ) +
  xlim(-10, 15) +
  scale_y_continuous(breaks = c(0,  1)) +
  labs(y = "Binary Y",
       x = "Continuous X",
       # title = "Model Predictions"
       ) +
  annotate(
    geom = "text",
    x = 2,
    y = 0.8,
    label = "Logit",
    color = "#800010",
    # size = 10
  ) +
  annotate(
    geom = "text",
    x = 7,
    y = 0.7,
    label = "Linear",
    color = "#003056",
    # size = 10
  )
```

## Logistic Regression Model {.smaller}

-   $Y = 1:$ yes, $Y = 0:$ no; two mutually exclusive outcomes
-   $\pi = Pr(Y = 1)$: **probability** that $Y=1$
-   $\frac{\pi}{1-\pi}$: **odds** that $Y = 1$
-   $\log\Big(\frac{\pi}{1-\pi}\Big)$: **log odds**
-   Go from $\pi$ to $\log\Big(\frac{\pi}{1-\pi}\Big)$ using the **logit
    transformation**

. . .

$$\underbrace{\log\Big(\frac{\pi}{1-\pi}\Big)}_{\text{straight line}} = \underbrace{\beta_0 + \beta_1~X_1 + \cdots + \beta_k~X_k}_{\text{linear predictor}}$$

## Odds and Probabilities Example {.smaller}

Suppose there is a **70% chance** it will rain tomorrow

-   Probability it will rain is $\pi = \mathbf{p = 0.7}$
-   Probability it won't rain is $1 - \pi = \mathbf{1 - p = 0.3}$
-   Odds $\omega$ it will rain are **7 to 3**, **7:3**,
    $\omega = \frac{\pi}{1-\pi}= \mathbf{\frac{0.7}{0.3} \approx 2.33}$

. . . 

```{r}
pi <- seq(0, 1, length.out = 6)
tibble(pi, 1 - pi, "odds" = pi / (1 - pi), "log odds" = log(pi / (1 - pi))) %>% 
  kable()
```

## Probabilities, Odds, and Log Odds {.smaller}

**Odds**

$$\omega = \frac{\pi}{1-\pi}  = \exp\Big\{\log\Big(\frac{\pi}{1-\pi}\Big)\Big\} 
= \exp\{\beta_0 + \beta_1~X_1 + \cdots + \beta_k~X_k\}$$

**Log odds**

$$\log(\omega) = \log\Big(\frac{\pi}{1-\pi}\Big) = \beta_0 + \beta_1~X_1 + \cdots + \beta_k~X_k$$

**Probability**

$$\pi = \frac{\omega}{1 + \omega}  = \frac{\exp\{\beta_0 + \beta_1~X_1 + \cdots + \beta_k~X_k\}}{1 + \exp\{\beta_0 + \beta_1~X_1 + \cdots + \beta_k~X_k\}}$$

<!-- (1) **Logistic model**: log odds = $\log\Big(\frac{\pi}{1-\pi}\Big) = \beta_0 + \beta_1~X$ -->

<!-- (2) **Odds =** $\exp\Big\{\log\Big(\frac{\pi}{1-\pi}\Big)\Big\} = \frac{\pi}{1-\pi}$ -->

<!-- (3) Combining (1) and (2) with what we saw earlier -->

<!-- $$\text{Probability} = \pi = \frac{\exp\{\beta_0 + \beta_1~X_1 + \cdots + \beta_k~X_k\}}{1 + \exp\{\beta_0 + \beta_1~X_1 + \cdots + \beta_k~X_k\}}$$ -->

<!-- ## Logistic Regression Recap -->

<!-- -   Model for binary DV $\pi = Pr(Y = 1)$ -->
<!-- -   Coefficients represent expected changes in **log-odds**, hence no intuitive -->
<!--     interpretation as with linear models -->

## Fitting the Model in `R` {.smaller}

```{r, echo=FALSE}
# OLS
m1 <- lm(pro_russian_vote ~ russian_tv + within_25km, data = UA)
# logistic regression 
m2 <- glm(pro_russian_vote ~ russian_tv + within_25km, data = UA, family = binomial)
```

```{r}
#| echo: false
modelsummary(list("OLS" = m1, "Logit" = m2), gof_omit = "RMSE|AIC|BIC")
```

## Logit Coefficients Interpretation {.smaller}

$$
\log\left[ \frac { \widehat{Pr( \text{Pro-russian Vote} = \operatorname{1} )} }{ 1 - \widehat{Pr(  \text{Pro-russian Vote} = \operatorname{1} )} } \right] = -1.474 + 1.79 \cdot\text{Russian TV} - 1.32\cdot\text{Living within 25 km}
$$

-   Sign and significance are straightforward to interpret

. . .

> The **log-odds** of voting for a pro-Russian party are expected to be 1.79
> more for those exposed to Russian TV compared to those without exposure to
> Russian TV (the baseline group), holding all else constant.

> The **odds** of voting for a pro-Russian party or those exposed to Russian TV
> are expected to be 5.98 ($e^{1.79}$) **times** the odds for those without
> exposure to Russian TV, holding all else constant.

$$\text{Odds Ratio}  = e^{\hat{\beta}_j} = \exp\{\hat{\beta}_j\}$$

## Predicted Probabilities Plot

```{r}
#| echo: false
m2 %>%
  augment(newdata = newdata, type.predict = "response") %>%
  # bind_cols(newdata) %>%
  ggplot(aes(
    y = .fitted,
    # ymin = lwr,
    shape = if_else(within_25km == 0, "No", "Yes"),
    color = if_else(within_25km == 0, "No", "Yes"),
    # ymax = upr,
    x = if_else(russian_tv == 0, "No", "Yes")
    # x = russian_tv
  )) +
  geom_point(position = position_dodge(width = 0.3),
             size = 3) +
  labs(
    y = "Probability of Pro-Russian Vote",
    shape = "Living within 25 km",
    color = "Living within 25 km",
    x = "Russian TV Reception"
  ) +
  theme(legend.position = "bottom") +
  scale_color_viridis_d(end = 0.8) +
  ylim(0, 1) +
  geom_text(aes(label = round(.fitted, 3),
                y = .fitted + 0.005), size = 4.5,
            position = position_dodge(width = -0.3))
  # geom_hline(yintercept = c(0,1), lty = 2) 
```

## Predicted Probabilities By Hand {.smaller}

$$
\log\left[ \frac { \widehat{Pr( \text{Pro-russian Vote} = \operatorname{1} )} }{ 1 - \widehat{Pr(  \text{Pro-russian Vote} = \operatorname{1} )} } \right] = -1.474 + 1.79 \cdot\text{Russian TV} - 1.32\cdot\text{Living within 25 km}
$$

. . . 


$$
 \hat\pi = \widehat{Pr( \text{Pro-russian Vote} = \operatorname{1} )} =\frac{exp(-1.474
 + 1.79 \cdot \text{Russian TV} - 1.32\cdot\text{Living within 25 km})}{1 + exp(-1.474 + 1.79 \cdot \text{Russian TV}- 1.32\cdot\text{Living within 25 km})}
$$ 

. . . 

$$
\widehat{Pr( \text{Pro-russian Vote} = \operatorname{1} |~\text{Russian TV} = 1,~\text{Living within 25 km} = 1)} \\=\frac{exp(-1.474 + 1.79  - 1.32)}{1 + exp(-1.474 + 1.79  - 1.32)}\\
= \frac{exp(-1.004)}{1 + exp(1 -1.004)} \approx \frac{0.366}{1.996} \approx 0.18
$$

. . . 

$$
\widehat{Pr( \text{Pro-russian Vote} = \operatorname{1} |~\text{Russian TV} = 0,~\text{Living within 25 km} = 1)} \\=\frac{exp(-1.474  - 1.32)}{1 + exp(-1.474   - 1.32)}\\
= \frac{exp(-2.794)}{1 + exp(1 -2.794)} \approx \frac{0.061}{1.166} \approx 0.05
$$


## Predicted Values {.smaller}

- Transforming probabilities $\pi$ back to 0/1 scale of the *response* variable $Y$
- Requires threshold for which values translate to $Y = 1$ and $Y = 0$; $\pi > 0.5$ most common 

```{r}
#| echo: false
# augment(m2, type.predict = "link") # .fitted is log odds
m2_df <- augment(m2, type.predict = "response") %>%
  mutate(prediction = if_else(.fitted > 0.5, 1, 0))
m2_df %>% dplyr::select(pro_russian_vote:.fitted, prediction) %>%
  slice_head(n = 10) %>% kable()
```

## Model Predictions vs. Actual Outomes {.smaller}

```{r}
#| echo: false
m2_df %>%
  group_by(prediction, pro_russian_vote) %>%
  summarise(count = n()) %>%
  cbind(" " = c("True Negatives", "False Negatives", "False Positives", "True Positives")) %>% kable()
```

# Appendix: Interpretation of Coefficients in Logit Models

## Compare the odds for two groups {.smaller}

| Russian TV | Pro-Russian Vote | No pro-Russian Vote |
|:-----------|-----------------:|--------------------:|
| No         |               27 |                 131 |
| Yes        |               58 |                 142 |

-   We want to compare the risk of Pro-Russian Vote for those with exposure to
    Russian TV and those without it.
-   We'll use the odds to compare the two groups

$$\text{odds} = \frac{P(\text{success})}{P(\text{failure})} = \frac{\text{# of successes}}{\text{# of failures}}$$

## Compare the odds for two groups {.smaller}

| Russian TV | Pro-Russian Vote | No pro-Russian Vote |
|:-----------|-----------------:|--------------------:|
| No         |               27 |                 131 |
| Yes        |               58 |                 142 |

-   Odds of voting pro-Russian with Russian TV exposure:
    $\frac{58}{142} = 0.408$

-   Odds of voting pro-Russian without Russian TV exposure:
    $\frac{27}{131} = 0.206$

-   Based on this, we see those with a Russian TV exposure had higher odds of
    voting pro-Russian than those without Russian propaganda exposure.

-   We can summarize the relationship with odds ratio (OR):
    $OR = \frac{\text{odds}_1}{\text{odds}_2} = \frac{\omega_1}{\omega_2}$

## Odds Ratio: with vs. without exposure to Russian TV {.smaller}

| Russian TV | Pro-Russian Vote | No pro-Russian Vote |
|:-----------|-----------------:|--------------------:|
| No         |               27 |                 131 |
| Yes        |               58 |                 142 |

-   Odds of voting pro-Russian with Russian TV exposure:
    $\frac{58}{142} = 0.408$
-   Odds of voting pro-Russian without Russian TV exposure:
    $\frac{27}{131} = 0.206$

$OR = \frac{\text{odds}_{with}}{\text{odds}_{without}} = \frac{0.408}{0.206} = 1.982$

The odds of voting pro-Russian are 1.982 times higher for those with exposure to
Russian TV than those without exposure to Russian TV.

## Coefficients in Logit Model {.smaller}

```{r}
m3 <- glm(pro_russian_vote ~ russian_tv,
          family = binomial,
          data = UA)
m3$coefficients[2] # log odds 
```

> The log odds of voting pro-Russian are 0.684 higher for those with exposure to
> Russian TV compared to those without exposure to Russian TV.

```{r}
exp(m3$coefficients[2]) # odds 
```

> The odds of voting pro-Russian are 1.982 **times** higher for those with
> exposure to Russian TV than those without exposure to Russian TV.

## Continous Predictors {.smaller}

> For each additional unit change in $X_k$, the log-odds of Y are expected to
> increase by $\beta_k$ (holding all else contant).

> For each additional unit change in $X_k$, the odds of Y are expected to
> mulitply by a factor of $e^{\beta_k}$ (holding all else contant).

OR

> For each additional unit change in $X_k$, the odds of Y are expected to
> increase by $e^{\beta_k}%$ (holding all else contant).

## Confidence Intervals {.smaller}

::: columns
::: {.column width="50%"}
#### Log Odds

We can calculate the **C% confidence interval** for $\beta_k$ as the following:

$$
\Large{\hat{\beta}_k \pm z^* SE_{\hat{\beta}_k}}
$$

where $z^*$ is calculated from the $N(0,1)$ distribution

This is an interval for the change in the log-odds for every one unit increase
in $x_k$.
:::

::: {.column width="50%"}
#### Odds

The change in **odds** for every one unit increase in $x_k$.

$$
\Large{e^{\hat{\beta}_k \pm z^* SE_{\hat{\beta}_k}}}
$$

. . .

**Interpretation:** We are $C\%$ confident that for every one unit increase in
$x_k$, the odds multiply by a factor of
$e^{\hat{\beta}_k - z^* SE_{\hat{\beta}_k}}$ to
$e^{\hat{\beta}_k + z^* SE_{\hat{\beta}_k}}$, holding all else constant.
:::
:::

# Appendix: Fitting Logit Models

## Maximum Likelihood Estimation (MLE) {.smaller}

-   20 data points that we assume come from a normal distribution. We know that
    normal distribution has two parameters, mean and variance
-   Which of the plotted distributions has most likely generated the data
    points?

```{r, echo=FALSE, out.width="80%"}
set.seed(1201)
ggplot(data = data.frame(y = c(-3, 3)), 
       aes(y)) +
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 0, sd = 1),
                col = viridis::viridis(4)[1],
                lwd = 1) + 
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 0, sd = 2),
                col = viridis::viridis(4)[2],
                lwd = 1) + 
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 2, sd = 1),
                col = viridis::viridis(4)[3],
                lwd = 1) + 
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 1, sd = 2),
                col = viridis::viridis(4)[4],
                lwd = 1) + 
  geom_point(data = data.frame(x = rnorm(30, 0, 1), y = 0),
             mapping = aes(x, y)) +
  ylab("") +
  scale_y_continuous(breaks = NULL) +
  xlim(c(-6,6))
```



## Maximum Likelihood Estimation (MLE) {.smaller}


```{r, echo=FALSE, out.width="40%"}
set.seed(1201)
ggplot(data = data.frame(y = c(-3, 3)), 
       aes(y)) +
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 0, sd = 1),
                col = viridis::viridis(4)[1],
                lwd = 1) + 
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 0, sd = 2),
                col = viridis::viridis(4)[2],
                lwd = 1) + 
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 2, sd = 1),
                col = viridis::viridis(4)[3],
                lwd = 1) + 
  stat_function(fun = dnorm, 
                n = 101, 
                args = list(mean = 1, sd = 2),
                col = viridis::viridis(4)[4],
                lwd = 1) + 
  geom_point(data = data.frame(x = rnorm(30, 0, 1), y = 0),
             mapping = aes(x, y)) +
  ylab("") +
  scale_y_continuous(breaks = NULL) +
  xlim(c(-6,6))
```

-   The points seem to be centered around zero and they range is between
    $[-2,2]$
-   Most likely, it is the violet distribution that has generated the points
-   With MLE: (1) we observe the data, (2) assume a distribution it come from,
    and (3) look for the values of parameters defining this distribution that
    result in the curve that best fits the data

--------------------------------------------------------------------------------
```{r, echo=FALSE}
set.seed(1201)
x = seq(-10, 10, length.out = 1000)
dat <- data.frame(x = x,
  # x=runif(1000, 0, 50),
                  y=rnorm(1000, 5*x, 20))

## breaks: where you want to compute densities
breaks <- seq(min(dat$x), max(dat$x), len=5)
dat$section <- cut(dat$x, breaks)

## Get the residuals
dat$res <- residuals(lm(y ~ x, data=dat))
dat$fitted <- fitted(lm(y ~ x, data=dat))


dens <- do.call(rbind, lapply(split(dat, dat$section), function(x) {
  d <- density(dat$res, n=50)
  # res <- data.frame(x=max(x$x)- d$y*2000, y=d$x+mean(x$y))
  # res <- res[order(res$y), ]
  ## Get some data for normal lines as well
  xs <- seq(min(dat$res), max(dat$res), len=50)
  res <- data.frame(y = xs + mean(x$y),
                    x = max(x$x) - 100 * dnorm(xs, 0, sd(dat$res)))
  res$type <- rep("normal", 50)
  res
}))
dens$section <- rep(levels(dat$section), each=50)


dens %<>% 
  filter(type == "normal") %>%
  group_by(section) %>%
  mutate(mn = if_else(x == min((x)), 1, 0))

dat1 = sample_n(dat, 200)
MLE <- ggplot(dat, aes(x, y)) +
  geom_point(alpha = 0.3, data = dat1, mapping = aes(x, y)) +
  # geom_smooth(method="lm", fill=NA, lwd=2) +
  geom_path(data=dens[dens$type=="normal",], 
            aes(x, y, group=section), 
            color="#800010", lwd=0.5) +
  annotate("segment", 
           x = dens$x[dens$mn == 1], 
           xend = breaks[2:5], 
           y = dens$y[dens$mn == 1], 
           yend = predict(lm(y ~ x, dat), 
                          newdata = data.frame(x = breaks[2:5])),
           lty = 2,
           lwd=0.7,
           color="#800010") +
  geom_point(data = dens[dens$mn == 1,],
             aes(x = x, 
             y = y),
             color="#800010") +
  # geom_vline(xintercept = breaks, lty = 2) +
  labs(
    x = "X",
    y = "Y",
    title = "MLE Estimator",
    subtitle = "Find Parameter Values for which \nObserved Data Are Most Likely"
  ) +
  geom_smooth(
    method = "lm",
    color = "#003056",
    se = FALSE,
    size = 0.5
  ) +
  ylim(-100, 100)

OLS <- ggplot(dat,
              mapping = aes(x, y)) +
  geom_point(alpha = 0.3,
             data = dat1,
             mapping = aes(x, y)) +
  geom_smooth(method = "lm", color = "#003056", se = FALSE,
              dat,
              mapping = aes(x, y)) +
  geom_segment(aes(xend = x,
                   yend = fitted),
               color = "#800010",
               # size = 1,
               data = dat1) +
  labs(x = "X",
       y = "Y",
       title = "OLS Estimator",
       subtitle = ("Find Parameter Values to Minimize \nSum of Squared Residuals")) +
  ylim(-100, 100) +
  theme(plot.subtitle = element_text(margin=margin(0,0,0,0)))


OLS + MLE
```
<!-- # Intro -->

<!-- ```{r} -->

<!-- #| echo: false -->

<!-- countdown(minutes = 4, color_background = "white", left = 1) -->

<!-- ``` -->

<!-- ::: columns -->

<!-- ::: {.column width="30%"} -->

<!-- ```{dot} -->

<!-- //| echo: false -->

<!-- //| fig-width: 3 -->

<!-- //| fig-height: 3 -->

<!-- //| out-width: 100% -->

<!-- digraph D { -->

<!--   node [shape=oval, fontname="AtkinsonHyperlegible-Regular"]; -->

<!--   edge [minlen = 1.2, arrowhead = vee, arrowsize=0.5]; -->

<!--   a [label = "Beauty Score"]; -->

<!--   b [label = "Evaluations"]; -->

<!--   c [label = "Female"]; -->

<!--   {rank=same a b}; -->

<!--   {rank=sink c}; -->

<!--   a->b; -->

<!--   c->a; -->

<!--   c->b; -->

<!-- } -->

<!-- ``` -->

<!-- :::  -->

<!-- ::: {.column width="70%"} -->

<!-- 1.  After specifying the DGP in a DAG and following our identification strategy, we can interpret the estimated coefficient for the effect of *Beauty Score* on *Evaluations* causally. -->

<!-- 2.  After specifying the DGP in a DAG and following our identification strategy, we can interpret the estimated coefficient for the effect of *Female* on *Evaluations* causally. -->

<!-- 3. On average, a 5-point increase in *Beauty Score* would be associated with 0.4-point increase in *Teaching Evaluations*, for both female and female instructors.  -->

<!-- 4. The effect of *Female* on *Teaching Evaluations* would be represented through the change in the slope of the regression coefficient for *Beauty Score*.  -->

<!-- 5. For $evals = \beta_0 + \beta_1 \cdot Female$,  $\beta_1$ would be the equal to difference-in-means estimator.  -->

<!-- :::  -->

<!-- :::  -->

